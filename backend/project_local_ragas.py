#!/usr/bin/env python3
"""
ä½¿ç”¨ç°æœ‰æœ¬åœ°LLMçš„RAGASè¯„ä¼°é›†æˆ
ç›´æ¥ä½¿ç”¨é¡¹ç›®é…ç½®çš„æœ¬åœ°æ¨¡å‹
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        context_entity_recall,
        answer_similarity,
        answer_correctness,
    )
    from datasets import Dataset
    from ragas.llms import LangchainLLMWrapper

    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    print("âš ï¸  RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")
    print("   æˆ–ä½¿ç”¨ç®€åŒ–è¯„ä¼°: python local_ragas_integration.py --mode simple")

# å¯¼å…¥é¡¹ç›®é…ç½®
from config import settings

# å¯¼å…¥RAGæœåŠ¡
from services.rag_generator import rag_generator
from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from models import RetrievalConfig, GenerationConfig, EmbeddingConfig, VectorDBConfig
from models import EmbeddingModelType, VectorDBType


class ProjectLocalRAGASIntegration:
    """ä½¿ç”¨é¡¹ç›®é…ç½®çš„æœ¬åœ°LLMçš„RAGASè¯„ä¼°é›†æˆç±»"""

    def __init__(self, use_ground_truth: bool = True):
        self.use_ground_truth = use_ground_truth
        self.evaluation_history = []
        self.initialized = False
        self.services_initialized = False

        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")

        # åŸºç¡€æŒ‡æ ‡ï¼ˆæ— éœ€ground truthï¼‰
        self.basic_metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]

        # é«˜çº§æŒ‡æ ‡ï¼ˆéœ€è¦ground truthï¼‰
        self.advanced_metrics = [
            answer_correctness,
            answer_similarity,
            context_entity_recall,
        ]

    def check_local_llm_status(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°LLMé…ç½®çŠ¶æ€"""
        print("ğŸ” æ£€æŸ¥æœ¬åœ°LLMé…ç½®...")
        print(f"   æä¾›å•†: {settings.llm_provider}")
        print(f"   æ¨¡å‹: {settings.llm_model}")
        print(f"   æœ¬åœ°æ¨¡å‹è·¯å¾„: {settings.local_llm_model_path}")

        if settings.llm_provider != "local":
            print(f"   âš ï¸  å½“å‰é…ç½®ä¸º {settings.llm_provider}ï¼Œä¸æ˜¯æœ¬åœ°æ¨¡å‹")
            print(f"   ğŸ’¡ å¦‚éœ€ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œè¯·ä¿®æ”¹ config.py:")
            print(f"      llm_provider = 'local'")
            return False

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        model_path = Path(settings.local_llm_model_path)
        if not model_path.exists():
            print(f"   âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False

        print(f"   âœ… æœ¬åœ°æ¨¡å‹é…ç½®æ­£å¸¸")
        return True

    def initialize_services(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        if self.services_initialized:
            return True

        print("ğŸš€ åˆå§‹åŒ–RAGæœåŠ¡...")

        try:
            # 1. åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if not embedding_service.is_loaded():
                print("   ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹...")
                config = EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name="BAAI/bge-small-zh-v1.5",
                    device="cpu",
                    batch_size=8,
                )
                response = embedding_service.load_model(config)
                if response.status != "success":
                    print(f"   âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {response.message}")
                    return False
                print(f"   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (ç»´åº¦: {response.dimension})")
            else:
                print("   âœ… åµŒå…¥æ¨¡å‹å·²åŠ è½½")

            # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            if vector_db_manager.db is None:
                print("   ğŸ“¥ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
                dimension = embedding_service.get_dimension()
                config = VectorDBConfig(
                    db_type=VectorDBType.FAISS, dimension=dimension, index_type="HNSW"
                )
                success = vector_db_manager.initialize(config)
                if not success:
                    print("   âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                    return False
                print("   âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("   âœ… å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–")

            # 3. æ£€æŸ¥å‘é‡åº“çŠ¶æ€
            status = vector_db_manager.get_status()
            print(f"   ğŸ“Š å‘é‡åº“çŠ¶æ€: {status.total_vectors} ä¸ªå‘é‡")

            if status.total_vectors == 0:
                print("   âš ï¸  è­¦å‘Š: å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
                return False

            self.services_initialized = True
            print("âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ\n")
            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def evaluate_single_query(
        self,
        query: str,
        ground_truth: str = None,
        retrieval_config: RetrievalConfig = None,
        generation_config: GenerationConfig = None,
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        # æ£€æŸ¥æœ¬åœ°LLMé…ç½®
        if not self.check_local_llm_status():
            return {
                "query": query,
                "error": "æœ¬åœ°LLMé…ç½®æ£€æŸ¥å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        # ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
        if not self.initialize_services():
            return {
                "query": query,
                "error": "RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        print(f"ğŸ” è¯„ä¼°æŸ¥è¯¢: {query[:50]}...")

        # ä½¿ç”¨é»˜è®¤é…ç½®
        if retrieval_config is None:
            retrieval_config = RetrievalConfig(top_k=5)
        if generation_config is None:
            # ä½¿ç”¨é¡¹ç›®é…ç½®çš„æœ¬åœ°LLM
            generation_config = GenerationConfig(
                llm_provider="local",
                llm_model=settings.llm_model,
                temperature=0.7,
                max_tokens=500,
            )

        # è¿è¡ŒRAGç³»ç»Ÿ
        start_time = time.time()
        try:
            response = rag_generator.generate(
                query=query,
                retrieval_config=retrieval_config,
                generation_config=generation_config,
            )
            rag_time = time.time() - start_time

            # æå–ä¿¡æ¯
            answer = response.answer
            contexts = (
                [chunk.content for chunk in response.context_chunks]
                if response.context_chunks
                else []
            )

            print(f"   âœ“ RAGæ‰§è¡Œå®Œæˆ ({rag_time:.2f}s)")
            print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            print(f"   ğŸ’¬ å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")

        except Exception as e:
            print(f"   âŒ RAGæ‰§è¡Œå¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

        # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°
        if not contexts:
            print("   âš ï¸  æ— æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°")
            return {
                "query": query,
                "answer": answer,
                "error": "æ— æ£€ç´¢ä¸Šä¸‹æ–‡",
                "timestamp": datetime.now().isoformat(),
            }

        # å‡†å¤‡RAGASæ•°æ®
        data_dict = {
            "question": [query],
            "answer": [answer],
            "contexts": [contexts],
        }

        if ground_truth:
            data_dict["ground_truth"] = [ground_truth]

        dataset = Dataset.from_dict(data_dict)

        # é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
        metrics = self.basic_metrics.copy()
        if ground_truth and self.use_ground_truth:
            metrics.extend(self.advanced_metrics)

        # è¿è¡Œè¯„ä¼°
        print("   ğŸ§ª è¿è¡ŒRAGASè¯„ä¼°...")
        try:
            eval_start = time.time()
            result = evaluate(dataset=dataset, metrics=metrics, raise_exceptions=False)
            eval_time = time.time() - eval_start

            # è½¬æ¢ç»“æœ
            scores = {
                k: float(v[0]) if hasattr(v, "__getitem__") else float(v)
                for k, v in result.items()
            }

            print(f"   âœ“ è¯„ä¼°å®Œæˆ ({eval_time:.2f}s)")

            # æ„å»ºç»“æœ
            evaluation_result = {
                "query": query,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": ground_truth,
                "scores": scores,
                "rag_time": rag_time,
                "eval_time": eval_time,
                "timestamp": datetime.now().isoformat(),
            }

            # æ·»åŠ åˆ°å†å²
            self.evaluation_history.append(evaluation_result)

            # æ‰“å°ç»“æœ
            self._print_scores(scores)

            return evaluation_result

        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "answer": answer,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def evaluate_test_dataset(
        self, test_file: str, max_samples: int = None
    ) -> Dict[str, Any]:
        """è¯„ä¼°æµ‹è¯•æ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_file}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_path = Path(test_file)
        if not test_path.exists():
            test_path = Path(__file__).parent / "test_data" / test_file

        if not test_path.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return {"error": f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"}

        try:
            with open(test_path, "r", encoding="utf-8") as f:
                test_data = json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            return {"error": f"åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}"}

        # è·å–æµ‹è¯•ç”¨ä¾‹
        test_cases = test_data.get("end_to_end_test_cases", [])
        if not test_cases:
            test_cases = test_data.get("retrieval_test_cases", [])

        if not test_cases:
            print("âŒ æµ‹è¯•æ•°æ®ä¸ºç©º")
            return {"error": "æµ‹è¯•æ•°æ®ä¸ºç©º"}

        if max_samples:
            test_cases = test_cases[:max_samples]

        print(f"ğŸ§ª å¼€å§‹è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹\n")

        # æ‰¹é‡è¯„ä¼°
        results = []
        for i, case in enumerate(test_cases, 1):
            print(f"[{i}/{len(test_cases)}] ", end="")

            query = case.get("query")
            expected = case.get("expected_answer_contains", [])
            ground_truth = (
                " ".join(expected) if isinstance(expected, list) else str(expected)
            )

            result = self.evaluate_single_query(
                query=query, ground_truth=ground_truth if ground_truth else None
            )
            results.append(result)
            print()

        # è®¡ç®—ç»Ÿè®¡
        successful_evals = [r for r in results if "scores" in r]
        if successful_evals:
            avg_scores = {}
            for metric in successful_evals[0]["scores"].keys():
                values = [
                    r["scores"][metric]
                    for r in successful_evals
                    if metric in r["scores"]
                ]
                avg_scores[metric] = sum(values) / len(values) if values else 0

            summary = {
                "total_cases": len(test_cases),
                "successful_evals": len(successful_evals),
                "failed_evals": len(test_cases) - len(successful_evals),
                "average_scores": avg_scores,
                "timestamp": datetime.now().isoformat(),
            }
        else:
            summary = {
                "total_cases": len(test_cases),
                "successful_evals": 0,
                "failed_evals": len(test_cases),
                "timestamp": datetime.now().isoformat(),
            }

        # ä¿å­˜ç»“æœ
        self._save_results(results, summary)
        self._print_summary(summary)

        return {"results": results, "summary": summary}

    def _print_scores(self, scores: Dict[str, float]):
        """æ‰“å°è¯„åˆ†"""
        for metric, score in scores.items():
            if score >= 0.8:
                status = "ğŸŸ¢"
            elif score >= 0.6:
                status = "ğŸŸ¡"
            else:
                status = "ğŸ”´"
            print(f"   {status} {metric}: {score:.3f}")

    def _print_summary(self, summary: Dict):
        """æ‰“å°æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ“Š è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {summary['total_cases']}")
        print(f"æˆåŠŸè¯„ä¼°: {summary['successful_evals']} âœ…")
        print(f"å¤±è´¥: {summary['failed_evals']} âŒ")

        if "average_scores" in summary and summary["average_scores"]:
            print("\nå¹³å‡æŒ‡æ ‡å¾—åˆ†:")
            for metric, score in summary["average_scores"].items():
                if score >= 0.8:
                    status = "ğŸŸ¢ ä¼˜ç§€"
                elif score >= 0.6:
                    status = "ğŸŸ¡ è‰¯å¥½"
                else:
                    status = "ğŸ”´ éœ€ä¼˜åŒ–"
                print(f"  {metric}: {score:.3f} {status}")

        print("=" * 70)

    def _save_results(self, results: List[Dict], summary: Dict):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_dir = Path(__file__).parent / "evaluation_results"
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"ragas_eval_local_{timestamp}.json"

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                {"summary": summary, "results": results},
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    def get_evaluation_report(self) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        if not self.evaluation_history:
            return "æš‚æ— è¯„ä¼°æ•°æ®"

        lines = []
        lines.append("=" * 70)
        lines.append("RAGASè¯„ä¼°æŠ¥å‘Š")
        lines.append("=" * 70)
        lines.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"è¯„ä¼°æ ·æœ¬æ•°: {len(self.evaluation_history)}")
        lines.append("")

        # è®¡ç®—å¹³å‡åˆ†
        all_scores = {}
        for eval_result in self.evaluation_history:
            if "scores" in eval_result:
                for metric, score in eval_result["scores"].items():
                    if metric not in all_scores:
                        all_scores[metric] = []
                    all_scores[metric].append(score)

        lines.append("æŒ‡æ ‡ç»Ÿè®¡:")
        for metric, scores in all_scores.items():
            avg = sum(scores) / len(scores)
            min_score = min(scores)
            max_score = max(scores)
            lines.append(f"  {metric}:")
            lines.append(f"    å¹³å‡: {avg:.3f}")
            lines.append(f"    æœ€å°: {min_score:.3f}")
            lines.append(f"    æœ€å¤§: {max_score:.3f}")

        lines.append("=" * 70)

        return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ä½¿ç”¨é¡¹ç›®æœ¬åœ°LLMçš„RAGASè¯„ä¼°")
    parser.add_argument(
        "--mode", choices=["single", "batch"], default="batch", help="è¯„ä¼°æ¨¡å¼"
    )
    parser.add_argument("--query", "-q", type=str, help="å•ä¸ªæŸ¥è¯¢")
    parser.add_argument("--ground-truth", "-g", type=str, help="æœŸæœ›ç­”æ¡ˆ")
    parser.add_argument(
        "--test-file",
        "-f",
        type=str,
        default="test_data/test_dataset.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶",
    )
    parser.add_argument("--max-samples", "-n", type=int, help="æœ€å¤§æ ·æœ¬æ•°")

    args = parser.parse_args()

    if not RAGAS_AVAILABLE:
        print("âŒ RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")
        return

    evaluator = ProjectLocalRAGASIntegration()

    if args.mode == "single":
        if not args.query:
            print("âŒ è¯·æä¾›æŸ¥è¯¢: --query 'ä½ çš„é—®é¢˜'")
            return

        result = evaluator.evaluate_single_query(
            query=args.query, ground_truth=args.ground_truth
        )

        if evaluator.evaluation_history:
            print("\n" + evaluator.get_evaluation_report())

    elif args.mode == "batch":
        evaluator.evaluate_test_dataset(
            test_file=args.test_file, max_samples=args.max_samples
        )


if __name__ == "__main__":
    main()
