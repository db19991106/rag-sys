#!/usr/bin/env python3
"""
æœ¬åœ°æ¨¡å‹RAGASè¯„ä¼°é›†æˆæ¨¡å—
ä½¿ç”¨Ollamaæˆ–æœ¬åœ°vLLM/LLaMA.cppç­‰æœ¬åœ°LLM
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        context_entity_recall,
        answer_similarity,
        answer_correctness,
    )
    from datasets import Dataset
    from ragas.llms import LangchainLLMWrapper

    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    print("âš ï¸  RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")

# å°è¯•å¯¼å…¥LangChainçš„æœ¬åœ°æ¨¡å‹æ”¯æŒ
try:
    from langchain_community.llms import Ollama
    from langchain.callbacks.manager import CallbackManager
    from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸  langchain-communityæœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain-community")

# å¯¼å…¥RAGæœåŠ¡
from services.rag_generator import rag_generator
from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from services.retriever import retriever
from models import RetrievalConfig, GenerationConfig, EmbeddingConfig, VectorDBConfig
from models import EmbeddingModelType, VectorDBType


class LocalLLMConfig:
    """æœ¬åœ°LLMé…ç½®"""

    # æ”¯æŒçš„æœ¬åœ°æ¨¡å‹æä¾›å•†
    PROVIDERS = {
        "ollama": {
            "name": "Ollama",
            "description": "æœ¬åœ°OllamaæœåŠ¡",
            "default_model": "qwen2.5:7b",
            "base_url": "http://localhost:11434",
        },
        "vllm": {
            "name": "vLLM",
            "description": "vLLMæ¨ç†æœåŠ¡",
            "default_model": "Qwen/Qwen2.5-7B-Instruct",
            "base_url": "http://localhost:8000",
        },
        "llamacpp": {
            "name": "llama.cpp",
            "description": "llama.cppæœ¬åœ°æ¨¡å‹",
            "default_model": "qwen2.5-7b-q4_k_m.gguf",
            "base_url": "http://localhost:8080",
        },
    }

    @classmethod
    def get_provider(cls, name: str):
        return cls.PROVIDERS.get(name, cls.PROVIDERS["ollama"])


class LocalRAGASIntegration:
    """ä½¿ç”¨æœ¬åœ°LLMçš„RAGASè¯„ä¼°é›†æˆç±»"""

    def __init__(
        self,
        provider: str = "ollama",
        model_name: str = None,
        base_url: str = None,
        temperature: float = 0.0,
        use_ground_truth: bool = True,
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°RAGASè¯„ä¼°å™¨

        Args:
            provider: æœ¬åœ°æ¨¡å‹æä¾›å•† (ollama/vllm/llamacpp)
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
            base_url: APIåœ°å€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„åœ°å€
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œè¯„ä¼°å»ºè®®ç”¨0.0ç¡®ä¿ä¸€è‡´æ€§
            use_ground_truth: æ˜¯å¦ä½¿ç”¨æ ‡å‡†ç­”æ¡ˆè¯„ä¼°
        """
        self.provider = provider
        self.config = LocalLLMConfig.get_provider(provider)
        self.model_name = model_name or self.config["default_model"]
        self.base_url = base_url or self.config["base_url"]
        self.temperature = temperature
        self.use_ground_truth = use_ground_truth
        self.evaluation_history = []
        self.initialized = False
        self.local_llm = None
        self.ragas_llm = None

        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")

        if not LANGCHAIN_AVAILABLE:
            raise ImportError(
                "langchain-communityæœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain-community"
            )

        # åŸºç¡€æŒ‡æ ‡
        self.basic_metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]

        # é«˜çº§æŒ‡æ ‡ï¼ˆéœ€è¦ground truthï¼‰
        self.advanced_metrics = [
            answer_correctness,
            answer_similarity,
            context_entity_recall,
        ]

    def initialize_local_llm(self):
        """åˆå§‹åŒ–æœ¬åœ°LLM"""
        if self.local_llm is not None:
            return True

        print(f"ğŸ¤– åˆå§‹åŒ–æœ¬åœ°LLM: {self.config['name']}")
        print(f"   æ¨¡å‹: {self.model_name}")
        print(f"   åœ°å€: {self.base_url}")

        try:
            if self.provider == "ollama":
                self.local_llm = Ollama(
                    model=self.model_name,
                    base_url=self.base_url,
                    temperature=self.temperature,
                    callback_manager=CallbackManager(
                        [StreamingStdOutCallbackHandler()]
                    ),
                )
            elif self.provider == "vllm":
                from langchain_community.llms import VLLM

                self.local_llm = VLLM(
                    model=self.model_name,
                    openai_api_base=self.base_url,
                    temperature=self.temperature,
                )
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æä¾›å•†: {self.provider}")
                return False

            # åŒ…è£…ä¸ºRAGAS LLM
            self.ragas_llm = LangchainLLMWrapper(self.local_llm)

            print(f"   âœ… æœ¬åœ°LLMåˆå§‹åŒ–æˆåŠŸ\n")
            return True

        except Exception as e:
            print(f"   âŒ æœ¬åœ°LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"\nğŸ’¡ è¯·ç¡®ä¿:")
            print(f"   1. {self.config['name']} æœåŠ¡å·²å¯åŠ¨")
            print(f"   2. æ¨¡å‹ {self.model_name} å·²ä¸‹è½½")
            if self.provider == "ollama":
                print(f"   3. è¿è¡Œ: ollama pull {self.model_name}")
            return False

    def test_local_llm(self):
        """æµ‹è¯•æœ¬åœ°LLMæ˜¯å¦å¯ç”¨"""
        if not self.initialize_local_llm():
            return False

        print("ğŸ§ª æµ‹è¯•æœ¬åœ°LLM...")
        try:
            # ç®€å•æµ‹è¯•
            response = self.local_llm.invoke("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±ã€‚")
            print(f"   âœ… LLMå“åº”æ­£å¸¸")
            print(f"   å“åº”: {response[:100]}...")
            return True
        except Exception as e:
            print(f"   âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
            return False

    def initialize_services(self):
        """åˆå§‹åŒ–RAGæœåŠ¡ï¼ˆåµŒå…¥æ¨¡å‹å’Œå‘é‡æ•°æ®åº“ï¼‰"""
        if self.initialized:
            return True

        print("ğŸš€ åˆå§‹åŒ–RAGæœåŠ¡...")

        try:
            # 1. åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if not embedding_service.is_loaded():
                print("   ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹...")
                config = EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name="BAAI/bge-small-zh-v1.5",
                    device="cpu",
                    batch_size=8,
                )
                response = embedding_service.load_model(config)
                if response.status != "success":
                    print(f"   âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {response.message}")
                    return False
                print(f"   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (ç»´åº¦: {response.dimension})")
            else:
                print("   âœ… åµŒå…¥æ¨¡å‹å·²åŠ è½½")

            # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            if vector_db_manager.db is None:
                print("   ğŸ“¥ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
                dimension = embedding_service.get_dimension()
                config = VectorDBConfig(
                    db_type=VectorDBType.FAISS, dimension=dimension, index_type="HNSW"
                )
                success = vector_db_manager.initialize(config)
                if not success:
                    print("   âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                    return False
                print("   âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("   âœ… å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–")

            # 3. æ£€æŸ¥å‘é‡åº“çŠ¶æ€
            status = vector_db_manager.get_status()
            print(f"   ğŸ“Š å‘é‡åº“çŠ¶æ€: {status.total_vectors} ä¸ªå‘é‡")

            if status.total_vectors == 0:
                print("   âš ï¸  è­¦å‘Š: å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
                return False

            self.initialized = True
            print("âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ\n")
            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def evaluate_single_query(
        self,
        query: str,
        ground_truth: str = None,
        retrieval_config: RetrievalConfig = None,
        generation_config: GenerationConfig = None,
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæŸ¥è¯¢
        """
        # ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
        if not self.initialize_services():
            return {
                "query": query,
                "error": "RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        # ç¡®ä¿LLMå·²åˆå§‹åŒ–
        if not self.initialize_local_llm():
            return {
                "query": query,
                "error": "æœ¬åœ°LLMåˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        print(f"ğŸ” è¯„ä¼°æŸ¥è¯¢: {query[:50]}...")

        # ä½¿ç”¨é»˜è®¤é…ç½®
        if retrieval_config is None:
            retrieval_config = RetrievalConfig(top_k=5)
        if generation_config is None:
            generation_config = GenerationConfig(temperature=0.7, max_tokens=500)

        # è¿è¡ŒRAGç³»ç»Ÿ
        start_time = time.time()
        try:
            response = rag_generator.generate(
                query=query,
                retrieval_config=retrieval_config,
                generation_config=generation_config,
            )
            rag_time = time.time() - start_time

            # æå–ä¿¡æ¯
            answer = response.answer
            contexts = (
                [chunk.content for chunk in response.context_chunks]
                if response.context_chunks
                else []
            )

            print(f"   âœ“ RAGæ‰§è¡Œå®Œæˆ ({rag_time:.2f}s)")
            print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            print(f"   ğŸ’¬ å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")

        except Exception as e:
            print(f"   âŒ RAGæ‰§è¡Œå¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

        # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°
        if not contexts:
            print("   âš ï¸  æ— æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°")
            return {
                "query": query,
                "answer": answer,
                "error": "æ— æ£€ç´¢ä¸Šä¸‹æ–‡",
                "timestamp": datetime.now().isoformat(),
            }

        # å‡†å¤‡RAGASæ•°æ®
        data_dict = {
            "question": [query],
            "answer": [answer],
            "contexts": [contexts],
        }

        if ground_truth:
            data_dict["ground_truth"] = [ground_truth]

        dataset = Dataset.from_dict(data_dict)

        # é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
        metrics = self.basic_metrics.copy()
        if ground_truth and self.use_ground_truth:
            metrics.extend(self.advanced_metrics)

        # è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨æœ¬åœ°LLMï¼‰
        print("   ğŸ§ª è¿è¡ŒRAGASè¯„ä¼°ï¼ˆæœ¬åœ°LLMï¼‰...")
        try:
            eval_start = time.time()
            result = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=self.ragas_llm,
                raise_exceptions=False,
            )
            eval_time = time.time() - eval_start

            # è½¬æ¢ç»“æœ
            scores = {
                k: float(v[0]) if hasattr(v, "__getitem__") else float(v)
                for k, v in result.items()
            }

            print(f"   âœ“ è¯„ä¼°å®Œæˆ ({eval_time:.2f}s)")

            # æ„å»ºç»“æœ
            evaluation_result = {
                "query": query,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": ground_truth,
                "scores": scores,
                "rag_time": rag_time,
                "eval_time": eval_time,
                "timestamp": datetime.now().isoformat(),
            }

            # æ·»åŠ åˆ°å†å²
            self.evaluation_history.append(evaluation_result)

            # æ‰“å°ç»“æœ
            self._print_scores(scores)

            return evaluation_result

        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "answer": answer,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def evaluate_test_dataset(
        self, test_file: str, max_samples: int = None
    ) -> Dict[str, Any]:
        """è¯„ä¼°æµ‹è¯•æ•°æ®é›†"""
        if not self.initialize_services():
            return {"error": "RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥"}

        if not self.initialize_local_llm():
            return {"error": "æœ¬åœ°LLMåˆå§‹åŒ–å¤±è´¥"}

        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_file}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_path = Path(test_file)
        if not test_path.exists():
            test_path = Path(__file__).parent / "test_data" / test_file

        if not test_path.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return {"error": f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"}

        try:
            with open(test_path, "r", encoding="utf-8") as f:
                test_data = json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            return {"error": f"åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}"}

        # è·å–æµ‹è¯•ç”¨ä¾‹
        test_cases = test_data.get("end_to_end_test_cases", [])
        if not test_cases:
            test_cases = test_data.get("retrieval_test_cases", [])

        if not test_cases:
            print("âŒ æµ‹è¯•æ•°æ®ä¸ºç©º")
            return {"error": "æµ‹è¯•æ•°æ®ä¸ºç©º"}

        if max_samples:
            test_cases = test_cases[:max_samples]

        print(f"ğŸ§ª å¼€å§‹è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹\n")

        # æ‰¹é‡è¯„ä¼°
        results = []
        for i, case in enumerate(test_cases, 1):
            print(f"[{i}/{len(test_cases)}] ", end="")

            query = case.get("query")
            expected = case.get("expected_answer_contains", [])
            ground_truth = (
                " ".join(expected) if isinstance(expected, list) else str(expected)
            )

            result = self.evaluate_single_query(
                query, ground_truth if ground_truth else None
            )
            results.append(result)
            print()

        # è®¡ç®—ç»Ÿè®¡
        successful_evals = [r for r in results if "scores" in r]
        if successful_evals:
            avg_scores = {}
            for metric in successful_evals[0]["scores"].keys():
                values = [
                    r["scores"][metric]
                    for r in successful_evals
                    if metric in r["scores"]
                ]
                avg_scores[metric] = sum(values) / len(values) if values else 0

            summary = {
                "total_cases": len(test_cases),
                "successful_evals": len(successful_evals),
                "failed_evals": len(test_cases) - len(successful_evals),
                "average_scores": avg_scores,
                "llm_provider": self.provider,
                "llm_model": self.model_name,
                "timestamp": datetime.now().isoformat(),
            }
        else:
            summary = {
                "total_cases": len(test_cases),
                "successful_evals": 0,
                "failed_evals": len(test_cases),
                "timestamp": datetime.now().isoformat(),
            }

        # ä¿å­˜ç»“æœ
        self._save_results(results, summary)
        self._print_summary(summary)

        return {"results": results, "summary": summary}

    def _print_scores(self, scores: Dict[str, float]):
        """æ‰“å°è¯„åˆ†"""
        for metric, score in scores.items():
            if score >= 0.8:
                status = "ğŸŸ¢"
            elif score >= 0.6:
                status = "ğŸŸ¡"
            else:
                status = "ğŸ”´"
            print(f"   {status} {metric}: {score:.3f}")

    def _print_summary(self, summary: Dict):
        """æ‰“å°æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ“Š è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {summary['total_cases']}")
        print(f"æˆåŠŸè¯„ä¼°: {summary['successful_evals']} âœ…")
        print(f"å¤±è´¥: {summary['failed_evals']} âŒ")

        if "average_scores" in summary and summary["average_scores"]:
            print("\nå¹³å‡æŒ‡æ ‡å¾—åˆ†:")
            for metric, score in summary["average_scores"].items():
                if score >= 0.8:
                    status = "ğŸŸ¢ ä¼˜ç§€"
                elif score >= 0.6:
                    status = "ğŸŸ¡ è‰¯å¥½"
                else:
                    status = "ğŸ”´ éœ€ä¼˜åŒ–"
                print(f"  {metric}: {score:.3f} {status}")

        print("=" * 70)

    def _save_results(self, results: List[Dict], summary: Dict):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_dir = Path(__file__).parent / "evaluation_results"
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        provider_name = self.provider if self.provider else "local"
        output_file = output_dir / f"ragas_eval_{provider_name}_{timestamp}.json"

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                {"summary": summary, "results": results},
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")


# ==================== å‘½ä»¤è¡Œå…¥å£ ====================


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æœ¬åœ°LLM RAGASè¯„ä¼°å·¥å…·")
    parser.add_argument(
        "--provider",
        "-p",
        choices=["ollama", "vllm", "llamacpp"],
        default="ollama",
        help="æœ¬åœ°LLMæä¾›å•†",
    )
    parser.add_argument(
        "--model", "-m", type=str, help="æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹ï¼‰"
    )
    parser.add_argument(
        "--base-url", "-u", type=str, help="APIåœ°å€ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„åœ°å€ï¼‰"
    )
    parser.add_argument("--test", "-t", action="store_true", help="æµ‹è¯•LLMè¿æ¥")
    parser.add_argument(
        "--mode", choices=["single", "batch"], default="batch", help="è¯„ä¼°æ¨¡å¼"
    )
    parser.add_argument("--query", "-q", type=str, help="å•ä¸ªæŸ¥è¯¢")
    parser.add_argument("--ground-truth", "-g", type=str, help="æœŸæœ›ç­”æ¡ˆ")
    parser.add_argument(
        "--test-file",
        "-f",
        type=str,
        default="test_data/test_dataset.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶",
    )
    parser.add_argument("--max-samples", "-n", type=int, help="æœ€å¤§æ ·æœ¬æ•°")

    args = parser.parse_args()

    if not RAGAS_AVAILABLE or not LANGCHAIN_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–ï¼Œè¯·å®‰è£…:")
        print("   pip install ragas langchain-community")
        return

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LocalRAGASIntegration(
        provider=args.provider,
        model_name=args.model,
        base_url=args.base_url,
        temperature=0.0,  # è¯„ä¼°æ—¶ä¿æŒç¡®å®šæ€§
    )

    if args.test:
        # ä»…æµ‹è¯•LLMè¿æ¥
        print("ğŸ”§ æµ‹è¯•æœ¬åœ°LLMè¿æ¥")
        print("=" * 70)
        if evaluator.test_local_llm():
            print("\nâœ… æœ¬åœ°LLMæµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œè¯„ä¼°")
        else:
            print("\nâŒ æœ¬åœ°LLMæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")

    elif args.mode == "single":
        if not args.query:
            print("âŒ è¯·æä¾›æŸ¥è¯¢: --query 'ä½ çš„é—®é¢˜'")
            return

        result = evaluator.evaluate_single_query(
            query=args.query, ground_truth=args.ground_truth
        )

        if evaluator.evaluation_history:
            print("\n" + evaluator.get_evaluation_report())

    elif args.mode == "batch":
        evaluator.evaluate_test_dataset(
            test_file=args.test_file, max_samples=args.max_samples
        )


if __name__ == "__main__":
    main()
