#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•æ¨¡å—
"""

import sys
import time
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.retriever import retriever
from services.rag_generator import rag_generator
from services.document_manager import document_manager
from models import RetrievalConfig, GenerationConfig


class TestPerformance:
    """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½"""

    def test_retrieval_performance(self):
        """æµ‹è¯•æ£€ç´¢æ€§èƒ½"""
        print("=" * 70)
        print("æµ‹è¯•æ£€ç´¢æ€§èƒ½")
        print("=" * 70)
        
        config = RetrievalConfig(
            top_k=5,
            similarity_threshold=0.6
        )
        
        # é¢„çƒ­
        print("é¢„çƒ­ä¸­...")
        retriever.retrieve("æµ‹è¯•é¢„çƒ­", config)
        
        # æµ‹è¯•å¤šæ¬¡æ£€ç´¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢æ€§èƒ½ï¼Ÿ",
            "å‘é‡æ•°æ®åº“çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•æé«˜ç”Ÿæˆè´¨é‡ï¼Ÿ",
            "ç³»ç»Ÿæ¶æ„è®¾è®¡"
        ]
        
        times = []
        for i, query in enumerate(test_queries, 1):
            start_time = time.time()
            response = retriever.retrieve(query, config)
            end_time = time.time()
            duration = (end_time - start_time) * 1000
            times.append(duration)
            
            print(f"æŸ¥è¯¢ {i}/{len(test_queries)}: {query}")
            print(f"  è€—æ—¶: {duration:.2f}ms")
            print(f"  ç»“æœæ•°: {len(response.results)}")
            print()
        
        avg_time = sum(times) / len(times)
        max_time = max(times)
        min_time = min(times)
        
        print("=" * 70)
        print("æ£€ç´¢æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {max_time:.2f}ms")
        print(f"æœ€å°å“åº”æ—¶é—´: {min_time:.2f}ms")
        print()
        
        # æ€§èƒ½è¯„ä¼°
        if avg_time < 500:
            print("âœ… æ£€ç´¢æ€§èƒ½ä¼˜ç§€")
        elif avg_time < 1000:
            print("âœ… æ£€ç´¢æ€§èƒ½è‰¯å¥½")
        else:
            print("âš ï¸  æ£€ç´¢æ€§èƒ½éœ€è¦ä¼˜åŒ–")
        print("=" * 70)
        print()

    def test_generation_performance(self):
        """æµ‹è¯•ç”Ÿæˆæ€§èƒ½"""
        print("=" * 70)
        print("æµ‹è¯•ç”Ÿæˆæ€§èƒ½")
        print("=" * 70)
        
        retrieval_config = RetrievalConfig(
            top_k=3,
            similarity_threshold=0.6
        )
        
        generation_config = GenerationConfig(
            llm_provider="local",
            llm_model="Qwen2.5-7B-Instruct",
            temperature=0.7,
            max_tokens=500
        )
        
        # é¢„çƒ­
        print("é¢„çƒ­ä¸­...")
        rag_generator.generate("æµ‹è¯•é¢„çƒ­", retrieval_config, generation_config)
        
        # æµ‹è¯•å¤šæ¬¡ç”Ÿæˆ
        test_queries = [
            "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢æ€§èƒ½ï¼Ÿ",
            "å‘é‡æ•°æ®åº“çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        times = []
        for i, query in enumerate(test_queries, 1):
            start_time = time.time()
            response = rag_generator.generate(query, retrieval_config, generation_config)
            end_time = time.time()
            duration = (end_time - start_time) * 1000
            times.append(duration)
            
            print(f"æŸ¥è¯¢ {i}/{len(test_queries)}: {query}")
            print(f"  æ€»è€—æ—¶: {duration:.2f}ms")
            print(f"  æ£€ç´¢è€—æ—¶: {response.retrieval_time_ms:.2f}ms")
            print(f"  ç”Ÿæˆè€—æ—¶: {response.generation_time_ms:.2f}ms")
            print(f"  å›ç­”é•¿åº¦: {len(response.answer)}å­—ç¬¦")
            print()
        
        avg_time = sum(times) / len(times)
        max_time = max(times)
        min_time = min(times)
        
        print("=" * 70)
        print("ç”Ÿæˆæ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {max_time:.2f}ms")
        print(f"æœ€å°å“åº”æ—¶é—´: {min_time:.2f}ms")
        print()
        
        # æ€§èƒ½è¯„ä¼°
        if avg_time < 5000:
            print("âœ… ç”Ÿæˆæ€§èƒ½ä¼˜ç§€")
        elif avg_time < 10000:
            print("âœ… ç”Ÿæˆæ€§èƒ½è‰¯å¥½")
        else:
            print("âš ï¸  ç”Ÿæˆæ€§èƒ½éœ€è¦ä¼˜åŒ–")
        print("=" * 70)
        print()

    async def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print("=" * 70)
        print("æµ‹è¯•å¹¶å‘æ€§èƒ½")
        print("=" * 70)
        
        retrieval_config = RetrievalConfig(
            top_k=3,
            similarity_threshold=0.6
        )
        
        generation_config = GenerationConfig(
            llm_provider="local",
            llm_model="Qwen2.5-7B-Instruct",
            temperature=0.7,
            max_tokens=300
        )
        
        # é¢„çƒ­
        print("é¢„çƒ­ä¸­...")
        rag_generator.generate("æµ‹è¯•é¢„çƒ­", retrieval_config, generation_config)
        
        # å¹¶å‘æµ‹è¯•
        concurrency_levels = [5, 10, 20]
        
        for level in concurrency_levels:
            print(f"æµ‹è¯•å¹¶å‘æ•°: {level}")
            print("-" * 70)
            
            async def test_task(i):
                query = f"æµ‹è¯•å¹¶å‘æŸ¥è¯¢ {i}"
                start_time = time.time()
                response = rag_generator.generate(query, retrieval_config, generation_config)
                end_time = time.time()
                return (end_time - start_time) * 1000
            
            tasks = [test_task(i) for i in range(level)]
            start_time = time.time()
            results = await asyncio.gather(*tasks)
            total_time = (time.time() - start_time) * 1000
            
            avg_time = sum(results) / len(results)
            max_time = max(results)
            min_time = min(results)
            
            print(f"æ€»è€—æ—¶: {total_time:.2f}ms")
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
            print(f"æœ€å¤§å“åº”æ—¶é—´: {max_time:.2f}ms")
            print(f"æœ€å°å“åº”æ—¶é—´: {min_time:.2f}ms")
            print(f"ååé‡: {level / (total_time / 1000):.2f} QPS")
            print()
        
        print("=" * 70)
        print("å¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print("=" * 70)
        print()

    async def test_document_upload_performance(self):
        """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ æ€§èƒ½"""
        print("=" * 70)
        print("æµ‹è¯•æ–‡æ¡£ä¸Šä¼ æ€§èƒ½")
        print("=" * 70)
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„æ–‡æ¡£
        test_sizes = [
            (10, "10KB"),
            (100, "100KB"),
            (500, "500KB")
        ]
        
        for size_kb, size_label in test_sizes:
            test_content = b"x" * (size_kb * 1024)
            test_filename = f"test_{size_label}.txt"
            
            print(f"æµ‹è¯•æ–‡æ¡£å¤§å°: {size_label}")
            
            start_time = time.time()
            response = await document_manager.upload_document(test_filename, test_content)
            end_time = time.time()
            duration = (end_time - start_time) * 1000
            
            print(f"  è€—æ—¶: {duration:.2f}ms")
            print(f"  çŠ¶æ€: {response.status.value}")
            print()
            
            # æ¸…ç†
            document_manager.delete_document(response.id)
        
        print("=" * 70)
        print("æ–‡æ¡£ä¸Šä¼ æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print("=" * 70)
        print()


if __name__ == "__main__":
    tester = TestPerformance()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tester.test_retrieval_performance()
    tester.test_generation_performance()
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    async def run_async_tests():
        await tester.test_document_upload_performance()
        await tester.test_concurrent_performance()
    
    asyncio.run(run_async_tests())
    
    print("=" * 70)
    print("ğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    print("=" * 70)
