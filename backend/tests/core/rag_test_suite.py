#!/usr/bin/env python3
"""
RAGç³»ç»Ÿç»¼åˆæµ‹è¯•å¥—ä»¶
ä¸€ç«™å¼æµ‹è¯•æ¡†æ¶ï¼Œæ¶µç›–åŠŸèƒ½ã€æ€§èƒ½ã€æ•ˆæœå’Œç«¯åˆ°ç«¯æµ‹è¯•
"""

import sys
import os
import time
import json
import asyncio
import unittest
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from services.retriever import retriever
from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from services.reranker import reranker_manager
from services.intent_recognizer import intent_recognizer
from services.evaluation import evaluator
from services.rag_generator import rag_generator
from services.chunker import RAGFlowChunker
from services.document_parser import DocumentParser
from models import (
    RetrievalConfig,
    EmbeddingConfig,
    VectorDBConfig,
    GenerationConfig,
    EmbeddingModelType,
    VectorDBType,
    SimilarityAlgorithm,
    ChunkConfig,
    ChunkType,
)


class RAGTestSuite:
    """RAGç³»ç»Ÿæµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.results = {}
        self.test_data_dir = Path(__file__).parent / "test_data"
        self.test_data_dir.mkdir(exist_ok=True)

    # ==================== 1. åŠŸèƒ½æµ‹è¯• ====================

    def test_document_parser(self):
        """æµ‹è¯•æ–‡æ¡£è§£æåŠŸèƒ½"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•1ã€‘æ–‡æ¡£è§£æåŠŸèƒ½æµ‹è¯•")
        print("=" * 70)

        parser = DocumentParser()
        test_cases = []

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_files = {
            "txt": ("test.txt", "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬æ–‡ä»¶ã€‚\nåŒ…å«å¤šè¡Œå†…å®¹ã€‚"),
            "md": ("test.md", "# æ ‡é¢˜\n\nè¿™æ˜¯æ­£æ–‡å†…å®¹ã€‚"),
            "json": ("test.json", '{"key": "value", "number": 123}'),
        }

        results = []
        for ext, (filename, content) in test_files.items():
            test_file = self.test_data_dir / filename
            test_file.write_text(content, encoding="utf-8")

            try:
                result = parser.parse(str(test_file))
                success = result is not None and len(result) > 0
                results.append(
                    {
                        "format": ext,
                        "success": success,
                        "content_length": len(result) if result else 0,
                    }
                )
                status = "âœ…" if success else "âŒ"
                print(f"{status} {ext.upper()}è§£æ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
            except Exception as e:
                results.append({"format": ext, "success": False, "error": str(e)})
                print(f"âŒ {ext.upper()}è§£æ: å¼‚å¸¸ - {e}")
            finally:
                test_file.unlink(missing_ok=True)

        success_rate = sum(1 for r in results if r["success"]) / len(results)
        self.results["document_parser"] = {
            "success_rate": success_rate,
            "details": results,
        }
        print(f"\nè§£ææˆåŠŸç‡: {success_rate * 100:.1f}%")
        return success_rate >= 0.8

    def test_chunking_strategies(self):
        """æµ‹è¯•æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•2ã€‘æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥æµ‹è¯•")
        print("=" * 70)

        chunker = RAGFlowChunker()

        # æµ‹è¯•å†…å®¹
        test_content = """
# æµ‹è¯•æ–‡æ¡£

## ç¬¬ä¸€ç«  æ€»åˆ™

è¿™æ˜¯ç¬¬ä¸€ç« çš„å†…å®¹ã€‚

### 1.1 æ¦‚è¿°
è¯¦ç»†è¯´æ˜å†…å®¹ã€‚

### 1.2 è§„å®š
å…·ä½“è§„å®šå†…å®¹ã€‚

## ç¬¬äºŒç«  å®æ–½ç»†åˆ™

è¿™æ˜¯ç¬¬äºŒç« çš„å†…å®¹ã€‚

| é¡¹ç›® | æ ‡å‡† |
|------|------|
| A | 100 |
| B | 200 |
"""

        strategies = [
            (
                "INTELLIGENT",
                ChunkConfig(type=ChunkType.INTELLIGENT, chunk_token_size=512),
            ),
            ("NAIVE", ChunkConfig(type=ChunkType.NAIVE, chunk_token_size=512)),
        ]

        results = []
        for name, config in strategies:
            try:
                chunks = chunker.chunk(test_content, "test_doc", config)
                results.append(
                    {
                        "strategy": name,
                        "success": len(chunks) > 0,
                        "chunk_count": len(chunks),
                        "avg_chunk_size": sum(len(c.content) for c in chunks)
                        / len(chunks)
                        if chunks
                        else 0,
                    }
                )
                print(f"âœ… {name}ç­–ç•¥: ç”Ÿæˆ{len(chunks)}ä¸ªchunk")
            except Exception as e:
                results.append({"strategy": name, "success": False, "error": str(e)})
                print(f"âŒ {name}ç­–ç•¥: å¤±è´¥ - {e}")

        success = all(r["success"] for r in results)
        self.results["chunking"] = {"success": success, "details": results}
        return success

    def test_embedding_service(self):
        """æµ‹è¯•åµŒå…¥æœåŠ¡"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•3ã€‘åµŒå…¥æœåŠ¡æµ‹è¯•")
        print("=" * 70)

        if not embedding_service.is_loaded():
            print("âš ï¸ åµŒå…¥æœåŠ¡æœªåŠ è½½ï¼Œå°è¯•åˆå§‹åŒ–...")
            try:
                config = EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name="BAAI/bge-small-zh-v1.5",
                    device="cpu",
                    batch_size=8,
                )
                response = embedding_service.load_model(config)
                if response.status != "success":
                    print(f"âŒ åµŒå…¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {response.message}")
                    return False
            except Exception as e:
                print(f"âŒ åµŒå…¥æœåŠ¡åˆå§‹åŒ–å¼‚å¸¸: {e}")
                return False

        # æµ‹è¯•ç¼–ç 
        test_texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­", "RAGç³»ç»Ÿæµ‹è¯•", "å‘é‡åµŒå…¥æµ‹è¯•"]

        try:
            start_time = time.time()
            embeddings = embedding_service.encode(test_texts)
            encode_time = time.time() - start_time

            success = embeddings.shape[0] == len(test_texts)
            dimension = embeddings.shape[1] if success else 0

            self.results["embedding"] = {
                "success": success,
                "dimension": dimension,
                "encode_time": encode_time,
                "avg_time_per_text": encode_time / len(test_texts),
            }

            print(f"âœ… ç¼–ç æˆåŠŸ: {len(test_texts)}ä¸ªæ–‡æœ¬, ç»´åº¦{dimension}")
            print(
                f"âœ… ç¼–ç è€—æ—¶: {encode_time * 1000:.2f}ms ({encode_time * 1000 / len(test_texts):.2f}ms/æ–‡æœ¬)"
            )
            return success
        except Exception as e:
            print(f"âŒ ç¼–ç å¤±è´¥: {e}")
            self.results["embedding"] = {"success": False, "error": str(e)}
            return False

    def test_vector_db(self):
        """æµ‹è¯•å‘é‡æ•°æ®åº“"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•4ã€‘å‘é‡æ•°æ®åº“æµ‹è¯•")
        print("=" * 70)

        if not embedding_service.is_loaded():
            print("âš ï¸ åµŒå…¥æœåŠ¡æœªåŠ è½½ï¼Œè·³è¿‡å‘é‡æ•°æ®åº“æµ‹è¯•")
            return False

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        try:
            dimension = embedding_service.get_dimension()
            config = VectorDBConfig(
                db_type=VectorDBType.FAISS, dimension=dimension, index_type="HNSW"
            )
            success = vector_db_manager.initialize(config)
            if not success:
                print("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¼‚å¸¸: {e}")
            return False

        # æµ‹è¯•æ·»åŠ å‘é‡
        test_texts = ["æµ‹è¯•æ–‡æ¡£1", "æµ‹è¯•æ–‡æ¡£2", "æµ‹è¯•æ–‡æ¡£3"]
        try:
            vectors = embedding_service.encode(test_texts)
            metadata = [
                {"text": t, "id": f"test_{i}"} for i, t in enumerate(test_texts)
            ]
            vector_db_manager.add_vectors(vectors, metadata)
            print(f"âœ… æ·»åŠ å‘é‡æˆåŠŸ: {len(test_texts)}ä¸ª")
        except Exception as e:
            print(f"âŒ æ·»åŠ å‘é‡å¤±è´¥: {e}")
            return False

        # æµ‹è¯•æœç´¢
        try:
            query_vector = embedding_service.encode(["æµ‹è¯•æŸ¥è¯¢"])
            distances, results = vector_db_manager.search(query_vector, top_k=3)

            self.results["vector_db"] = {
                "success": True,
                "total_vectors": vector_db_manager.get_status().total_vectors,
                "search_results_count": len(results[0]) if results else 0,
            }

            print(f"âœ… æœç´¢æˆåŠŸ: è¿”å›{len(results[0]) if results else 0}ä¸ªç»“æœ")
            return True
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            self.results["vector_db"] = {"success": False, "error": str(e)}
            return False

    # ==================== 2. æ•ˆæœæµ‹è¯• ====================

    def test_retrieval_quality(self):
        """æµ‹è¯•æ£€ç´¢è´¨é‡"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•5ã€‘æ£€ç´¢è´¨é‡æµ‹è¯•ï¼ˆéœ€è¦é¢„è®¾æµ‹è¯•é›†ï¼‰")
        print("=" * 70)

        # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢å’ŒæœŸæœ›ç»“æœ
        test_cases = [
            {
                "query": "8-9çº§å‘˜å·¥å‡ºå·®ä½å®¿æ ‡å‡†",
                "expected_keywords": ["ä½å®¿", "æ ‡å‡†", "å‘˜å·¥"],
            },
            {"query": "å·®æ—…è´¹æŠ¥é”€æµç¨‹", "expected_keywords": ["æŠ¥é”€", "å·®æ—…", "æµç¨‹"]},
            {
                "query": "ç»ç†çº§åˆ«äº¤é€šè´¹ç”¨",
                "expected_keywords": ["ç»ç†", "äº¤é€š", "è´¹ç”¨"],
            },
        ]

        results = []
        config = RetrievalConfig(top_k=5)

        for case in test_cases:
            try:
                response = retriever.retrieve(case["query"], config)
                retrieved_texts = [r.content for r in response.results]

                # æ£€æŸ¥å…³é”®è¯å‘½ä¸­ç‡
                hits = 0
                for keyword in case["expected_keywords"]:
                    if any(keyword in text for text in retrieved_texts):
                        hits += 1

                hit_rate = hits / len(case["expected_keywords"])
                results.append(
                    {
                        "query": case["query"],
                        "hit_rate": hit_rate,
                        "results_count": len(response.results),
                    }
                )

                status = "âœ…" if hit_rate >= 0.6 else "âš ï¸"
                print(
                    f"{status} æŸ¥è¯¢: {case['query'][:30]}... å‘½ä¸­ç‡: {hit_rate * 100:.0f}%"
                )
            except Exception as e:
                results.append({"query": case["query"], "error": str(e)})
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {case['query'][:30]}... - {e}")

        avg_hit_rate = np.mean([r["hit_rate"] for r in results if "hit_rate" in r])
        self.results["retrieval_quality"] = {
            "avg_hit_rate": avg_hit_rate,
            "details": results,
        }

        print(f"\nå¹³å‡å…³é”®è¯å‘½ä¸­ç‡: {avg_hit_rate * 100:.1f}%")
        return avg_hit_rate >= 0.5

    def test_end_to_end(self):
        """ç«¯åˆ°ç«¯æµ‹è¯•"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•6ã€‘ç«¯åˆ°ç«¯RAGæµ‹è¯•")
        print("=" * 70)

        test_queries = [
            "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯",
            "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢æ€§èƒ½",
        ]

        results = []
        for query in test_queries:
            try:
                start_time = time.time()

                # æ‰§è¡Œå®Œæ•´RAGæµç¨‹
                retrieval_config = RetrievalConfig(top_k=3)
                generation_config = GenerationConfig(
                    llm_provider="local", temperature=0.7, max_tokens=300
                )

                response = rag_generator.generate(
                    query, retrieval_config, generation_config
                )
                total_time = time.time() - start_time

                success = len(response.answer) > 50  # å›ç­”é•¿åº¦æ£€æŸ¥
                results.append(
                    {
                        "query": query,
                        "success": success,
                        "total_time": total_time,
                        "retrieval_time": response.retrieval_time_ms / 1000,
                        "generation_time": response.generation_time_ms / 1000,
                        "answer_length": len(response.answer),
                        "sources_count": len(response.sources),
                    }
                )

                status = "âœ…" if success else "âŒ"
                print(f"{status} æŸ¥è¯¢: {query[:30]}...")
                print(
                    f"   æ€»è€—æ—¶: {total_time * 1000:.0f}ms (æ£€ç´¢{response.retrieval_time_ms:.0f}ms + ç”Ÿæˆ{response.generation_time_ms:.0f}ms)"
                )
                print(
                    f"   å›ç­”é•¿åº¦: {len(response.answer)}å­—ç¬¦, æ¥æº: {len(response.sources)}ä¸ª"
                )
            except Exception as e:
                results.append({"query": query, "success": False, "error": str(e)})
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {query[:30]}... - {e}")

        success_rate = sum(1 for r in results if r.get("success")) / len(results)
        self.results["end_to_end"] = {"success_rate": success_rate, "details": results}

        return success_rate >= 0.5

    # ==================== 3. æ€§èƒ½æµ‹è¯• ====================

    def test_retrieval_performance(self):
        """æµ‹è¯•æ£€ç´¢æ€§èƒ½"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•7ã€‘æ£€ç´¢æ€§èƒ½æµ‹è¯•")
        print("=" * 70)

        test_queries = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            "å¦‚ä½•ä½¿ç”¨Python",
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "æ·±åº¦å­¦ä¹ åŸç†",
            "æ•°æ®åˆ†ææ–¹æ³•",
        ] * 4  # 20æ¬¡æŸ¥è¯¢

        config = RetrievalConfig(top_k=5)
        times = []

        # é¢„çƒ­
        retriever.retrieve("é¢„çƒ­æŸ¥è¯¢", config)

        for query in test_queries:
            start = time.time()
            retriever.retrieve(query, config)
            times.append((time.time() - start) * 1000)

        avg_time = np.mean(times)
        p95_time = np.percentile(times, 95)
        p99_time = np.percentile(times, 99)

        self.results["retrieval_performance"] = {
            "avg_time_ms": avg_time,
            "p95_time_ms": p95_time,
            "p99_time_ms": p99_time,
            "throughput_qps": 1000 / avg_time if avg_time > 0 else 0,
        }

        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
        print(f"âœ… P95å“åº”æ—¶é—´: {p95_time:.2f}ms")
        print(f"âœ… P99å“åº”æ—¶é—´: {p99_time:.2f}ms")
        print(f"âœ… ä¼°ç®—ååé‡: {1000 / avg_time:.1f} QPS")

        return avg_time < 1000  # å¹³å‡1ç§’å†…

    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print("\n" + "=" * 70)
        print("ã€æµ‹è¯•8ã€‘å¹¶å‘æ€§èƒ½æµ‹è¯•")
        print("=" * 70)

        import concurrent.futures

        test_queries = ["æŸ¥è¯¢" + str(i) for i in range(20)]
        concurrent_users = 5

        def worker_task(queries):
            times = []
            config = RetrievalConfig(top_k=3)
            for query in queries:
                start = time.time()
                try:
                    retriever.retrieve(query, config)
                    times.append((time.time() - start) * 1000)
                except:
                    times.append(-1)
            return times

        # åˆ†é…æŸ¥è¯¢ç»™æ¯ä¸ªworker
        queries_per_worker = len(test_queries) // concurrent_users
        worker_queries = [
            test_queries[i * queries_per_worker : (i + 1) * queries_per_worker]
            for i in range(concurrent_users)
        ]

        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=concurrent_users
        ) as executor:
            futures = [executor.submit(worker_task, q) for q in worker_queries]
            all_times = []
            for future in concurrent.futures.as_completed(futures):
                all_times.extend(future.result())

        total_time = time.time() - start_time
        valid_times = [t for t in all_times if t > 0]

        throughput = len(test_queries) / total_time
        avg_time = np.mean(valid_times) if valid_times else 0

        self.results["concurrent_performance"] = {
            "concurrent_users": concurrent_users,
            "total_requests": len(test_queries),
            "total_time": total_time,
            "throughput_qps": throughput,
            "avg_response_time_ms": avg_time,
        }

        print(f"âœ… å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
        print(f"âœ… æ€»è¯·æ±‚æ•°: {len(test_queries)}")
        print(f"âœ… æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"âœ… ååé‡: {throughput:.1f} QPS")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")

        return throughput > 5  # è‡³å°‘5 QPS

    # ==================== 4. è¿è¡Œæ‰€æœ‰æµ‹è¯• ====================

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "=" * 70)
        print("RAGç³»ç»Ÿç»¼åˆæµ‹è¯•å¥—ä»¶")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)

        test_methods = [
            ("æ–‡æ¡£è§£æ", self.test_document_parser),
            ("æ–‡æ¡£åˆ‡åˆ†", self.test_chunking_strategies),
            ("åµŒå…¥æœåŠ¡", self.test_embedding_service),
            ("å‘é‡æ•°æ®åº“", self.test_vector_db),
            ("æ£€ç´¢è´¨é‡", self.test_retrieval_quality),
            ("ç«¯åˆ°ç«¯æµ‹è¯•", self.test_end_to_end),
            ("æ£€ç´¢æ€§èƒ½", self.test_retrieval_performance),
            ("å¹¶å‘æ€§èƒ½", self.test_concurrent_performance),
        ]

        passed = 0
        failed = 0

        for name, test_func in test_methods:
            try:
                result = test_func()
                if result:
                    passed += 1
                else:
                    failed += 1
            except Exception as e:
                print(f"\nâŒ {name}æµ‹è¯•å¼‚å¸¸: {e}")
                failed += 1

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(passed, failed)

        return passed, failed

    def _generate_report(self, passed, failed):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("=" * 70)
        print(f"é€šè¿‡: {passed}")
        print(f"å¤±è´¥: {failed}")
        print(f"æˆåŠŸç‡: {passed / (passed + failed) * 100:.1f}%")
        print("=" * 70)

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.test_data_dir / f"test_report_{int(time.time())}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "timestamp": datetime.now().isoformat(),
                    "summary": {"passed": passed, "failed": failed},
                    "results": self.results,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


# ==================== å¿«é€Ÿæµ‹è¯•å‡½æ•° ====================


def quick_test():
    """å¿«é€Ÿæµ‹è¯• - åªæµ‹è¯•æ ¸å¿ƒåŠŸèƒ½"""
    suite = RAGTestSuite()

    print("\nğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰\n")

    tests = [
        ("æ–‡æ¡£è§£æ", suite.test_document_parser),
        ("åµŒå…¥æœåŠ¡", suite.test_embedding_service),
        ("å‘é‡æ•°æ®åº“", suite.test_vector_db),
        ("æ£€ç´¢è´¨é‡", suite.test_retrieval_quality),
    ]

    passed = 0
    for name, test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"âŒ {name}æµ‹è¯•å¼‚å¸¸: {e}")

    print(f"\nå¿«é€Ÿæµ‹è¯•å®Œæˆ: {passed}/{len(tests)}é€šè¿‡")
    return passed == len(tests)


def benchmark_test():
    """åŸºå‡†æµ‹è¯• - é‡ç‚¹æµ‹è¯•æ€§èƒ½"""
    suite = RAGTestSuite()

    print("\nâš¡ åŸºå‡†æµ‹è¯•æ¨¡å¼ï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰\n")

    # ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
    suite.test_embedding_service()
    suite.test_vector_db()

    # æ€§èƒ½æµ‹è¯•
    suite.test_retrieval_performance()
    suite.test_concurrent_performance()

    print("\nåŸºå‡†æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿæµ‹è¯•å¥—ä»¶")
    parser.add_argument(
        "--mode",
        choices=["full", "quick", "benchmark"],
        default="full",
        help="æµ‹è¯•æ¨¡å¼",
    )
    args = parser.parse_args()

    if args.mode == "full":
        suite = RAGTestSuite()
        suite.run_all_tests()
    elif args.mode == "quick":
        quick_test()
    elif args.mode == "benchmark":
        benchmark_test()
