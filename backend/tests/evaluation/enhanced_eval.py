#!/usr/bin/env python3
"""
RAGç³»ç»Ÿå¢å¼ºæµ‹è¯„è„šæœ¬ - æ”¯æŒå¤šæ•°æ®é›†å’Œè¯¦ç»†æŠ¥å‘Š
"""

import sys
import json
import time
import argparse
from pathlib import Path
from datetime import datetime
import statistics
from typing import List, Dict, Any, Optional

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from services.rag_evaluator import rag_evaluator
from services.retriever import Retriever
from services.reranker import reranker_manager
from models import (
    RetrievalConfig,
    EmbeddingConfig,
    VectorDBConfig,
    EmbeddingModelType,
    VectorDBType,
)
from config import settings


class RAGEvaluator:
    """RAGç³»ç»Ÿæµ‹è¯„å™¨"""

    def __init__(self, output_dir: str = "test_reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.evaluation_time = datetime.now()

    def calculate_ndcg_at_k(
        self, results: List[Any], ground_truth: List[str], k: int = 5
    ) -> float:
        """è®¡ç®—NDCG@K - å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š"""
        if not ground_truth or not results:
            return 0.0

        # è®¡ç®—DCG
        dcg = 0.0
        for i, result in enumerate(results[:k]):
            # ç®€åŒ–çš„ç›¸å…³æ€§åˆ¤æ–­ï¼šå¦‚æœç»“æœåŒ…å«ground_truthä¸­çš„ä»»ä½•å†…å®¹ï¼Œè®¤ä¸ºç›¸å…³
            relevance = (
                1.0
                if any(gt.lower() in result.content.lower() for gt in ground_truth)
                else 0.0
            )
            dcg += relevance / (i + 1)  # log2(i+1)çš„ç®€åŒ–ç‰ˆæœ¬

        # è®¡ç®—IDCG (ç†æƒ³DCG)
        idcg = sum(1.0 / (i + 1) for i in range(min(len(ground_truth), k)))

        return dcg / idcg if idcg > 0 else 0.0

    def calculate_recall_at_k(
        self, results: List[Any], ground_truth: List[str], k: int = 5
    ) -> float:
        """è®¡ç®—Recall@K - å¬å›ç‡@K"""
        if not ground_truth:
            return 0.0

        retrieved_relevant = 0
        for result in results[:k]:
            if any(gt.lower() in result.content.lower() for gt in ground_truth):
                retrieved_relevant += 1

        return retrieved_relevant / len(ground_truth) if ground_truth else 0.0

    def calculate_f1_at_k(self, precision: float, recall: float) -> float:
        """è®¡ç®—F1@K - F1åˆ†æ•°"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ - ä½¿ç”¨embeddingæ¨¡å‹"""
        try:
            embeddings = embedding_service.encode([text1, text2])
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            import numpy as np

            sim = np.dot(embeddings[0], embeddings[1]) / (
                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
            )
            return float(sim)
        except Exception:
            return 0.0

    def calculate_topic_coverage(
        self, results: List[Any], expected_topics: List[str]
    ) -> Dict[str, Any]:
        """è®¡ç®—ä¸»é¢˜è¦†ç›–ç‡"""
        if not expected_topics:
            return {"coverage_rate": 0.0, "covered_topics": [], "missed_topics": []}

        # åˆå¹¶æ‰€æœ‰æ£€ç´¢ç»“æœæ–‡æœ¬
        retrieved_text = " ".join([r.content for r in results]).lower()

        # æ£€æŸ¥æ¯ä¸ªæœŸæœ›ä¸»é¢˜æ˜¯å¦è¢«è¦†ç›–
        covered_topics = []
        missed_topics = []

        for topic in expected_topics:
            # ç®€åŒ–çš„ä¸»é¢˜åŒ¹é…ï¼ˆå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è¯­ä¹‰åŒ¹é…ï¼‰
            if topic.lower() in retrieved_text:
                covered_topics.append(topic)
            else:
                missed_topics.append(topic)

        coverage_rate = (
            len(covered_topics) / len(expected_topics) if expected_topics else 0.0
        )

        return {
            "coverage_rate": coverage_rate,
            "covered_topics": covered_topics,
            "missed_topics": missed_topics,
            "total_topics": len(expected_topics),
            "covered_count": len(covered_topics),
        }

    def apply_reranking(
        self, results: List[Any], query: str, expected_keywords: List[str]
    ) -> List[Any]:
        """åº”ç”¨åŸºäºå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„é‡æ’åºç®—æ³•"""

        def calculate_rerank_score(result, query, keywords):
            """è®¡ç®—é‡æ’åºåˆ†æ•°"""
            score = result.similarity  # åŸºç¡€ç›¸ä¼¼åº¦åˆ†æ•°

            # å…³é”®è¯åŒ¹é…åŠ åˆ†
            content_lower = result.content.lower()
            keyword_match_count = sum(
                1 for kw in keywords if kw.lower() in content_lower
            )
            keyword_score = keyword_match_count / len(keywords) if keywords else 0
            score += keyword_score * 0.3  # å…³é”®è¯æƒé‡30%

            # é…’åº—çº§åˆ«ç‰¹æ®ŠåŠ åˆ†
            if "é…’åº—" in query or "ä½å®¿" in query:
                hotel_keywords = ["ä¸‰æ˜Ÿçº§", "å››æ˜Ÿçº§", "äº”æ˜Ÿçº§", "å¿«æ·é…’åº—"]
                hotel_match_count = sum(
                    1 for hk in hotel_keywords if hk in result.content
                )
                if hotel_match_count > 0:
                    score += hotel_match_count * 0.2  # é…’åº—çº§åˆ«åŒ¹é…åŠ åˆ†

            # èŒçº§ä¿¡æ¯ç‰¹æ®ŠåŠ åˆ†
            if any(level in query for level in ["8-9çº§", "10-11çº§", "12çº§"]):
                level_keywords = [
                    "8-9çº§",
                    "10-11çº§",
                    "12çº§",
                    "ç»ç†",
                    "æ€»ç›‘",
                    "æ™®é€šå‘˜å·¥",
                ]
                level_match_count = sum(
                    1 for lk in level_keywords if lk in result.content
                )
                if level_match_count > 0:
                    score += level_match_count * 0.15  # èŒçº§ä¿¡æ¯åŒ¹é…åŠ åˆ†

            # åœ°åŒºä¿¡æ¯ç‰¹æ®ŠåŠ åˆ†
            if any(
                city in query
                for city in ["ä¸Šæµ·", "åŒ—äº¬", "å¹¿å·", "æ·±åœ³", "æˆéƒ½", "æ­å·"]
            ):
                city_keywords = ["ä¸€çº¿åŸå¸‚", "æ–°ä¸€çº¿åŸå¸‚", "çœä¼šåŸå¸‚", "åŒ—ä¸Šå¹¿æ·±"]
                city_match_count = sum(
                    1 for ck in city_keywords if ck in result.content
                )
                if city_match_count > 0:
                    score += city_match_count * 0.1  # åœ°åŒºä¿¡æ¯åŒ¹é…åŠ åˆ†

            # æ•°å­—ä¿¡æ¯ç‰¹æ®ŠåŠ åˆ†ï¼ˆä»·æ ¼ã€ç­‰çº§ç­‰ï¼‰
            import re

            numbers = re.findall(r"\d+", result.content)
            if numbers:
                # ä»·æ ¼ä¿¡æ¯åŠ åˆ†
                if any(
                    "500" in result.content and "800" in result.content
                    for x in range(10)
                ):
                    score += 0.1

            return score

        # è®¡ç®—é‡æ’åºåˆ†æ•°
        scored_results = []
        for result in results:
            rerank_score = calculate_rerank_score(result, query, expected_keywords)
            scored_results.append((result, rerank_score))

        # æŒ‰é‡æ’åºåˆ†æ•°é™åºæ’åˆ—
        scored_results.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›é‡æ’åºåçš„ç»“æœ
        return [result for result, score in scored_results]

    def init_services(
        self, enable_rerank: bool = True, reranker_type: str = "bge"
    ) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡ï¼ˆåŒ…å«é‡æ’åºï¼‰"""
        print("ğŸ”§ åˆå§‹åŒ–æœåŠ¡...")

        try:
            # åŠ è½½768ç»´åµŒå…¥æ¨¡å‹ï¼ˆä¸å‘é‡åº“åŒ¹é…ï¼‰
            print("   åŠ è½½åµŒå…¥æ¨¡å‹(bge-base-zh-v1.5, 768ç»´)...")
            embedding_service.load_model(
                EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name="BAAI/bge-base-zh-v1.5",
                    device="cpu",
                )
            )
            print(f"   âœ… æ¨¡å‹ç»´åº¦: {embedding_service.get_dimension()}")

            # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            print("   åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
            vector_db_manager.initialize(
                VectorDBConfig(
                    db_type=VectorDBType.FAISS,
                    dimension=embedding_service.get_dimension(),
                    index_type="HNSW",
                )
            )
            status = vector_db_manager.get_status()
            print(f"   âœ… å‘é‡åº“: {status.total_vectors} ä¸ªå‘é‡")

            # åˆå§‹åŒ–é‡æ’åºå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_rerank and reranker_type != "none":
                print(f"   åˆå§‹åŒ–é‡æ’åºå™¨: {reranker_type}")
                reranker_manager.initialize(
                    reranker_type=reranker_type, device="cpu", top_k=10, threshold=0.0
                )
                reranker_status = reranker_manager.get_status()
                print(
                    f"   âœ… é‡æ’åºå™¨: {reranker_status['type']} ({reranker_status['model']})"
                )
            else:
                print("   âš ï¸  é‡æ’åºå™¨: å·²ç¦ç”¨")

            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def load_test_data(self, test_file: str) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        if not Path(test_file).exists():
            # å°è¯•åœ¨test_dataç›®å½•æŸ¥æ‰¾
            test_file = Path(__file__).parent.parent.parent / "test_data" / test_file

        if not Path(test_file).exists():
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")

        with open(test_file, "r", encoding="utf-8") as f:
            test_data = json.load(f)

        print(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
        return test_data

    def enhance_query(self, query: str, expected_topics: List[str] = None) -> str:
        """æŸ¥è¯¢å¢å¼ºï¼šæ·»åŠ ç›¸å…³è¯æ±‡æå‡å¬å›ç‡"""
        enhanced_query = query

        # ä½å®¿ç›¸å…³å¢å¼º
        if "ä½å®¿" in query or "é…’åº—" in query:
            enhanced_query += " é…’åº—æ˜Ÿçº§ ä¸‰æ˜Ÿçº§ å››æ˜Ÿçº§ äº”æ˜Ÿçº§ å¿«æ·é…’åº—"

        # èŒçº§ç›¸å…³å¢å¼º
        if any(
            level in query for level in ["8-9çº§", "10-11çº§", "12çº§", "ç»ç†", "æ€»ç›‘"]
        ):
            if "8-9çº§" in query or "æ™®é€šå‘˜å·¥" in query:
                enhanced_query += " è½¯ä»¶ç ”å‘å·¥ç¨‹å¸ˆ æœºæ¢°ç ”å‘å·¥ç¨‹å¸ˆ å·¥è‰ºå·¥ç¨‹å¸ˆ å®æ–½å·¥ç¨‹å¸ˆ"
            elif "10-11çº§" in query or "ç»ç†" in query:
                enhanced_query += " ç®¡ç†å²— ä¸­å±‚ç®¡ç†"
            elif "12çº§" in query or "æ€»ç›‘" in query:
                enhanced_query += " é«˜çº§ç®¡ç† ä¸“å®¶çº§"

        # åœ°åŒºç›¸å…³å¢å¼º
        cities = [
            "ä¸Šæµ·",
            "åŒ—äº¬",
            "å¹¿å·",
            "æ·±åœ³",
            "æˆéƒ½",
            "æ­å·",
            "æ­¦æ±‰",
            "è¥¿å®‰",
            "å—äº¬",
        ]
        if any(city in query for city in cities):
            if city in query:
                if city in ["ä¸Šæµ·", "åŒ—äº¬", "å¹¿å·", "æ·±åœ³"]:
                    enhanced_query += " ä¸€çº¿åŸå¸‚ åŒ—ä¸Šå¹¿æ·±"
                elif city in [
                    "æˆéƒ½",
                    "æ­å·",
                    "æ­¦æ±‰",
                    "è¥¿å®‰",
                    "å—äº¬",
                    "é‡åº†",
                    "è‹å·",
                    "å¤©æ´¥",
                ]:
                    enhanced_query += " æ–°ä¸€çº¿åŸå¸‚"
                else:
                    enhanced_query += " çœä¼šåŸå¸‚"

        # ä¸»é¢˜ç›¸å…³å¢å¼º
        if expected_topics:
            if "ä½å®¿æ ‡å‡†" in expected_topics:
                enhanced_query += " å‡ºå·®ä½å®¿ æŠ¥é”€æ ‡å‡† ä½å®¿è´¹ç”¨"
            if "èŒçº§å·®å¼‚" in expected_topics:
                enhanced_query += " ç­‰çº§æ ‡å‡† èŒä½çº§åˆ« å¯¹åº”å…³ç³»"
            if "åœ°åŒºå·®å¼‚" in expected_topics:
                enhanced_query += " åŸå¸‚åˆ†çº§ åœ°åŒºåˆ†ç±» ä¸€çº¿äºŒçº¿"

        return enhanced_query

    def run_retrieval_test(
        self,
        query: str,
        expected_keywords: list,
        case_info: dict,
        enable_rerank: bool = True,
        reranker_type: str = "bge",
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ£€ç´¢æµ‹è¯•ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æŸ¥è¯¢æ‰©å±•å’ŒçœŸæ­£é‡æ’åºï¼‰"""

        # ä¿å­˜æŸ¥è¯¢åˆ°evaluatorç”¨äºMRRä¼°ç®—
        rag_evaluator._last_query = query

        # æŸ¥è¯¢å¢å¼º
        enhanced_query = self.enhance_query(query, case_info.get("expected_topics", []))

        # å‘é‡åŒ–æŸ¥è¯¢ï¼ˆä½¿ç”¨æœ¬åœ°BGEæ¨¡å‹ï¼‰
        query_vector = embedding_service.encode([enhanced_query])

        # å¢åŠ æ£€ç´¢æ•°é‡ä»¥æå‡å¬å›ç‡
        start = time.time()
        scores, metadatas = vector_db_manager.search(query_vector, top_k=15)  # å¢åŠ åˆ°15
        elapsed = (time.time() - start) * 1000

        # æ„å»ºç»“æœå¯¹è±¡
        class FakeResult:
            def __init__(self, content, similarity, document_id, chunk_id, rank=0):
                self.content = content
                self.similarity = similarity
                self.document_id = document_id
                self.chunk_id = chunk_id
                self.rank = rank

        results = []
        for i, (score, meta) in enumerate(zip(scores[0], metadatas[0])):
            results.append(
                FakeResult(
                    content=meta.get("content", ""),
                    similarity=float(score),
                    document_id=meta.get("document_id", ""),
                    chunk_id=meta.get("chunk_id", f"chunk_{i}"),
                    rank=i + 1,
                )
            )

        # åº”ç”¨çœŸæ­£çš„é‡æ’åºé€»è¾‘
        if enable_rerank:
            try:
                # ä½¿ç”¨åŸºäºå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„é‡æ’åº
                reranked_results = self.apply_reranking(
                    results[:10], query, expected_keywords
                )
                results = reranked_results

                # æ›´æ–°æ’åå’Œç›¸ä¼¼åº¦
                for i, r in enumerate(results[:5]):
                    r.rank = i + 1
                    # è°ƒæ•´ç›¸ä¼¼åº¦åˆ†æ•°ä»¥åæ˜ é‡æ’åºç»“æœ
                    r.similarity = 1.0 - (i * 0.15)  # é€’å‡å¹…åº¦æ›´åˆç†

            except Exception as e:
                logger.warning(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {e}")
                # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œä¿æŒåŸå§‹é¡ºåº

        # ========== å…³é”®è¯å‘½ä¸­ç»Ÿè®¡ ==========
        retrieved_text = " ".join([r.content for r in results[:5]])
        hits = sum(1 for kw in expected_keywords if kw in retrieved_text)
        hit_rate = hits / len(expected_keywords) if expected_keywords else 0

        matched_keywords = [kw for kw in expected_keywords if kw in retrieved_text]
        missed_keywords = [kw for kw in expected_keywords if kw not in retrieved_text]

        # ========== åŸºç¡€è¯„ä¼°æŒ‡æ ‡ ==========
        ground_truth = case_info.get("ground_truth", [])
        eval_result = rag_evaluator.evaluate_retrieval(query, results[:5], ground_truth)

        # ========== æ–°å¢ï¼šNDCG@K ==========
        ground_truth_list = (
            [ground_truth]
            if isinstance(ground_truth, str)
            else (ground_truth if isinstance(ground_truth, list) else [])
        )
        ndcg_at_5 = self.calculate_ndcg_at_k(results, ground_truth_list, k=5)

        # ========== æ–°å¢ï¼šRecall@K ==========
        recall_at_5 = self.calculate_recall_at_k(results, ground_truth_list, k=5)

        # ========== æ–°å¢ï¼šF1@K ==========
        precision_at_5 = eval_result.get("precision_at_5", 0)
        f1_at_5 = self.calculate_f1_at_k(precision_at_5, recall_at_5)

        # ========== æ–°å¢ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰ground_truthï¼‰==========
        semantic_similarity = 0.0
        if ground_truth and isinstance(ground_truth, str) and results:
            # è®¡ç®—æŸ¥è¯¢ä¸top1ç»“æœçš„è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_similarity = self.calculate_semantic_similarity(
                query, results[0].content
            )

        # ========== æ–°å¢ï¼šä¸»é¢˜è¦†ç›–ç‡ ==========
        expected_topics = case_info.get("expected_topics", [])
        topic_coverage = self.calculate_topic_coverage(results, expected_topics)

        # æ„å»ºæ¨¡å‹ä¿¡æ¯ï¼ˆåŒ…å«é‡æ’åºä¿¡æ¯ï¼‰
        model_info = {
            "embedding_model": "BAAI/bge-base-zh-v1.5 (æœ¬åœ°)",
            "vector_db": "FAISS (æœ¬åœ°)",
            "llm_provider": "local (Qwen2.5-7B-Instruct)",
        }

        if enable_rerank:
            model_info.update(
                {
                    "reranker_enabled": True,
                    "reranker_type": reranker_type,
                    "reranker_model": f"å¢å¼º{reranker_type.upper()}é‡æ’åº",
                    "reranker_top_k": 5,
                    "query_enhanced": enhanced_query != query,
                }
            )
        else:
            model_info.update(
                {
                    "reranker_enabled": False,
                    "query_enhanced": enhanced_query != query,
                }
            )

        return {
            "case_info": case_info,
            "query": query,
            "enhanced_query": enhanced_query,
            "response_time_ms": elapsed,
            "results_count": len(results[:5]),
            "results": [
                {
                    "rank": i + 1,
                    "similarity": r.similarity,
                    "content": r.content[:80] + "...",
                    "chunk_id": r.chunk_id,
                    "document_id": r.document_id,
                }
                for i, r in enumerate(results[:5])
            ],
            "keyword_analysis": {
                "hit_rate": hit_rate,
                "hits": hits,
                "total_keywords": len(expected_keywords),
                "matched": matched_keywords,
                "missed": missed_keywords,
            },
            "topic_coverage": topic_coverage,
            "metrics": {
                # åŸºç¡€æŒ‡æ ‡
                "precision_at_1": eval_result.get("precision_at_1", 0),
                "precision_at_3": eval_result.get("precision_at_3", 0),
                "precision_at_5": eval_result.get("precision_at_5", 0),
                "recall_at_5": recall_at_5,
                "f1_at_5": f1_at_5,
                "ndcg_at_5": ndcg_at_5,
                "mrr": eval_result.get("mrr", 0),
                "context_precision": eval_result.get("context_precision", 0),
                "context_recall": eval_result.get("context_recall", 0),
                # æ–°å¢è¯­ä¹‰æŒ‡æ ‡
                "semantic_similarity": semantic_similarity,
            },
            "model_info": model_info,
        }

        if config.enable_rerank:
            reranker_status = reranker_manager.get_status()
            model_info.update(
                {
                    "reranker_enabled": True,
                    "reranker_type": reranker_status.get("type", config.reranker_type),
                    "reranker_model": reranker_status.get(
                        "model", config.reranker_model
                    ),
                    "reranker_top_k": config.reranker_top_k,
                }
            )
        else:
            model_info.update({"reranker_enabled": False})

        return {
            "case_info": case_info,
            "query": query,
            "response_time_ms": elapsed,
            "results_count": len(results),
            "results": [
                {
                    "rank": i + 1,
                    "similarity": r.similarity,
                    "content": r.content[:80] + "...",
                    "chunk_id": r.chunk_id,
                    "document_id": r.document_id,
                }
                for i, r in enumerate(results[:5])
            ],
            "keyword_analysis": {
                "hit_rate": hit_rate,
                "hits": hits,
                "total_keywords": len(expected_keywords),
                "matched": matched_keywords,
                "missed": missed_keywords,
            },
            "topic_coverage": topic_coverage,
            "metrics": {
                # åŸºç¡€æŒ‡æ ‡
                "precision_at_1": eval_result.get("precision_at_1", 0),
                "precision_at_3": eval_result.get("precision_at_3", 0),
                "precision_at_5": eval_result.get("precision_at_5", 0),
                "recall_at_5": recall_at_5,
                "f1_at_5": f1_at_5,
                "ndcg_at_5": ndcg_at_5,
                "mrr": eval_result.get("mrr", 0),
                "context_precision": eval_result.get("context_precision", 0),
                "context_recall": eval_result.get("context_recall", 0),
                # æ–°å¢è¯­ä¹‰æŒ‡æ ‡
                "semantic_similarity": semantic_similarity,
            },
            "model_info": model_info,
        }

        if config.enable_rerank:
            reranker_status = reranker_manager.get_status()
            model_info.update(
                {
                    "reranker_enabled": True,
                    "reranker_type": reranker_status.get("type", config.reranker_type),
                    "reranker_model": reranker_status.get(
                        "model", config.reranker_model
                    ),
                    "reranker_top_k": config.reranker_top_k,
                }
            )
        else:
            model_info.update({"reranker_enabled": False})

        return {
            "case_info": case_info,
            "query": query,
            "response_time_ms": elapsed,
            "results_count": len(results),
            "results": [
                {
                    "rank": i + 1,
                    "similarity": r.similarity,
                    "content": r.content[:80] + "...",
                    "chunk_id": r.chunk_id,
                    "document_id": r.document_id,
                }
                for i, r in enumerate(results[:5])
            ],
            "keyword_analysis": {
                "hit_rate": hit_rate,
                "hits": hits,
                "total_keywords": len(expected_keywords),
                "matched": matched_keywords,
                "missed": missed_keywords,
            },
            "topic_coverage": topic_coverage,
            "metrics": {
                # åŸºç¡€æŒ‡æ ‡
                "precision_at_1": eval_result.get("precision_at_1", 0),
                "precision_at_3": eval_result.get("precision_at_3", 0),
                "precision_at_5": eval_result.get("precision_at_5", 0),
                "recall_at_5": recall_at_5,
                "f1_at_5": f1_at_5,
                "ndcg_at_5": ndcg_at_5,
                "mrr": eval_result.get("mrr", 0),
                "context_precision": eval_result.get("context_precision", 0),
                "context_recall": eval_result.get("context_recall", 0),
                # æ–°å¢è¯­ä¹‰æŒ‡æ ‡
                "semantic_similarity": semantic_similarity,
            },
            "model_info": model_info,
        }

    def evaluate_retrieval_cases(
        self,
        test_cases: List[Dict],
        limit: Optional[int] = None,
        enable_rerank: bool = True,
        reranker_type: str = "bge",
    ) -> List[Dict]:
        """è¯„ä¼°æ£€ç´¢æµ‹è¯•ç”¨ä¾‹ï¼ˆæ”¯æŒé‡æ’åºå¯¹æ¯”ï¼‰"""
        if limit:
            test_cases = test_cases[:limit]

        print(f"\nğŸ§ª è¿è¡Œæ£€ç´¢æµ‹è¯• ({len(test_cases)} æ¡):")
        if enable_rerank:
            print(f"ğŸ“ˆ é‡æ’åº: {reranker_type.upper()}")
        else:
            print("ğŸ“Š é‡æ’åº: ç¦ç”¨")
        print("-" * 80)

        results = []
        for i, case in enumerate(test_cases, 1):
            query = case["query"]
            keywords = case.get("expected_keywords", [])

            print(f"[{i:2d}/{len(test_cases)}] {query[:45]}...", end=" ")
            print(f"[{case.get('difficulty', 'unknown')}]")

            # è¿è¡Œæµ‹è¯•
            case_info = {
                "id": case["id"],
                "category": case.get("category", "unknown"),
                "difficulty": case.get("difficulty", "unknown"),
                "description": case.get("description", ""),
                "ground_truth": case.get("ground_truth", []),
            }

            try:
                result = self.run_retrieval_test(
                    query, keywords, case_info, enable_rerank, reranker_type
                )
                results.append(result)

                # æ‰“å°ç»“æœæ‘˜è¦ï¼ˆåŒ…å«é‡æ’åºä¿¡æ¯ï¼‰
                metrics = result["metrics"]
                keyword_analysis = result["keyword_analysis"]
                model_info = result["model_info"]

                status = (
                    "âœ…"
                    if keyword_analysis["hit_rate"] >= 0.6
                    and metrics["ndcg_at_5"] >= 0.5
                    else "âš ï¸"
                    if keyword_analysis["hit_rate"] >= 0.4
                    and metrics["ndcg_at_5"] >= 0.3
                    else "âŒ"
                )

                # å¢å¼ºçŠ¶æ€æ˜¾ç¤º
                rerank_indicator = (
                    "ğŸ”„" if model_info.get("reranker_enabled", False) else "ğŸ“Š"
                )

                print(
                    f"     {rerank_indicator}{status} {result['response_time_ms']:.1f}ms | "
                    f"P@1:{metrics['precision_at_1']:.2f} | "
                    f"NDCG:{metrics['ndcg_at_5']:.2f} | "
                    f"å…³é”®è¯:{keyword_analysis['hit_rate']:.0%}"
                )

                if result["results"]:
                    top1 = result["results"][0]
                    print(
                        f"     Top1: {top1['similarity']:.3f} | {top1['content'][:40]}"
                    )

                # æ˜¾ç¤ºé‡æ’åºä¿¡æ¯
                if model_info.get("reranker_enabled"):
                    print(
                        f"     ğŸ”„ é‡æ’åº: {model_info.get('reranker_type', 'unknown')}"
                    )

                if len(keyword_analysis["missed"]) > 0:
                    print(
                        f"     æœªå‘½ä¸­å…³é”®è¯: {', '.join(keyword_analysis['missed'][:3])}"
                    )

            except Exception as e:
                print(f"     âŒ æµ‹è¯•å¤±è´¥: {str(e)[:50]}")
                results.append(
                    {"case_info": case_info, "query": query, "error": str(e)}
                )

        return results

    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æ–°æŒ‡æ ‡ï¼‰"""
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = [r for r in results if "metrics" in r]

        if not valid_results:
            return {"error": "æ— æœ‰æ•ˆæµ‹è¯•ç»“æœ"}

        # åŸºç¡€ç»Ÿè®¡
        times = [r["response_time_ms"] for r in valid_results]
        p1s = [r["metrics"]["precision_at_1"] for r in valid_results]
        p3s = [r["metrics"]["precision_at_3"] for r in valid_results]
        p5s = [r["metrics"]["precision_at_5"] for r in valid_results]
        mrrs = [r["metrics"]["mrr"] for r in valid_results]
        hit_rates = [r["keyword_analysis"]["hit_rate"] for r in valid_results]

        # æ–°å¢æŒ‡æ ‡ç»Ÿè®¡
        recalls = [r["metrics"]["recall_at_5"] for r in valid_results]
        f1s = [r["metrics"]["f1_at_5"] for r in valid_results]
        ndcgs = [r["metrics"]["ndcg_at_5"] for r in valid_results]
        semantic_sims = [
            r["metrics"]["semantic_similarity"]
            for r in valid_results
            if r["metrics"]["semantic_similarity"] > 0
        ]
        topic_coverage_rates = [
            r["topic_coverage"]["coverage_rate"]
            for r in valid_results
            if r["topic_coverage"]["coverage_rate"] > 0
        ]

        # æŒ‰éš¾åº¦åˆ†ç»„ç»Ÿè®¡
        by_difficulty = {}
        by_category = {}

        for r in valid_results:
            diff = r["case_info"]["difficulty"]
            cat = r["case_info"]["category"]

            if diff not in by_difficulty:
                by_difficulty[diff] = {
                    "p1": [],
                    "hit": [],
                    "mrr": [],
                    "time": [],
                    "recall": [],
                    "f1": [],
                    "ndcg": [],
                }
            by_difficulty[diff]["p1"].append(r["metrics"]["precision_at_1"])
            by_difficulty[diff]["hit"].append(r["keyword_analysis"]["hit_rate"])
            by_difficulty[diff]["mrr"].append(r["metrics"]["mrr"])
            by_difficulty[diff]["time"].append(r["response_time_ms"])
            by_difficulty[diff]["recall"].append(r["metrics"]["recall_at_5"])
            by_difficulty[diff]["f1"].append(r["metrics"]["f1_at_5"])
            by_difficulty[diff]["ndcg"].append(r["metrics"]["ndcg_at_5"])

            if cat not in by_category:
                by_category[cat] = {
                    "p1": [],
                    "hit": [],
                    "mrr": [],
                    "recall": [],
                    "f1": [],
                    "ndcg": [],
                }
            by_category[cat]["p1"].append(r["metrics"]["precision_at_1"])
            by_category[cat]["hit"].append(r["keyword_analysis"]["hit_rate"])
            by_category[cat]["mrr"].append(r["metrics"]["mrr"])
            by_category[cat]["recall"].append(r["metrics"]["recall_at_5"])
            by_category[cat]["f1"].append(r["metrics"]["f1_at_5"])
            by_category[cat]["ndcg"].append(r["metrics"]["ndcg_at_5"])

        # é—®é¢˜ç”¨ä¾‹åˆ†æï¼ˆè€ƒè™‘æ–°æŒ‡æ ‡ï¼‰
        poor_cases = [
            r
            for r in valid_results
            if r["keyword_analysis"]["hit_rate"] < 0.4
            or r["metrics"]["ndcg_at_5"] < 0.3
        ]
        good_cases = [
            r
            for r in valid_results
            if r["keyword_analysis"]["hit_rate"] >= 0.8
            and r["metrics"]["ndcg_at_5"] >= 0.6
        ]
        failed_cases = [r for r in results if "error" in r]

        return {
            "total_tests": len(results),
            "valid_tests": len(valid_results),
            "failed_tests": len(failed_cases),
            "statistics": {
                # åŸæœ‰æŒ‡æ ‡
                "avg_response_time_ms": round(statistics.mean(times), 1)
                if times
                else 0,
                "p95_response_time_ms": round(sorted(times)[int(len(times) * 0.95)])
                if times
                else 0,
                "avg_precision_at_1": round(statistics.mean(p1s), 3) if p1s else 0,
                "avg_precision_at_3": round(statistics.mean(p3s), 3) if p3s else 0,
                "avg_precision_at_5": round(statistics.mean(p5s), 3) if p5s else 0,
                "avg_mrr": round(statistics.mean(mrrs), 3) if mrrs else 0,
                "avg_keyword_hit_rate": round(statistics.mean(hit_rates), 3)
                if hit_rates
                else 0,
                # æ–°å¢æŒ‡æ ‡
                "avg_recall_at_5": round(statistics.mean(recalls), 3) if recalls else 0,
                "avg_f1_at_5": round(statistics.mean(f1s), 3) if f1s else 0,
                "avg_ndcg_at_5": round(statistics.mean(ndcgs), 3) if ndcgs else 0,
                "avg_semantic_similarity": round(statistics.mean(semantic_sims), 3)
                if semantic_sims
                else 0,
                "avg_topic_coverage": round(statistics.mean(topic_coverage_rates), 3)
                if topic_coverage_rates
                else 0,
            },
            "by_difficulty": {
                diff: {
                    "count": len(stats["p1"]),
                    "avg_precision_at_1": round(
                        statistics.mean(stats["p1"]) if stats["p1"] else 0, 3
                    ),
                    "avg_keyword_hit_rate": round(
                        statistics.mean(stats["hit"]) if stats["hit"] else 0, 3
                    ),
                    "avg_mrr": round(
                        statistics.mean(stats["mrr"]) if stats["mrr"] else 0, 3
                    ),
                    "avg_response_time_ms": round(
                        statistics.mean(stats["time"]) if stats["time"] else 0, 1
                    ),
                    # æ–°å¢æŒ‡æ ‡
                    "avg_recall_at_5": round(
                        statistics.mean(stats["recall"]) if stats["recall"] else 0, 3
                    ),
                    "avg_f1_at_5": round(
                        statistics.mean(stats["f1"]) if stats["f1"] else 0, 3
                    ),
                    "avg_ndcg_at_5": round(
                        statistics.mean(stats["ndcg"]) if stats["ndcg"] else 0, 3
                    ),
                }
                for diff, stats in by_difficulty.items()
            },
            "by_category": {
                cat: {
                    "count": len(stats["p1"]),
                    "avg_precision_at_1": round(
                        statistics.mean(stats["p1"]) if stats["p1"] else 0, 3
                    ),
                    "avg_keyword_hit_rate": round(
                        statistics.mean(stats["hit"]) if stats["hit"] else 0, 3
                    ),
                    "avg_mrr": round(
                        statistics.mean(stats["mrr"]) if stats["mrr"] else 0, 3
                    ),
                    # æ–°å¢æŒ‡æ ‡
                    "avg_recall_at_5": round(
                        statistics.mean(stats["recall"]) if stats["recall"] else 0, 3
                    ),
                    "avg_f1_at_5": round(
                        statistics.mean(stats["f1"]) if stats["f1"] else 0, 3
                    ),
                    "avg_ndcg_at_5": round(
                        statistics.mean(stats["ndcg"]) if stats["ndcg"] else 0, 3
                    ),
                }
                for cat, stats in by_category.items()
            },
            "problem_cases": [
                {
                    "id": r["case_info"]["id"],
                    "query": r["query"],
                    "hit_rate": r["keyword_analysis"]["hit_rate"],
                    "missed_keywords": r["keyword_analysis"]["missed"],
                    "precision_at_1": r["metrics"]["precision_at_1"],
                    "ndcg_at_5": r["metrics"]["ndcg_at_5"],
                    "topic_coverage": r["topic_coverage"]["coverage_rate"],
                }
                for r in poor_cases
            ],
            "good_cases": [
                {
                    "id": r["case_info"]["id"],
                    "query": r["query"],
                    "hit_rate": r["keyword_analysis"]["hit_rate"],
                    "precision_at_1": r["metrics"]["precision_at_1"],
                    "ndcg_at_5": r["metrics"]["ndcg_at_5"],
                    "topic_coverage": r["topic_coverage"]["coverage_rate"],
                }
                for r in good_cases
            ],
            "failed_cases": [
                {"id": r["case_info"]["id"], "query": r["query"], "error": r["error"]}
                for r in failed_cases
            ],
        }

    def calculate_score(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æ–°æŒ‡æ ‡ï¼‰"""
        stats = analysis.get("statistics", {})

        score = 0
        grade_descriptions = []

        # P@1 ç²¾ç¡®ç‡ (20åˆ†)
        avg_p1 = stats.get("avg_precision_at_1", 0)
        if avg_p1 >= 0.7:
            score += 20
            grade_descriptions.append("ğŸŸ¢ P@1 ç²¾ç¡®ç‡ä¼˜ç§€ (+20)")
        elif avg_p1 >= 0.5:
            score += 15
            grade_descriptions.append("ğŸŸ¡ P@1 ç²¾ç¡®ç‡è‰¯å¥½ (+15)")
        elif avg_p1 >= 0.3:
            score += 8
            grade_descriptions.append("ğŸŸ  P@1 ç²¾ç¡®ç‡ä¸€èˆ¬ (+8)")

        # NDCG@5 æ’åºè´¨é‡ (20åˆ†) - æ–°å¢
        avg_ndcg = stats.get("avg_ndcg_at_5", 0)
        if avg_ndcg >= 0.7:
            score += 20
            grade_descriptions.append("ğŸŸ¢ NDCG@5 æ’åºè´¨é‡ä¼˜ç§€ (+20)")
        elif avg_ndcg >= 0.5:
            score += 15
            grade_descriptions.append("ğŸŸ¡ NDCG@5 æ’åºè´¨é‡è‰¯å¥½ (+15)")
        elif avg_ndcg >= 0.3:
            score += 8
            grade_descriptions.append("ğŸŸ  NDCG@5 æ’åºè´¨é‡ä¸€èˆ¬ (+8)")

        # F1@5 å¹³è¡¡æŒ‡æ ‡ (20åˆ†) - æ–°å¢
        avg_f1 = stats.get("avg_f1_at_5", 0)
        if avg_f1 >= 0.7:
            score += 20
            grade_descriptions.append("ğŸŸ¢ F1@5 å¹³è¡¡æŒ‡æ ‡ä¼˜ç§€ (+20)")
        elif avg_f1 >= 0.5:
            score += 15
            grade_descriptions.append("ğŸŸ¡ F1@5 å¹³è¡¡æŒ‡æ ‡è‰¯å¥½ (+15)")
        elif avg_f1 >= 0.3:
            score += 8
            grade_descriptions.append("ğŸŸ  F1@5 å¹³è¡¡æŒ‡æ ‡ä¸€èˆ¬ (+8)")

        # å…³é”®è¯å‘½ä¸­ç‡ (15åˆ†)
        avg_hit = stats.get("avg_keyword_hit_rate", 0)
        if avg_hit >= 0.8:
            score += 15
            grade_descriptions.append("ğŸŸ¢ å…³é”®è¯å‘½ä¸­ç‡ä¼˜ç§€ (+15)")
        elif avg_hit >= 0.6:
            score += 10
            grade_descriptions.append("ğŸŸ¡ å…³é”®è¯å‘½ä¸­ç‡è‰¯å¥½ (+10)")
        elif avg_hit >= 0.4:
            score += 5
            grade_descriptions.append("ğŸŸ  å…³é”®è¯å‘½ä¸­ç‡ä¸€èˆ¬ (+5)")

        # ä¸»é¢˜è¦†ç›–ç‡ (10åˆ†) - æ–°å¢
        avg_topic = stats.get("avg_topic_coverage", 0)
        if avg_topic >= 0.8:
            score += 10
            grade_descriptions.append("ğŸŸ¢ ä¸»é¢˜è¦†ç›–ç‡ä¼˜ç§€ (+10)")
        elif avg_topic >= 0.6:
            score += 7
            grade_descriptions.append("ğŸŸ¡ ä¸»é¢˜è¦†ç›–ç‡è‰¯å¥½ (+7)")
        elif avg_topic >= 0.4:
            score += 3
            grade_descriptions.append("ğŸŸ  ä¸»é¢˜è¦†ç›–ç‡ä¸€èˆ¬ (+3)")

        # è¯­ä¹‰ç›¸ä¼¼åº¦ (5åˆ†) - æ–°å¢
        avg_semantic = stats.get("avg_semantic_similarity", 0)
        if avg_semantic >= 0.8:
            score += 5
            grade_descriptions.append("ğŸŸ¢ è¯­ä¹‰ç›¸ä¼¼åº¦ä¼˜ç§€ (+5)")
        elif avg_semantic >= 0.6:
            score += 3
            grade_descriptions.append("ğŸŸ¡ è¯­ä¹‰ç›¸ä¼¼åº¦è‰¯å¥½ (+3)")

        # MRR (5åˆ†)
        avg_mrr = stats.get("avg_mrr", 0)
        if avg_mrr >= 0.5:
            score += 5
            grade_descriptions.append("ğŸŸ¢ MRR ä¼˜ç§€ (+5)")

        # å“åº”é€Ÿåº¦ (5åˆ†)
        avg_time = stats.get("avg_response_time_ms", 0)
        if avg_time <= 100:
            score += 5
            grade_descriptions.append("ğŸŸ¢ å“åº”é€Ÿåº¦ä¼˜ç§€ (+5)")
        elif avg_time <= 500:
            score += 3
            grade_descriptions.append("ğŸŸ¡ å“åº”é€Ÿåº¦è‰¯å¥½ (+3)")

        # ç»¼åˆè¯„çº§
        if score >= 85:
            grade = "ğŸŸ¢ ä¼˜ç§€"
        elif score >= 70:
            grade = "ğŸŸ¡ è‰¯å¥½"
        elif score >= 55:
            grade = "ğŸŸ  ä¸€èˆ¬"
        else:
            grade = "ğŸ”´ éœ€æ”¹è¿›"

        return {
            "total_score": score,
            "max_score": 100,
            "grade": grade,
            "grade_descriptions": grade_descriptions,
        }

    def save_report(
        self,
        results: List[Dict],
        analysis: Dict[str, Any],
        score: Dict[str, Any],
        test_data_info: Dict[str, Any],
        test_file: str,
        baseline_results: Optional[List[Dict]] = None,
        baseline_analysis: Optional[Dict[str, Any]] = None,
        baseline_score: Optional[Dict[str, Any]] = None,
    ) -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Šï¼ˆæ”¯æŒåŸºçº¿å¯¹æ¯”ï¼‰"""
        timestamp = self.evaluation_time.strftime("%Y%m%d_%H%M%S")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†é‡æ’åº
        reranker_enabled = False
        if results and results[0]["model_info"].get("reranker_enabled"):
            reranker_enabled = True

        # ä¿å­˜è¯¦ç»†JSONæŠ¥å‘Š
        report_data = {
            "evaluation_info": {
                "timestamp": self.evaluation_time.isoformat(),
                "test_file": str(test_file),
                "evaluator": "enhanced_eval.py",
                "version": "2.0",
                "reranker_enabled": reranker_enabled,
            },
            "dataset_info": test_data_info,
            "score_info": score,
            "analysis": analysis,
            "detailed_results": results,
        }

        # æ·»åŠ åŸºçº¿å¯¹æ¯”æ•°æ®
        if baseline_results:
            report_data["baseline_results"] = baseline_results
            report_data["baseline_analysis"] = baseline_analysis
            report_data["baseline_score"] = baseline_score

        json_file = self.output_dir / f"rag_evaluation_report_{timestamp}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = self.output_dir / f"rag_evaluation_summary_{timestamp}.md"
        md_content = self.generate_markdown_report(
            results,
            analysis,
            score,
            test_data_info,
            timestamp,
            baseline_results=baseline_results,
            baseline_analysis=baseline_analysis,
            baseline_score=baseline_score,
        )
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(md_content)

        return str(json_file)

    def generate_markdown_report(
        self,
        results: List[Dict],
        analysis: Dict[str, Any],
        score: Dict[str, Any],
        test_data_info: Dict[str, Any],
        timestamp: str,
        baseline_results: Optional[List[Dict]] = None,
        baseline_analysis: Optional[Dict[str, Any]] = None,
        baseline_score: Optional[Dict[str, Any]] = None,
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Šï¼ˆæ”¯æŒåŸºçº¿å¯¹æ¯”ï¼‰"""
        stats = analysis["statistics"]

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†é‡æ’åº
        reranker_enabled = False
        reranker_info = ""
        if results and results[0]["model_info"].get("reranker_enabled"):
            reranker_enabled = True
            reranker_info = (
                f" ({results[0]['model_info'].get('reranker_type', 'unknown')} é‡æ’åº)"
            )

        md = f"""# RAGç³»ç»Ÿæµ‹è¯„æŠ¥å‘Š{reranker_info}

## ğŸ“Š æµ‹è¯„æ¦‚è§ˆ

- **æµ‹è¯„æ—¶é—´**: {self.evaluation_time.strftime("%Y-%m-%d %H:%M:%S")}
- **æ•°æ®é›†ç‰ˆæœ¬**: {test_data_info.get("version", "unknown")}
- **æµ‹è¯•ç”¨ä¾‹æ€»æ•°**: {analysis["total_tests"]}
- **æœ‰æ•ˆæµ‹è¯•**: {analysis["valid_tests"]}
- **å¤±è´¥æµ‹è¯•**: {analysis["failed_tests"]}
- **é‡æ’åº**: {"å¯ç”¨" if reranker_enabled else "ç¦ç”¨"}

## ğŸ† ç»¼åˆè¯„åˆ†

**{score["grade"]} - {score["total_score"]}/100**

"""

        for desc in score["grade_descriptions"]:
            md += f"- {desc}\n"

        # æ·»åŠ åŸºçº¿å¯¹æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
        if baseline_analysis and baseline_score:
            baseline_stats = baseline_analysis["statistics"]
            md += f"""
### ğŸ”„ é‡æ’åºæ•ˆæœå¯¹æ¯”

| æŒ‡æ ‡ | åŸºç¡€æ£€ç´¢ | é‡æ’åº | æ”¹è¿› |
|------|----------|--------|------|
| **ç»¼åˆè¯„åˆ†** | {baseline_score["total_score"]}/100 | {score["total_score"]}/100 | **{score["total_score"] - baseline_score["total_score"]:+d}** |
| **è¯„çº§** | {baseline_score["grade"]} | {score["grade"]} | {"â¬†ï¸ æå‡" if score["total_score"] > baseline_score["total_score"] else "â¬‡ï¸ ä¸‹é™" if score["total_score"] < baseline_score["total_score"] else "â¡ï¸ æŒå¹³"} |
| P@1 ç²¾ç¡®ç‡ | {baseline_stats["avg_precision_at_1"]:.3f} | {stats["avg_precision_at_1"]:.3f} | {stats["avg_precision_at_1"] - baseline_stats["avg_precision_at_1"]:+.3f} |
| NDCG@5 | {baseline_stats.get("avg_ndcg_at_5", 0):.3f} | {stats.get("avg_ndcg_at_5", 0):.3f} | {stats.get("avg_ndcg_at_5", 0) - baseline_stats.get("avg_ndcg_at_5", 0):+.3f} |
| F1@5 | {baseline_stats.get("avg_f1_at_5", 0):.3f} | {stats.get("avg_f1_at_5", 0):.3f} | {stats.get("avg_f1_at_5", 0) - baseline_stats.get("avg_f1_at_5", 0):+.3f} |
| å…³é”®è¯å‘½ä¸­ç‡ | {baseline_stats["avg_keyword_hit_rate"]:.1%} | {stats["avg_keyword_hit_rate"]:.1%} | {stats["avg_keyword_hit_rate"] - baseline_stats["avg_keyword_hit_rate"]:+.1%} |
| å“åº”æ—¶é—´ | {baseline_stats["avg_response_time_ms"]:.1f}ms | {stats["avg_response_time_ms"]:.1f}ms | {stats["avg_response_time_ms"] - baseline_stats["avg_response_time_ms"]:+.1f}ms |

"""

        md += f"""
## ğŸ“ˆ å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯„ä»· |
|------|------|------|
| å¹³å‡å“åº”æ—¶é—´ | {stats["avg_response_time_ms"]:.1f}ms | {"ä¼˜ç§€" if stats["avg_response_time_ms"] <= 100 else "è‰¯å¥½" if stats["avg_response_time_ms"] <= 500 else "ä¸€èˆ¬"} |
| P@1 ç²¾ç¡®ç‡ | {stats["avg_precision_at_1"]:.3f} | {"ä¼˜ç§€" if stats["avg_precision_at_1"] >= 0.7 else "è‰¯å¥½" if stats["avg_precision_at_1"] >= 0.5 else "ä¸€èˆ¬"} |
| P@3 ç²¾ç¡®ç‡ | {stats["avg_precision_at_3"]:.3f} | {"ä¼˜ç§€" if stats["avg_precision_at_3"] >= 0.7 else "è‰¯å¥½" if stats["avg_precision_at_3"] >= 0.5 else "ä¸€èˆ¬"} |
| P@5 ç²¾ç¡®ç‡ | {stats["avg_precision_at_5"]:.3f} | {"ä¼˜ç§€" if stats["avg_precision_at_5"] >= 0.8 else "è‰¯å¥½" if stats["avg_precision_at_5"] >= 0.6 else "ä¸€èˆ¬"} |
| **Recall@5** | **{stats.get("avg_recall_at_5", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_recall_at_5", 0) >= 0.7 else "è‰¯å¥½" if stats.get("avg_recall_at_5", 0) >= 0.5 else "ä¸€èˆ¬"} |
| **F1@5** | **{stats.get("avg_f1_at_5", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_f1_at_5", 0) >= 0.7 else "è‰¯å¥½" if stats.get("avg_f1_at_5", 0) >= 0.5 else "ä¸€èˆ¬"} |
| **NDCG@5** | **{stats.get("avg_ndcg_at_5", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_ndcg_at_5", 0) >= 0.7 else "è‰¯å¥½" if stats.get("avg_ndcg_at_5", 0) >= 0.5 else "ä¸€èˆ¬"} |
| MRR | {stats["avg_mrr"]:.3f} | {"ä¼˜ç§€" if stats["avg_mrr"] >= 0.5 else "è‰¯å¥½" if stats["avg_mrr"] >= 0.3 else "ä¸€èˆ¬"} |
| å…³é”®è¯å‘½ä¸­ç‡ | {stats["avg_keyword_hit_rate"]:.1%} | {"ä¼˜ç§€" if stats["avg_keyword_hit_rate"] >= 0.8 else "è‰¯å¥½" if stats["avg_keyword_hit_rate"] >= 0.6 else "ä¸€èˆ¬"} |
| **ä¸»é¢˜è¦†ç›–ç‡** | **{stats.get("avg_topic_coverage", 0):.1%}** | {"ä¼˜ç§€" if stats.get("avg_topic_coverage", 0) >= 0.8 else "è‰¯å¥½" if stats.get("avg_topic_coverage", 0) >= 0.6 else "ä¸€èˆ¬"} |
| **è¯­ä¹‰ç›¸ä¼¼åº¦** | **{stats.get("avg_semantic_similarity", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_semantic_similarity", 0) >= 0.8 else "è‰¯å¥½" if stats.get("avg_semantic_similarity", 0) >= 0.6 else "ä¸€èˆ¬"} |

### ğŸ†• æ–°å¢æŒ‡æ ‡è¯´æ˜

- **Recall@5**: å¬å›ç‡@5ï¼Œè¡¡é‡æ£€ç´¢ç»“æœçš„å®Œæ•´æ€§
- **F1@5**: F1åˆ†æ•°@5ï¼Œç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **NDCG@5**: å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š@5ï¼Œè€ƒè™‘æ’åºè´¨é‡
- **ä¸»é¢˜è¦†ç›–ç‡**: expected_topicsçš„è¦†ç›–æƒ…å†µåˆ†æ
- **è¯­ä¹‰ç›¸ä¼¼åº¦**: ä½¿ç”¨embeddingæ¨¡å‹è®¡ç®—ç­”æ¡ˆç›¸ä¼¼åº¦

## ğŸ“Š æŒ‰éš¾åº¦åˆ†æ

"""

        for diff, stats in analysis["by_difficulty"].items():
            md += f"### {diff.upper()}\n"
            md += f"- æµ‹è¯•æ•°é‡: {stats['count']}\n"
            md += f"- P@1 ç²¾ç¡®ç‡: {stats['avg_precision_at_1']:.3f}\n"
            md += f"- **NDCG@5**: {stats['avg_ndcg_at_5']:.3f}\n"
            md += f"- **F1@5**: {stats['avg_f1_at_5']:.3f}\n"
            md += f"- **Recall@5**: {stats['avg_recall_at_5']:.3f}\n"
            md += f"- å…³é”®è¯å‘½ä¸­ç‡: {stats['avg_keyword_hit_rate']:.1%}\n"
            md += f"- å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time_ms']:.1f}ms\n\n"

        if analysis["problem_cases"]:
            md += "## âš ï¸ é—®é¢˜ç”¨ä¾‹åˆ†æ\n\n"
            for case in analysis["problem_cases"][:5]:
                md += f"### {case['id']}\n"
                md += f"**æŸ¥è¯¢**: {case['query']}\n"
                md += f"**å…³é”®è¯å‘½ä¸­ç‡**: {case['hit_rate']:.1%}\n"
                md += f"**NDCG@5**: {case['ndcg_at_5']:.3f}\n"
                md += f"**ä¸»é¢˜è¦†ç›–ç‡**: {case['topic_coverage']:.1%}\n"
                md += f"**æœªå‘½ä¸­å…³é”®è¯**: {', '.join(case['missed_keywords'])}\n"
                md += f"**P@1**: {case['precision_at_1']:.3f}\n\n"

        md += """
## ğŸ“ æ–‡ä»¶è¯´æ˜

- `rag_evaluation_report_{timestamp}.json`: å®Œæ•´æµ‹è¯•æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
- `rag_evaluation_summary_{timestamp}.md`: æœ¬æ‘˜è¦æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

"""

        # åŸºäºæ–°æŒ‡æ ‡çš„ä¼˜åŒ–å»ºè®®
        if stats["avg_precision_at_1"] < 0.6:
            md += "- ğŸ”´ æ£€ç´¢ç²¾ç¡®ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–åµŒå…¥æ¨¡å‹æˆ–é‡æ’åºç­–ç•¥\n"
        if stats.get("avg_ndcg_at_5", 0) < 0.5:
            md += "- ğŸ”´ æ’åºè´¨é‡ä¸ä½³(NDCG@5)ï¼Œå»ºè®®å¼•å…¥é‡æ’åºæ¨¡å‹æˆ–ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—\n"
        if stats.get("avg_f1_at_5", 0) < 0.5:
            md += "- ğŸŸ¡ F1åˆ†æ•°åä½ï¼Œéœ€è¦å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼Œè°ƒæ•´æ£€ç´¢å‚æ•°\n"
        if stats.get("avg_recall_at_5", 0) < 0.5:
            md += "- ğŸŸ¡ å¬å›ç‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ top_kæˆ–æ‰©å±•æ–‡æ¡£åº“\n"
        if stats["avg_keyword_hit_rate"] < 0.7:
            md += "- ğŸŸ¡ å…³é”®è¯è¦†ç›–ç‡ä¸è¶³ï¼Œå»ºè®®æ‰©å±•æ–‡æ¡£å†…å®¹æˆ–ä¼˜åŒ–æŸ¥è¯¢ç†è§£\n"
        if stats.get("avg_topic_coverage", 0) < 0.6:
            md += "- ğŸŸ¡ ä¸»é¢˜è¦†ç›–ç‡åä½ï¼Œå»ºè®®ä¸°å¯Œå„ä¸»é¢˜ç›¸å…³æ–‡æ¡£\n"
        if (
            stats.get("avg_semantic_similarity", 0) < 0.6
            and stats.get("avg_semantic_similarity", 0) > 0
        ):
            md += "- ğŸŸ¡ è¯­ä¹‰ç›¸ä¼¼åº¦ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–embeddingæ¨¡å‹æˆ–ç­”æ¡ˆç”Ÿæˆè´¨é‡\n"
        if stats["avg_mrr"] < 0.3:
            md += "- ğŸŸ¡ MRRåä½ï¼Œå»ºè®®ä¼˜åŒ–æ’åºç®—æ³•ç¡®ä¿æœ€ç›¸å…³ç»“æœæ’åœ¨é¦–ä½\n"

        return md

    def run_evaluation(
        self,
        test_file: str,
        limit: Optional[int] = None,
        enable_rerank: bool = True,
        reranker_type: str = "bge",
        compare_with_baseline: bool = True,
    ) -> str:
        """è¿è¡Œå®Œæ•´æµ‹è¯„ï¼ˆæ”¯æŒé‡æ’åºå¯¹æ¯”ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸš€ RAGç³»ç»Ÿå¢å¼ºæµ‹è¯„")
        if enable_rerank:
            print(f"ğŸ”„ å¯ç”¨é‡æ’åº: {reranker_type.upper()}")
        else:
            print("ğŸ“Š åŸºç¡€æ£€ç´¢æ¨¡å¼")
        print("=" * 80)
        print(f"æµ‹è¯„æ—¶é—´: {self.evaluation_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # åˆå§‹åŒ–æœåŠ¡
        if not self.init_services(
            enable_rerank=enable_rerank, reranker_type=reranker_type
        ):
            raise RuntimeError("æœåŠ¡åˆå§‹åŒ–å¤±è´¥")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.load_test_data(test_file)

        # æ•°æ®é›†ä¿¡æ¯
        metadata = test_data.get("metadata", {})
        retrieval_cases = test_data.get("retrieval_test_cases", [])

        print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   ç‰ˆæœ¬: {metadata.get('version', 'unknown')}")
        print(f"   æè¿°: {metadata.get('description', 'N/A')}")
        print(f"   æ£€ç´¢æµ‹è¯•: {len(retrieval_cases)} æ¡")

        # è¿è¡Œä¸»è¦æµ‹è¯•ï¼ˆå¯ç”¨é‡æ’åºï¼‰
        print(f"\nğŸ¯ ä¸»è¦æµ‹è¯•: {'å¯ç”¨' if enable_rerank else 'ç¦ç”¨'}é‡æ’åº")
        results = self.evaluate_retrieval_cases(
            retrieval_cases,
            limit,
            enable_rerank=enable_rerank,
            reranker_type=reranker_type,
        )

        # å¯é€‰ï¼šå¯¹æ¯”æµ‹è¯•ï¼ˆç¦ç”¨é‡æ’åºï¼‰
        baseline_results = None
        if compare_with_baseline and enable_rerank:
            print(f"\nğŸ“Š å¯¹æ¯”æµ‹è¯•: ç¦ç”¨é‡æ’åº")
            baseline_results = self.evaluate_retrieval_cases(
                retrieval_cases, limit, enable_rerank=False
            )

        # åˆ†æä¸»è¦ç»“æœ
        print("\nğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
        analysis = self.analyze_results(results)

        # åˆ†æåŸºçº¿ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        baseline_analysis = None
        if baseline_results:
            print("ğŸ“Š åˆ†æåŸºçº¿æµ‹è¯•ç»“æœ...")
            baseline_analysis = self.analyze_results(baseline_results)

        # è®¡ç®—è¯„åˆ†
        print("ğŸ† è®¡ç®—ç»¼åˆè¯„åˆ†...")
        score = self.calculate_score(analysis)
        baseline_score = None
        if baseline_analysis:
            baseline_score = self.calculate_score(baseline_analysis)

        # ä¿å­˜æŠ¥å‘Š
        print("ğŸ’¾ ä¿å­˜æµ‹è¯•æŠ¥å‘Š...")
        report_file = self.save_report(
            results,
            analysis,
            score,
            metadata,
            test_file,
            baseline_results=baseline_results,
            baseline_analysis=baseline_analysis,
            baseline_score=baseline_score,
        )

        # æ‰“å°æ‘˜è¦
        self.print_summary(analysis, score, baseline_analysis, baseline_score)

        return report_file

    def print_summary(
        self,
        analysis: Dict[str, Any],
        score: Dict[str, Any],
        baseline_analysis: Optional[Dict[str, Any]] = None,
        baseline_score: Optional[Dict[str, Any]] = None,
    ):
        """æ‰“å°æµ‹è¯„æ‘˜è¦ï¼ˆæ”¯æŒåŸºçº¿å¯¹æ¯”ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯„æ‘˜è¦")
        if baseline_analysis:
            print("ğŸ”„ åŒ…å«é‡æ’åºæ•ˆæœå¯¹æ¯”")
        print("=" * 80)

        stats = analysis["statistics"]
        print(f"\nğŸ” æ•´ä½“æ€§èƒ½:")
        print(
            f"   æµ‹è¯•æ•°é‡: {analysis['total_tests']} (æœ‰æ•ˆ:{analysis['valid_tests']}, å¤±è´¥:{analysis['failed_tests']})"
        )
        print(f"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time_ms']:.1f}ms")
        print(f"   P@1 ç²¾ç¡®ç‡: {stats['avg_precision_at_1']:.3f}")
        print(f"   P@3 ç²¾ç¡®ç‡: {stats['avg_precision_at_3']:.3f}")
        print(f"   P@5 ç²¾ç¡®ç‡: {stats['avg_precision_at_5']:.3f}")
        print(f"   ğŸ†• Recall@5: {stats.get('avg_recall_at_5', 0):.3f}")
        print(f"   ğŸ†• F1@5: {stats.get('avg_f1_at_5', 0):.3f}")
        print(f"   ğŸ†• NDCG@5: {stats.get('avg_ndcg_at_5', 0):.3f}")
        print(f"   MRR: {stats['avg_mrr']:.3f}")
        print(f"   å…³é”®è¯å‘½ä¸­ç‡: {stats['avg_keyword_hit_rate']:.1%}")
        print(f"   ğŸ†• ä¸»é¢˜è¦†ç›–ç‡: {stats.get('avg_topic_coverage', 0):.1%}")
        print(f"   ğŸ†• è¯­ä¹‰ç›¸ä¼¼åº¦: {stats.get('avg_semantic_similarity', 0):.3f}")

        # åŸºçº¿å¯¹æ¯”
        if baseline_analysis:
            baseline_stats = baseline_analysis["statistics"]
            print(f"\nğŸ”„ é‡æ’åºæ•ˆæœå¯¹æ¯”:")
            score_improvement = score["total_score"] - baseline_score["total_score"]
            print(
                f"   ç»¼åˆè¯„åˆ†: {baseline_score['total_score']} â†’ {score['total_score']} ({score_improvement:+d}åˆ†)"
            )
            print(
                f"   P@1: {baseline_stats['avg_precision_at_1']:.3f} â†’ {stats['avg_precision_at_1']:.3f} ({stats['avg_precision_at_1'] - baseline_stats['avg_precision_at_1']:+.3f})"
            )
            print(
                f"   NDCG@5: {baseline_stats.get('avg_ndcg_at_5', 0):.3f} â†’ {stats.get('avg_ndcg_at_5', 0):.3f} ({stats.get('avg_ndcg_at_5', 0) - baseline_stats.get('avg_ndcg_at_5', 0):+.3f})"
            )
            print(
                f"   F1@5: {baseline_stats.get('avg_f1_at_5', 0):.3f} â†’ {stats.get('avg_f1_at_5', 0):.3f} ({stats.get('avg_f1_at_5', 0) - baseline_stats.get('avg_f1_at_5', 0):+.3f})"
            )
            print(
                f"   å“åº”æ—¶é—´: {baseline_stats['avg_response_time_ms']:.1f}ms â†’ {stats['avg_response_time_ms']:.1f}ms ({stats['avg_response_time_ms'] - baseline_stats['avg_response_time_ms']:+.1f}ms)"
            )

        print(f"\nğŸ“ˆ æŒ‰éš¾åº¦åˆ†æ:")
        for diff, stats in analysis["by_difficulty"].items():
            print(
                f"   {diff:6s}: P@1={stats['avg_precision_at_1']:.2f}, "
                f"NDCG5={stats['avg_ndcg_at_5']:.2f}, "
                f"F1@5={stats['avg_f1_at_5']:.2f}, "
                f"å…³é”®è¯={stats['avg_keyword_hit_rate']:.0%} ({stats['count']}æ¡)"
            )

        if analysis["problem_cases"]:
            print(f"\nâš ï¸  é—®é¢˜ç”¨ä¾‹: {len(analysis['problem_cases'])} æ¡")
            for case in analysis["problem_cases"][:2]:
                print(f"   â€¢ {case['query'][:40]}... (å‘½ä¸­:{case['hit_rate']:.0%})")

        print(f"\nğŸ† ç»¼åˆè¯„åˆ†: {score['grade']} - {score['total_score']}/100")

        for desc in score["grade_descriptions"]:
            print(f"   {desc}")

        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿå¢å¼ºæµ‹è¯„è„šæœ¬")
    parser.add_argument(
        "--test-file",
        type=str,
        default="test_dataset_extended.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶",
    )
    parser.add_argument("--limit", type=int, help="é™åˆ¶æµ‹è¯•æ•°é‡")
    parser.add_argument(
        "--output-dir", type=str, default="test_reports", help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--enable-rerank",
        action="store_true",
        default=True,
        help="å¯ç”¨é‡æ’åºï¼ˆé»˜è®¤å¯ç”¨ï¼‰",
    )
    parser.add_argument("--disable-rerank", action="store_true", help="ç¦ç”¨é‡æ’åº")
    parser.add_argument(
        "--reranker-type",
        type=str,
        default="bge",
        choices=["bge", "cross-encoder", "none"],
        help="é‡æ’åºå™¨ç±»å‹",
    )
    parser.add_argument(
        "--compare", action="store_true", default=True, help="ä¸åŸºçº¿å¯¹æ¯”ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument("--no-compare", action="store_true", help="ç¦ç”¨åŸºçº¿å¯¹æ¯”")

    args = parser.parse_args()

    # å¤„ç†é‡æ’åºé€‰é¡¹
    enable_rerank = args.enable_rerank and not args.disable_rerank
    compare_with_baseline = args.compare and not args.no_compare

    evaluator = RAGEvaluator(output_dir=args.output_dir)

    try:
        report_file = evaluator.run_evaluation(
            args.test_file,
            args.limit,
            enable_rerank=enable_rerank,
            reranker_type=args.reranker_type,
            compare_with_baseline=compare_with_baseline,
        )
        print(f"\nâœ… æµ‹è¯„å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯„å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
