#!/usr/bin/env python3
"""
RAGç³»ç»Ÿå¢å¼ºæµ‹è¯„è„šæœ¬ - AutoDLé€‚é…ç‰ˆ
æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„å’ŒGPUåŠ é€Ÿ
"""

import os
# ç¦ç”¨ stdout é‡å®šå‘ï¼Œé¿å…ä¸è„šæœ¬è‡ªèº«çš„ logging å†²çª
os.environ['RAG_DISABLE_STDOUT_REDIRECT'] = 'true'

import sys
import json
import time
import math
import logging
import argparse
from pathlib import Path
from datetime import datetime
import statistics
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

import torch
import numpy as np

# é…ç½®æ—¥å¿— - ä½¿ç”¨eval_configä¸­çš„æ—¥å¿—é…ç½®
import sys
import os
from pathlib import Path

# è·å–eval_configä¸­çš„æ—¥å¿—é…ç½®
from eval_config import LOG_CONFIG

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_file = LOG_CONFIG.get("log_file")
if log_file:
    log_file = Path(log_file)
    log_file.parent.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=getattr(logging, LOG_CONFIG.get("log_level", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(str(log_file), encoding="utf-8")
        if log_file
        else logging.NullHandler(),
    ],
    force=True,  # å¼ºåˆ¶é‡æ–°é…ç½®
)
logger = logging.getLogger(__name__)

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# å¯¼å…¥eval_configé…ç½®
from eval_config import (
    VECTOR_DB_DIR,
    MODELS_DIR,
    TEST_DATASET_PATH,
)

# å¯¼å…¥RAGç”Ÿæˆå™¨
from services.rag_generator import rag_generator
from models import RetrievalConfig, GenerationConfig

# ==================== æ–‡æœ¬ç›¸ä¼¼åº¦è¯„ä¼°æŒ‡æ ‡ ====================

def calculate_bleu(reference: str, candidate: str, max_n: int = 4) -> Dict[str, float]:
    """
    è®¡ç®—BLEUåˆ†æ•°ï¼ˆåŸºäºn-gramç²¾ç¡®ç‡çš„å‡ ä½•å¹³å‡ï¼‰
    
    Args:
        reference: å‚è€ƒæ–‡æœ¬ï¼ˆGround Truthï¼‰
        candidate: å€™é€‰æ–‡æœ¬ï¼ˆLLMç”Ÿæˆï¼‰
        max_n: æœ€å¤§n-gramé˜¶æ•°
    
    Returns:
        BLEU-1åˆ°BLEU-4çš„åˆ†æ•°
    """
    import re
    from collections import Counter
    
    def get_ngrams(tokens, n):
        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
    
    def tokenize(text):
        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦åˆ†è¯ï¼‰
        text = re.sub(r'[^\w\s]', ' ', text)
        return list(text.replace(' ', ''))
    
    ref_tokens = tokenize(reference)
    cand_tokens = tokenize(candidate)
    
    if len(cand_tokens) == 0:
        return {f'bleu_{n}': 0.0 for n in range(1, max_n+1)}
    
    results = {}
    for n in range(1, max_n+1):
        ref_ngrams = Counter(get_ngrams(ref_tokens, n))
        cand_ngrams = Counter(get_ngrams(cand_tokens, n))
        
        matches = sum((cand_ngrams & ref_ngrams).values())
        total = sum(cand_ngrams.values())
        
        if total == 0:
            results[f'bleu_{n}'] = 0.0
        else:
            # ç®€åŒ–ç‰ˆBLEUï¼ˆæ— çŸ­å¥æƒ©ç½šï¼‰
            results[f'bleu_{n}'] = matches / total
    
    return results

def calculate_rouge(reference: str, candidate: str) -> Dict[str, float]:
    """
    è®¡ç®—ROUGEåˆ†æ•°ï¼ˆåŸºäºå¬å›ç‡çš„n-gramé‡å ï¼‰
    
    Args:
        reference: å‚è€ƒæ–‡æœ¬ï¼ˆGround Truthï¼‰
        candidate: å€™é€‰æ–‡æœ¬ï¼ˆLLMç”Ÿæˆï¼‰
    
    Returns:
        ROUGE-1, ROUGE-2, ROUGE-Låˆ†æ•°
    """
    import re
    
    def tokenize(text):
        text = re.sub(r'[^\w\s]', ' ', text)
        return list(text.replace(' ', ''))
    
    def lcs_length(X, Y):
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(X), len(Y)
        if m == 0 or n == 0:
            return 0
        
        # ä½¿ç”¨æ»šåŠ¨æ•°ç»„ä¼˜åŒ–ç©ºé—´
        prev = [0] * (n + 1)
        curr = [0] * (n + 1)
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if X[i-1] == Y[j-1]:
                    curr[j] = prev[j-1] + 1
                else:
                    curr[j] = max(prev[j], curr[j-1])
            prev, curr = curr, prev
        
        return prev[n]
    
    ref_tokens = tokenize(reference)
    cand_tokens = tokenize(candidate)
    
    results = {}
    
    # ROUGE-N (N-gram recall)
    for n in [1, 2]:
        ref_ngrams = set()
        cand_ngrams = set()
        
        for i in range(len(ref_tokens) - n + 1):
            ref_ngrams.add(tuple(ref_tokens[i:i+n]))
        for i in range(len(cand_tokens) - n + 1):
            cand_ngrams.add(tuple(cand_tokens[i:i+n]))
        
        if len(ref_ngrams) == 0:
            results[f'rouge_{n}'] = 0.0
        else:
            overlap = len(ref_ngrams & cand_ngrams)
            results[f'rouge_{n}'] = overlap / len(ref_ngrams)
    
    # ROUGE-L (æœ€é•¿å…¬å…±å­åºåˆ—)
    lcs = lcs_length(ref_tokens, cand_tokens)
    if len(ref_tokens) == 0:
        results['rouge_l'] = 0.0
    else:
        results['rouge_l'] = lcs / len(ref_tokens)
    
    return results

def calculate_answer_metrics(reference: str, candidate: str) -> Dict[str, float]:
    """
    è®¡ç®—ç­”æ¡ˆè´¨é‡ç»¼åˆæŒ‡æ ‡
    
    Args:
        reference: å‚è€ƒæ–‡æœ¬ï¼ˆGround Truthï¼‰
        candidate: å€™é€‰æ–‡æœ¬ï¼ˆLLMç”Ÿæˆï¼‰
    
    Returns:
        åŒ…å«BLEUã€ROUGEã€è¯­ä¹‰ç›¸ä¼¼åº¦ç­‰çš„ç»¼åˆæŒ‡æ ‡
    """
    metrics = {}
    
    # 1. BLEUåˆ†æ•°
    bleu_scores = calculate_bleu(reference, candidate)
    metrics.update(bleu_scores)
    # è®¡ç®—å¹³å‡BLEU
    metrics['bleu_avg'] = sum(bleu_scores.values()) / len(bleu_scores)
    
    # 2. ROUGEåˆ†æ•°
    rouge_scores = calculate_rouge(reference, candidate)
    metrics.update(rouge_scores)
    # è®¡ç®—å¹³å‡ROUGE
    metrics['rouge_avg'] = sum(rouge_scores.values()) / len(rouge_scores)
    
    # 3. å­—ç¬¦çº§ç²¾ç¡®ç‡å’Œå¬å›ç‡
    ref_set = set(reference)
    cand_set = set(candidate)
    
    if len(cand_set) > 0:
        metrics['char_precision'] = len(ref_set & cand_set) / len(cand_set)
    else:
        metrics['char_precision'] = 0.0
    
    if len(ref_set) > 0:
        metrics['char_recall'] = len(ref_set & cand_set) / len(ref_set)
    else:
        metrics['char_recall'] = 0.0
    
    if metrics['char_precision'] + metrics['char_recall'] > 0:
        metrics['char_f1'] = 2 * metrics['char_precision'] * metrics['char_recall'] / (metrics['char_precision'] + metrics['char_recall'])
    else:
        metrics['char_f1'] = 0.0
    
    # 4. ç­”æ¡ˆé•¿åº¦æ¯”
    metrics['length_ratio'] = len(candidate) / len(reference) if len(reference) > 0 else 0.0
    
    return metrics

from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from services.rag_evaluator import rag_evaluator
from services.retriever import Retriever
from services.reranker import reranker_manager
from models import (
    RetrievalConfig,
    EmbeddingConfig,
    VectorDBConfig,
    EmbeddingModelType,
    VectorDBType,
    ChunkInfo,
)
# from config import settings  # æ³¨é‡Šæ‰ï¼Œé¿å…æ—¥å¿—å†²çª


@dataclass
class FakeResult:
    """æ¨¡æ‹Ÿæ£€ç´¢ç»“æœå¯¹è±¡"""

    content: str
    similarity: float
    document_id: str
    chunk_id: str
    rank: int = 0
    rerank_score: Optional[float] = None


class RAGEvaluator:
    """RAGç³»ç»Ÿæµ‹è¯„å™¨"""

    def __init__(
        self,
        output_dir: str = "test_reports",
        model_base_path: Optional[str] = None,
        vector_db_path: Optional[str] = None,
        keep_llm_loaded: bool = True,  # æ˜¯å¦ä¿æŒLLMæ¨¡å‹å¸¸é©»æ˜¾å­˜
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.evaluation_time = datetime.now()
        # ä½¿ç”¨ä¼ å…¥çš„è·¯å¾„æˆ–eval_configä¸­çš„é»˜è®¤è·¯å¾„
        self.model_base_path = Path(model_base_path) if model_base_path else MODELS_DIR
        self.vector_db_path = Path(vector_db_path) if vector_db_path else VECTOR_DB_DIR
        # LLMå¸¸é©»æ˜¾å­˜é…ç½®
        self.keep_llm_loaded = keep_llm_loaded
        self._llm_client = None  # ç¼“å­˜LLMå®¢æˆ·ç«¯
        self._llm_config = None  # ç¼“å­˜LLMé…ç½®

    def calculate_ndcg_at_k(
        self, results: List[FakeResult], ground_truth: List[str], k: int = 5
    ) -> float:
        """è®¡ç®—NDCG@K - å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š"""
        if not ground_truth or not results:
            return 0.0

        dcg = 0.0
        for i, result in enumerate(results[:k]):
            relevance = 0.0
            for gt in ground_truth:
                # ä½¿ç”¨ç»Ÿä¸€çš„åŒ¹é…æ£€æŸ¥
                if self._check_text_match(result.content, gt, use_semantic=False):
                    relevance = 1.0
                    break

                # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç”¨äºè®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼‰
                try:
                    sim = self.calculate_semantic_similarity(gt, result.content[:500])
                    relevance = max(relevance, sim)
                except (RuntimeError, ValueError, TypeError):
                    pass

            # é™åˆ¶relevanceåœ¨[0,1]èŒƒå›´å†…ï¼Œé˜²æ­¢NDCG>1
            relevance = min(relevance, 1.0)
            if relevance > 0:
                dcg += (2**relevance - 1) / math.log2(i + 2)

        # è®¡ç®—ç†æƒ³DCGï¼ˆå‰kä¸ªç»“æœéƒ½å®Œå…¨ç›¸å…³ï¼‰
        ideal_relevances = [1.0] * k

        idcg = sum(
            (2**rel - 1) / math.log2(i + 2) for i, rel in enumerate(ideal_relevances)
        )

        return dcg / idcg if idcg > 0 else 0.0

    def calculate_recall_at_k(
        self, results: List[FakeResult], ground_truth: List[str], k: int = 5
    ) -> float:
        """è®¡ç®—Recall@K - å¬å›ç‡@K"""
        if not ground_truth:
            return 0.0

        covered_ground_truths = set()
        for gt in ground_truth:
            for result in results[:k]:
                if self._check_text_match(
                    result.content, gt, use_semantic=True, semantic_threshold=0.6
                ):
                    covered_ground_truths.add(gt)
                    break

        return len(covered_ground_truths) / len(ground_truth)

    def calculate_precision_at_k(
        self, results: List[FakeResult], ground_truth: List[str], k: int
    ) -> float:
        """è®¡ç®—Precision@K"""
        if not results or not ground_truth or k <= 0:
            return 0.0

        relevant_count = 0
        for result in results[:k]:
            for gt in ground_truth:
                if self._check_text_match(
                    result.content, gt, use_semantic=True, semantic_threshold=0.6
                ):
                    relevant_count += 1
                    break

        return relevant_count / k

    def calculate_mrr(
        self, results: List[FakeResult], ground_truth: List[str]
    ) -> float:
        """è®¡ç®—MRR - Mean Reciprocal Rank"""
        if not ground_truth or not results:
            return 0.0

        for i, result in enumerate(results[:5], 1):
            for gt in ground_truth:
                if self._check_text_match(
                    result.content, gt, use_semantic=True, semantic_threshold=0.6
                ):
                    return 1.0 / i
        return 0.0

    def calculate_f1_at_k(self, precision: float, recall: float) -> float:
        """è®¡ç®—F1@K - F1åˆ†æ•°"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    def _check_text_match(
        self,
        text: str,
        ground_truth: str,
        use_semantic: bool = False,
        semantic_threshold: float = 0.6,
    ) -> bool:
        """ç»Ÿä¸€çš„æ–‡æœ¬åŒ¹é…æ£€æŸ¥é€»è¾‘

        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
            ground_truth: åŸºå‡†æ–‡æœ¬
            use_semantic: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
            semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æ˜¯å¦åŒ¹é…
        """
        text_lower = text.lower()
        gt_lower = ground_truth.lower()

        # 1. å®Œå…¨åŒ…å«åŒ¹é…
        if gt_lower in text_lower or text_lower in gt_lower:
            return True

        # 2. éƒ¨åˆ†åŒ¹é…ï¼ˆå¯¹äºè¾ƒé•¿çš„ground_truthï¼‰
        if len(gt_lower) > 4:
            gt_parts = gt_lower.split()
            if len(gt_parts) > 1:
                match_count = sum(
                    1 for part in gt_parts if len(part) > 2 and part in text_lower
                )
                if match_count >= len(gt_parts) * 0.5:
                    return True

        # 3. è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
        if use_semantic:
            try:
                sim = self.calculate_semantic_similarity(ground_truth, text[:500])
                if sim > semantic_threshold:
                    return True
            except (RuntimeError, ValueError, TypeError) as e:
                logger.debug(f"è¯­ä¹‰åŒ¹é…æ£€æŸ¥å¤±è´¥: {e}")

        return False

    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ - ä½¿ç”¨embeddingæ¨¡å‹"""
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if not embedding_service.is_loaded():
            logger.warning("åµŒå…¥æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦")
            return 0.0

        try:
            embeddings = embedding_service.encode([text1, text2])

            norm1 = np.linalg.norm(embeddings[0])
            norm2 = np.linalg.norm(embeddings[1])

            if norm1 == 0 or norm2 == 0:
                return 0.0

            sim = np.dot(embeddings[0], embeddings[1]) / (norm1 * norm2)
            return float(sim)
        except (RuntimeError, ValueError, TypeError) as e:
            logger.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def calculate_topic_coverage(
        self, results: List[FakeResult], expected_topics: List[str]
    ) -> Dict[str, Any]:
        """è®¡ç®—ä¸»é¢˜è¦†ç›–ç‡"""
        if not expected_topics:
            return {
                "coverage_rate": 0.0,
                "covered_topics": [],
                "missed_topics": [],
                "total_topics": 0,
                "covered_count": 0,
            }

        retrieved_text = " ".join([r.content for r in results]).lower()

        covered_topics = []
        missed_topics = []

        for topic in expected_topics:
            if topic.lower() in retrieved_text:
                covered_topics.append(topic)
            else:
                missed_topics.append(topic)

        coverage_rate = (
            len(covered_topics) / len(expected_topics) if expected_topics else 0.0
        )

        return {
            "coverage_rate": coverage_rate,
            "covered_topics": covered_topics,
            "missed_topics": missed_topics,
            "total_topics": len(expected_topics),
            "covered_count": len(covered_topics),
        }

    def apply_reranking(
        self, results: List[FakeResult], query: str, expected_keywords: List[str]
    ) -> List[FakeResult]:
        """åº”ç”¨åŸºäºå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„é‡æ’åºç®—æ³•"""

        def calculate_rerank_score(
            result: FakeResult, query: str, keywords: List[str]
        ) -> float:
            """è®¡ç®—é‡æ’åºåˆ†æ•°"""
            score = result.similarity * 0.4

            content_lower = result.content.lower()
            if keywords:
                keyword_match_count = sum(
                    1 for kw in keywords if kw.lower() in content_lower
                )
                keyword_score = keyword_match_count / len(keywords)
                score += keyword_score * 0.3

            if "é…’åº—" in query or "ä½å®¿" in query:
                hotel_keywords = ["ä¸‰æ˜Ÿçº§", "å››æ˜Ÿçº§", "äº”æ˜Ÿçº§", "å¿«æ·é…’åº—", "ç»æµå‹"]
                hotel_match_count = sum(
                    1 for hk in hotel_keywords if hk in result.content
                )
                if hotel_match_count > 0:
                    score += min(hotel_match_count * 0.05, 0.15)

            level_keywords = {
                "8-9çº§": ["8-9çº§", "æ™®é€šå‘˜å·¥", "å·¥ç¨‹å¸ˆ", "ä¸“å‘˜"],
                "10-11çº§": ["10-11çº§", "ç»ç†", "ä¸»ç®¡"],
                "12çº§": ["12çº§", "æ€»ç›‘", "ä¸“å®¶", "é«˜çº§"],
            }
            for level_key, level_words in level_keywords.items():
                if level_key in query:
                    level_match_count = sum(
                        1 for lw in level_words if lw in result.content
                    )
                    if level_match_count > 0:
                        score += min(level_match_count * 0.03, 0.1)
                        break

            city_keywords = {
                "ä¸€çº¿åŸå¸‚": ["ä¸Šæµ·", "åŒ—äº¬", "å¹¿å·", "æ·±åœ³", "ä¸€çº¿åŸå¸‚", "åŒ—ä¸Šå¹¿æ·±"],
                "æ–°ä¸€çº¿": [
                    "æˆéƒ½",
                    "æ­å·",
                    "æ­¦æ±‰",
                    "è¥¿å®‰",
                    "å—äº¬",
                    "é‡åº†",
                    "æ–°ä¸€çº¿",
                    "æ–°ä¸€çº¿åŸå¸‚",
                ],
            }
            for city_type, cities in city_keywords.items():
                if any(c in query for c in cities):
                    if any(c in result.content for c in cities):
                        score += 0.05
                        break

            return score

        scored_results = []
        for result in results:
            rerank_score = calculate_rerank_score(result, query, expected_keywords)
            new_result = FakeResult(
                content=result.content,
                similarity=result.similarity,
                document_id=result.document_id,
                chunk_id=result.chunk_id,
                rank=result.rank,
                rerank_score=rerank_score,
            )
            scored_results.append((new_result, rerank_score))

        scored_results.sort(key=lambda x: x[1], reverse=True)

        final_results = []
        for i, (result, _) in enumerate(scored_results):
            result.rank = i + 1
            final_results.append(result)

        return final_results

    def init_services(
        self, enable_rerank: bool = True, reranker_type: str = "bge"
    ) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡ï¼ˆAutoDLæœ¬åœ°è·¯å¾„ç‰ˆï¼‰"""
        print("ğŸ”§ åˆå§‹åŒ–æœåŠ¡...")

        device = "cuda" if torch.cuda.is_available() else "cpu"

        try:
            # åŠ è½½æœ¬åœ° Embedding æ¨¡å‹
            embedding_model_path = self.model_base_path / "bge-base-zh-v1.5"
            print(f"   åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹: {embedding_model_path}")
            print(f"   ä½¿ç”¨è®¾å¤‡: {device}")

            if not embedding_model_path.exists():
                print(f"   âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {embedding_model_path}")
                print(
                    f"   è¯·ä» ModelScope ä¸‹è½½: modelscope download --model BAAI/bge-base-zh-v1.5 --local_dir {embedding_model_path}"
                )
                return False

            embedding_service.load_model(
                EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name=str(embedding_model_path),
                    device=device,
                )
            )
            print(f"   âœ… æ¨¡å‹ç»´åº¦: {embedding_service.get_dimension()}")

            # åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“
            print(f"   åŠ è½½å‘é‡æ•°æ®åº“: {self.vector_db_path}")
            if not self.vector_db_path.exists():
                print(f"   âŒ å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨: {self.vector_db_path}")
                return False

            vector_db_manager.initialize(
                VectorDBConfig(
                    db_type=VectorDBType.FAISS,
                    dimension=embedding_service.get_dimension(),
                    index_type="HNSW",
                    index_path=str(self.vector_db_path),
                )
            )
            status = vector_db_manager.get_status()
            if status.total_vectors == 0:
                print(f"   âŒ å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆå‘é‡åŒ–æ–‡æ¡£")
                return False
            print(f"   âœ… å‘é‡åº“: {status.total_vectors} ä¸ªå‘é‡")

            # åˆå§‹åŒ–é‡æ’åºå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_rerank and reranker_type != "none":
                reranker_model_path = self.model_base_path / "bge-reranker-base"
                print(f"   åˆå§‹åŒ–é‡æ’åºå™¨: {reranker_type}")

                if reranker_model_path.exists():
                    print(f"   ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {reranker_model_path}")
                    reranker_manager.initialize(
                        reranker_type=reranker_type,
                        model_name=str(reranker_model_path),
                        device=device,
                        top_k=10,
                        threshold=0.0,
                    )
                else:
                    print(f"   âš ï¸  æœ¬åœ°é‡æ’åºæ¨¡å‹ä¸å­˜åœ¨: {reranker_model_path}")
                    print(
                        f"   è¯·ä» ModelScope ä¸‹è½½: modelscope download --model BAAI/bge-reranker-base --local_dir {reranker_model_path}"
                    )
                    print(f"   æš‚æ—¶ä½¿ç”¨åŸºç¡€é‡æ’åºï¼ˆæ— æ¨¡å‹ï¼‰...")
                    reranker_manager.initialize(
                        reranker_type="none",  # ä½¿ç”¨è§„åˆ™é‡æ’åº
                        device=device,
                        top_k=10,
                        threshold=0.0,
                    )
            else:
                print("   âš ï¸  é‡æ’åºå™¨: å·²ç¦ç”¨")

            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            return False

    def load_test_data(self, test_file: str) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        if not Path(test_file).exists():
            test_file = Path(__file__).parent.parent.parent / "test_data" / test_file

        if not Path(test_file).exists():
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")

        with open(test_file, "r", encoding="utf-8") as f:
            test_data = json.load(f)

        print(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
        return test_data

    def enhance_query(self, query: str, expected_topics: List[str] = None) -> str:
        """æŸ¥è¯¢å¢å¼ºï¼šæ·»åŠ ç›¸å…³è¯æ±‡æå‡å¬å›ç‡"""
        enhanced_query = query
        additions = set()

        if "ä½å®¿" in query or "é…’åº—" in query:
            additions.update(["é…’åº—æ˜Ÿçº§", "ä¸‰æ˜Ÿçº§", "å››æ˜Ÿçº§", "äº”æ˜Ÿçº§", "å¿«æ·é…’åº—"])

        level_mappings = {
            "8-9çº§": [
                "8-9çº§",
                "æ™®é€šå‘˜å·¥",
                "è½¯ä»¶ç ”å‘å·¥ç¨‹å¸ˆ",
                "æœºæ¢°ç ”å‘å·¥ç¨‹å¸ˆ",
                "å·¥è‰ºå·¥ç¨‹å¸ˆ",
                "å®æ–½å·¥ç¨‹å¸ˆ",
            ],
            "10-11çº§": ["10-11çº§", "ç»ç†", "ä¸­å±‚ç®¡ç†", "ä¸»ç®¡"],
            "12çº§": ["12çº§", "æ€»ç›‘", "ä¸“å®¶çº§", "é«˜çº§ç®¡ç†"],
        }

        for level_key, level_terms in level_mappings.items():
            if level_key in query or any(term in query for term in level_terms[:2]):
                additions.update(level_terms)
                break

        city_mappings = {
            "ä¸€çº¿åŸå¸‚": ["ä¸Šæµ·", "åŒ—äº¬", "å¹¿å·", "æ·±åœ³"],
            "æ–°ä¸€çº¿": ["æˆéƒ½", "æ­å·", "æ­¦æ±‰", "è¥¿å®‰", "å—äº¬", "é‡åº†", "è‹å·", "å¤©æ´¥"],
        }

        for city_type, cities in city_mappings.items():
            if any(city in query for city in cities):
                additions.add(city_type)
                if city_type == "ä¸€çº¿åŸå¸‚":
                    additions.add("åŒ—ä¸Šå¹¿æ·±")
                break

        if expected_topics:
            topic_mappings = {
                "ä½å®¿æ ‡å‡†": ["å‡ºå·®ä½å®¿", "æŠ¥é”€æ ‡å‡†", "ä½å®¿è´¹ç”¨", "é…’åº—æ ‡å‡†"],
                "èŒçº§å·®å¼‚": ["ç­‰çº§æ ‡å‡†", "èŒä½çº§åˆ«", "å¯¹åº”å…³ç³»", "èŒçº§åˆ’åˆ†"],
                "åœ°åŒºå·®å¼‚": ["åŸå¸‚åˆ†çº§", "åœ°åŒºåˆ†ç±»", "ä¸€çº¿äºŒçº¿", "åŸå¸‚çº§åˆ«"],
            }
            for topic in expected_topics:
                if topic in topic_mappings:
                    additions.update(topic_mappings[topic])

        for add in additions:
            if add not in enhanced_query:
                enhanced_query += " " + add

        return enhanced_query

    def run_retrieval_test(
        self,
        query: str,
        expected_keywords: List[str],
        case_info: Dict[str, Any],
        enable_rerank: bool = True,
        reranker_type: str = "bge",
        verbose: bool = True,
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ£€ç´¢æµ‹è¯•"""
        case_id = case_info.get('id', 'unknown')
        
        # è®°å½•æµ‹è¯•å¼€å§‹
        logger.info(f"=" * 80)
        logger.info(f"ã€æµ‹è¯•ç”¨ä¾‹ã€‘{case_id}")
        logger.info(f"=" * 80)

        # æ­¥éª¤1: æŸ¥è¯¢å¢å¼º
        enhanced_query = self.enhance_query(query, case_info.get("expected_topics", []))
        if verbose:
            print(f"\nğŸ“ åŸå§‹æŸ¥è¯¢: {query}")
            if enhanced_query != query:
                print(f"ğŸ”§ å¢å¼ºæŸ¥è¯¢: {enhanced_query}")
        # å†™å…¥æ—¥å¿—
        logger.info(f"åŸå§‹æŸ¥è¯¢: {query}")
        if enhanced_query != query:
            logger.info(f"å¢å¼ºæŸ¥è¯¢: {enhanced_query}")

        # æ­¥éª¤2: å‘é‡ç¼–ç 
        query_vector = embedding_service.encode([enhanced_query])
        if verbose:
            print(f"ğŸ”¢ æŸ¥è¯¢å‘é‡ç»´åº¦: {query_vector.shape}")
        logger.info(f"æŸ¥è¯¢å‘é‡ç»´åº¦: {query_vector.shape}")

        # æ­¥éª¤3: å‘é‡æ£€ç´¢
        start = time.time()
        try:
            scores, metadatas = vector_db_manager.search(query_vector, top_k=15)
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            raise
        elapsed = (time.time() - start) * 1000

        if verbose:
            print(f"\nğŸ” å‘é‡æ£€ç´¢ (è€—æ—¶: {elapsed:.1f}ms)")
            print(f"   æ£€ç´¢åˆ° {len(metadatas[0])} ä¸ªç»“æœ")
        logger.info(f"å‘é‡æ£€ç´¢è€—æ—¶: {elapsed:.1f}ms, æ£€ç´¢åˆ° {len(metadatas[0])} ä¸ªç»“æœ")

        # æ­¥éª¤4: æ„å»ºç»“æœåˆ—è¡¨
        results = []
        for i, (score, meta) in enumerate(zip(scores[0], metadatas[0])):
            document_id = meta.get("document_id", "")
            chunk_id = (
                meta.get("chunk_id") or f"{document_id}_chunk_{i}"
                if document_id
                else f"chunk_{i}"
            )
            results.append(
                FakeResult(
                    content=meta.get("content", ""),
                    similarity=float(score),
                    document_id=document_id,
                    chunk_id=chunk_id,
                    rank=i + 1,
                )
            )

        if verbose:
            print(f"\nğŸ“„ åŸå§‹æ£€ç´¢ç»“æœ (Top 5):")
            for i, r in enumerate(results[:5], 1):
                content_preview = r.content[:100] + "..." if len(r.content) > 100 else r.content
                print(f"   [{i}] ç›¸ä¼¼åº¦: {r.similarity:.3f} | {content_preview}")
        # å†™å…¥æ—¥å¿—
        logger.info("åŸå§‹æ£€ç´¢ç»“æœ (Top 5):")
        for i, r in enumerate(results[:5], 1):
            content_log = r.content[:200] + "..." if len(r.content) > 200 else r.content
            logger.info(f"  [{i}] ç›¸ä¼¼åº¦: {r.similarity:.3f} | æ–‡æ¡£: {r.document_id} | {content_log}")

        # æ­¥éª¤5: é‡æ’åº
        if enable_rerank:
            try:
                reranked_results = self.apply_reranking(
                    results[:10], query, expected_keywords
                )
                if verbose:
                    print(f"\nğŸ”„ é‡æ’åºå®Œæˆ ({reranker_type})")
                    print(f"   é‡æ’åºå‰ Top3: {[r.similarity for r in results[:3]]}")
                    print(f"   é‡æ’åºå Top3: {[r.rerank_score for r in reranked_results[:3]]}")
                # å†™å…¥æ—¥å¿—
                logger.info(f"é‡æ’åºå®Œæˆ ({reranker_type})")
                logger.info(f"  é‡æ’åºå‰ Top3: {[r.similarity for r in results[:3]]}")
                logger.info(f"  é‡æ’åºå Top3: {[r.rerank_score for r in reranked_results[:3]]}")
                results = reranked_results
            except (RuntimeError, ValueError, TypeError) as e:
                logger.warning(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {e}")
                if verbose:
                    print(f"   âš ï¸ é‡æ’åºå¤±è´¥: {e}")

        # æ­¥éª¤6: å‡†å¤‡ Ground Truth
        ground_truth_raw = case_info.get("ground_truth", [])
        if isinstance(ground_truth_raw, str):
            ground_truth = [ground_truth_raw]
        elif isinstance(ground_truth_raw, list) and ground_truth_raw:
            ground_truth = ground_truth_raw
        else:
            ground_truth = expected_keywords if expected_keywords else []
            if not ground_truth and verbose:
                print(f"   âš ï¸ ç”¨ä¾‹ {case_info.get('id', 'unknown')} ç¼ºå°‘ground_truthå’Œkeywords")

        if verbose and ground_truth:
            print(f"\nğŸ¯ Ground Truth / æœŸæœ›å…³é”®è¯: {ground_truth[:5]}")
        if ground_truth:
            logger.info(f"Ground Truth / æœŸæœ›å…³é”®è¯: {ground_truth}")

        # æ­¥éª¤7: å…³é”®è¯åŒ¹é…åˆ†æ
        retrieved_text = " ".join([r.content for r in results[:5]])
        hits = sum(1 for kw in expected_keywords if kw in retrieved_text)
        hit_rate = hits / len(expected_keywords) if expected_keywords else 0

        matched_keywords = [kw for kw in expected_keywords if kw in retrieved_text]
        missed_keywords = [kw for kw in expected_keywords if kw not in retrieved_text]

        if verbose and expected_keywords:
            print(f"\nğŸ”‘ å…³é”®è¯åˆ†æ:")
            print(f"   æ€»å…³é”®è¯: {len(expected_keywords)} ({expected_keywords})")
            print(f"   å‘½ä¸­: {hits} ({matched_keywords})")
            print(f"   æœªå‘½ä¸­: {len(missed_keywords)} ({missed_keywords})")
            print(f"   å‘½ä¸­ç‡: {hit_rate:.1%}")
        # å†™å…¥æ—¥å¿—
        if expected_keywords:
            logger.info(f"å…³é”®è¯åˆ†æ:")
            logger.info(f"  æ€»å…³é”®è¯: {len(expected_keywords)} - {expected_keywords}")
            logger.info(f"  å‘½ä¸­: {hits} - {matched_keywords}")
            logger.info(f"  æœªå‘½ä¸­: {len(missed_keywords)} - {missed_keywords}")
            logger.info(f"  å‘½ä¸­ç‡: {hit_rate:.1%}")

        # æ­¥éª¤8: è®¡ç®—å„é¡¹æŒ‡æ ‡
        precision_at_1 = self.calculate_precision_at_k(results, ground_truth, 1)
        precision_at_3 = self.calculate_precision_at_k(results, ground_truth, 3)
        precision_at_5 = self.calculate_precision_at_k(results, ground_truth, 5)

        recall_at_5 = self.calculate_recall_at_k(results, ground_truth, k=5)
        f1_at_5 = self.calculate_f1_at_k(precision_at_5, recall_at_5)
        ndcg_at_5 = self.calculate_ndcg_at_k(results, ground_truth, k=5)
        mrr = self.calculate_mrr(results, ground_truth)

        semantic_similarity = 0.0
        if ground_truth and results:
            semantic_similarity = self.calculate_semantic_similarity(
                query, results[0].content
            )

        expected_topics = case_info.get("expected_topics", [])
        topic_coverage = self.calculate_topic_coverage(results, expected_topics)

        if verbose:
            print(f"\nğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
            print(f"   P@1: {precision_at_1:.3f} | P@3: {precision_at_3:.3f} | P@5: {precision_at_5:.3f}")
            print(f"   Recall@5: {recall_at_5:.3f} | F1@5: {f1_at_5:.3f}")
            print(f"   NDCG@5: {ndcg_at_5:.3f} | MRR: {mrr:.3f}")
            print(f"   è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_similarity:.3f}")
            if topic_coverage.get('coverage_rate', 0) > 0:
                print(f"   ä¸»é¢˜è¦†ç›–ç‡: {topic_coverage['coverage_rate']:.1%}")
        # å†™å…¥æ—¥å¿—
        logger.info(f"è¯„ä¼°æŒ‡æ ‡:")
        logger.info(f"  P@1: {precision_at_1:.3f} | P@3: {precision_at_3:.3f} | P@5: {precision_at_5:.3f}")
        logger.info(f"  Recall@5: {recall_at_5:.3f} | F1@5: {f1_at_5:.3f}")
        logger.info(f"  NDCG@5: {ndcg_at_5:.3f} | MRR: {mrr:.3f}")
        logger.info(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_similarity:.3f}")
        if topic_coverage.get('coverage_rate', 0) > 0:
            logger.info(f"  ä¸»é¢˜è¦†ç›–ç‡: {topic_coverage['coverage_rate']:.1%}")

        # æ­¥éª¤9: æ¨¡å‹ä¿¡æ¯
        model_info = {
            "embedding_model": "BAAI/bge-base-zh-v1.5 (æœ¬åœ°)",
            "vector_db": "FAISS (æœ¬åœ°)",
            "llm_provider": "local (GPU)" if torch.cuda.is_available() else "local (CPU)",
            "reranker_enabled": enable_rerank,
            "reranker_type": reranker_type if enable_rerank else None,
            "reranker_top_k": 5 if enable_rerank else None,
            "query_enhanced": enhanced_query != query,
        }

        retrieved_text = " ".join([r.content for r in results[:5]])
        hits = sum(1 for kw in expected_keywords if kw in retrieved_text)
        hit_rate = hits / len(expected_keywords) if expected_keywords else 0

        matched_keywords = [kw for kw in expected_keywords if kw in retrieved_text]
        missed_keywords = [kw for kw in expected_keywords if kw not in retrieved_text]

        precision_at_1 = self.calculate_precision_at_k(results, ground_truth, 1)
        precision_at_3 = self.calculate_precision_at_k(results, ground_truth, 3)
        precision_at_5 = self.calculate_precision_at_k(results, ground_truth, 5)

        recall_at_5 = self.calculate_recall_at_k(results, ground_truth, k=5)
        f1_at_5 = self.calculate_f1_at_k(precision_at_5, recall_at_5)
        ndcg_at_5 = self.calculate_ndcg_at_k(results, ground_truth, k=5)
        mrr = self.calculate_mrr(results, ground_truth)

        semantic_similarity = 0.0
        if ground_truth and results:
            semantic_similarity = self.calculate_semantic_similarity(
                query, results[0].content
            )

        expected_topics = case_info.get("expected_topics", [])
        topic_coverage = self.calculate_topic_coverage(results, expected_topics)

        model_info = {
            "embedding_model": "BAAI/bge-base-zh-v1.5 (æœ¬åœ°)",
            "vector_db": "FAISS (æœ¬åœ°)",
            "llm_provider": "local (GPU)"
            if torch.cuda.is_available()
            else "local (CPU)",
            "reranker_enabled": enable_rerank,
            "reranker_type": reranker_type if enable_rerank else None,
            "reranker_top_k": 5 if enable_rerank else None,
            "query_enhanced": enhanced_query != query,
        }

        # æ­¥éª¤10: LLMç”Ÿæˆç­”æ¡ˆå’Œç­”æ¡ˆè´¨é‡è¯„ä¼°
        llm_result = None
        answer_metrics = None
        ground_truth_text = ground_truth[0] if isinstance(ground_truth, list) and ground_truth else ""
        
        if ground_truth_text and len(results) > 0:
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            context = "\n\n".join([f"[{i+1}] {r.content}" for i, r in enumerate(results[:5])])
            
            # ç”ŸæˆLLMç­”æ¡ˆ
            llm_result = self.generate_llm_answer(query, context, verbose)
            
            # è¯„ä¼°ç­”æ¡ˆè´¨é‡
            if llm_result.get("success") and llm_result.get("answer"):
                answer_metrics = self.evaluate_answer_quality(
                    ground_truth_text, 
                    llm_result["answer"], 
                    llm_result,  # ä¼ å…¥å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡
                    verbose
                )
                # æ›´æ–°model_info
                model_info['llm_generation_time_ms'] = llm_result.get('generation_time_ms', 0)
                model_info['llm_tokens_per_second'] = llm_result.get('tokens_per_second', 0)
                model_info['llm_input_tokens'] = llm_result.get('input_tokens', 0)
                model_info['llm_output_tokens'] = llm_result.get('output_tokens', 0)

        return {
            "case_info": case_info,
            "query": query,
            "enhanced_query": enhanced_query,
            "response_time_ms": elapsed,
            "results_count": len(results[:5]),
            "results": [
                {
                    "rank": r.rank,
                    "similarity": r.similarity,
                    "rerank_score": r.rerank_score,
                    "content": r.content[:80] + "..."
                    if len(r.content) > 80
                    else r.content,
                    "chunk_id": r.chunk_id,
                    "document_id": r.document_id,
                }
                for r in results[:5]
            ],
            "keyword_analysis": {
                "hit_rate": hit_rate,
                "hits": hits,
                "total_keywords": len(expected_keywords),
                "matched": matched_keywords,
                "missed": missed_keywords,
            },
            "topic_coverage": topic_coverage,
            "metrics": {
                "precision_at_1": precision_at_1,
                "precision_at_3": precision_at_3,
                "precision_at_5": precision_at_5,
                "recall_at_5": recall_at_5,
                "f1_at_5": f1_at_5,
                "ndcg_at_5": ndcg_at_5,
                "mrr": mrr,
                "context_precision": precision_at_5,
                "context_recall": recall_at_5,
                "semantic_similarity": semantic_similarity,
            },
            "llm_answer": llm_result.get("answer") if llm_result else None,
            "llm_generation": llm_result,
            "answer_metrics": answer_metrics,
            "model_info": model_info,
        }

    def generate_llm_answer(self, query: str, context: str, verbose: bool = True) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢ä¸Šä¸‹æ–‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            # æ„å»ºRAGæç¤º
            prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™ï¼Œå›ç­”é—®é¢˜ï¼š

å‚è€ƒèµ„æ–™ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·æ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ï¼Œå¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚"""
            
            if verbose:
                print(f"\nğŸ¤– LLMç”Ÿæˆç­”æ¡ˆ...")
                print(f"   ä½¿ç”¨ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info(f"LLMç”Ÿæˆç­”æ¡ˆ - ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            generation_config = GenerationConfig(
                llm_provider="local",
                llm_model="Qwen2.5-7B-Instruct",
                temperature=0.7,
                max_tokens=512
            )
            
            # è·å–LLMå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ç¼“å­˜æˆ–æ–°å»ºï¼‰
            llm_start = time.time()
            if self.keep_llm_loaded and self._llm_client is not None:
                # ä½¿ç”¨å·²ç¼“å­˜çš„å®¢æˆ·ç«¯
                llm_client = self._llm_client
                if verbose:
                    print(f"   ä½¿ç”¨å¸¸é©»æ˜¾å­˜çš„LLMæ¨¡å‹")
                logger.info("ä½¿ç”¨å¸¸é©»æ˜¾å­˜çš„LLMæ¨¡å‹")
            else:
                # æ–°å»ºå®¢æˆ·ç«¯
                llm_client = rag_generator._get_llm_client(generation_config)
                if self.keep_llm_loaded:
                    self._llm_client = llm_client
                    self._llm_config = generation_config
                    if verbose:
                        print(f"   LLMæ¨¡å‹å·²åŠ è½½åˆ°æ˜¾å­˜ï¼ˆå°†ä¿æŒå¸¸é©»ï¼‰")
                    logger.info("LLMæ¨¡å‹å·²åŠ è½½åˆ°æ˜¾å­˜ï¼ˆå°†ä¿æŒå¸¸é©»ï¼‰")
            
            generation_result = llm_client.generate(prompt)
            llm_elapsed = (time.time() - llm_start) * 1000
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬å’Œæ€§èƒ½æŒ‡æ ‡
            answer = generation_result.get("text", "")
            input_tokens = generation_result.get("input_tokens", 0)
            output_tokens = generation_result.get("output_tokens", 0)
            total_tokens = generation_result.get("total_tokens", 0)
            time_to_first_token_ms = generation_result.get("time_to_first_token_ms", 0)
            total_time_ms = generation_result.get("total_time_ms", 0)
            generation_time_ms = generation_result.get("generation_time_ms", 0)
            tokens_per_second = generation_result.get("tokens_per_second", 0)
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¸è½½æ¨¡å‹
            if not self.keep_llm_loaded:
                # ç”Ÿæˆå®Œæˆåå¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜
                if hasattr(llm_client, "unload"):
                    llm_client.unload()
            else:
                logger.info("LLMæ¨¡å‹ä¿æŒå¸¸é©»æ˜¾å­˜ï¼ˆæœªå¸è½½ï¼‰")
            
            if verbose:
                print(f"\nğŸ“Š LLMæ€§èƒ½æŒ‡æ ‡:")
                print(f"   è¾“å…¥Token: {input_tokens} | è¾“å‡ºToken: {output_tokens} | æ€»è®¡: {total_tokens}")
                print(f"   é¦–Tokenæ—¶å»¶: {time_to_first_token_ms:.1f}ms")
                print(f"   æ€»ç”Ÿæˆæ—¶é—´: {total_time_ms:.1f}ms")
                print(f"   âš¡ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.2f} tokens/s")
                print(f"   ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
                print(f"\nğŸ’¬ LLMå›ç­”:\n{answer[:200]}..." if len(answer) > 200 else f"\nğŸ’¬ LLMå›ç­”:\n{answer}")
            logger.info(f"LLMæ€§èƒ½ - è¾“å…¥Token: {input_tokens}, è¾“å‡ºToken: {output_tokens}, "
                       f"é¦–Tokenæ—¶å»¶: {time_to_first_token_ms:.1f}ms, "
                       f"ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.2f} tokens/s")
            logger.info(f"LLMå›ç­”: {answer[:500]}..." if len(answer) > 500 else f"LLMå›ç­”: {answer}")
            
            return {
                "answer": answer,
                "generation_time_ms": llm_elapsed,
                "answer_length": len(answer),
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "time_to_first_token_ms": time_to_first_token_ms,
                "total_time_ms": total_time_ms,
                "generation_time_ms": generation_time_ms,
                "tokens_per_second": tokens_per_second,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            if verbose:
                print(f"   âš ï¸ LLMç”Ÿæˆå¤±è´¥: {e}")
            return {
                "answer": "",
                "generation_time_ms": 0,
                "answer_length": 0,
                "success": False,
                "error": str(e)
            }
    
    def unload_llm_model(self):
        """æ‰‹åŠ¨å¸è½½LLMæ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜"""
        if self._llm_client is not None:
            if hasattr(self._llm_client, "unload"):
                self._llm_client.unload()
                logger.info("LLMæ¨¡å‹å·²æ‰‹åŠ¨å¸è½½")
            self._llm_client = None
            self._llm_config = None

    def evaluate_answer_quality(self, ground_truth: str, llm_answer: str, 
                                   llm_performance: Dict[str, Any] = None,
                                   verbose: bool = True) -> Dict[str, float]:
        """
        è¯„ä¼°LLMç­”æ¡ˆè´¨é‡
        
        Args:
            ground_truth: æ ‡å‡†ç­”æ¡ˆ
            llm_answer: LLMç”Ÿæˆçš„ç­”æ¡ˆ
            llm_performance: LLMç”Ÿæˆæ€§èƒ½æŒ‡æ ‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
            
        Returns:
            åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        try:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            metrics = calculate_answer_metrics(ground_truth, llm_answer)
            
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_sim = self.calculate_semantic_similarity(ground_truth, llm_answer)
            metrics['semantic_similarity'] = semantic_sim
            
            # æ·»åŠ æ€§èƒ½æŒ‡æ ‡åˆ°metricsä¸­
            if llm_performance:
                metrics['input_tokens'] = llm_performance.get('input_tokens', 0)
                metrics['output_tokens'] = llm_performance.get('output_tokens', 0)
                metrics['total_tokens'] = llm_performance.get('total_tokens', 0)
                metrics['time_to_first_token_ms'] = llm_performance.get('time_to_first_token_ms', 0)
                metrics['total_time_ms'] = llm_performance.get('total_time_ms', 0)
                metrics['generation_time_ms'] = llm_performance.get('generation_time_ms', 0)
                metrics['tokens_per_second'] = llm_performance.get('tokens_per_second', 0)
            
            if verbose:
                print(f"\nğŸ“Š ç­”æ¡ˆè´¨é‡è¯„ä¼°:")
                print(f"   BLEU-1: {metrics['bleu_1']:.3f} | BLEU-2: {metrics['bleu_2']:.3f} | BLEU-avg: {metrics['bleu_avg']:.3f}")
                print(f"   ROUGE-1: {metrics['rouge_1']:.3f} | ROUGE-2: {metrics['rouge_2']:.3f} | ROUGE-L: {metrics['rouge_l']:.3f}")
                print(f"   ROUGE-avg: {metrics['rouge_avg']:.3f}")
                print(f"   å­—ç¬¦ç²¾ç¡®ç‡: {metrics['char_precision']:.3f} | å¬å›ç‡: {metrics['char_recall']:.3f} | F1: {metrics['char_f1']:.3f}")
                print(f"   è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_sim:.3f}")
                print(f"   é•¿åº¦æ¯”: {metrics['length_ratio']:.2f}")
                
                # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
                if llm_performance:
                    print(f"\nâš¡ LLMæ€§èƒ½æŒ‡æ ‡:")
                    print(f"   è¾“å…¥Token: {llm_performance.get('input_tokens', 0)} | è¾“å‡ºToken: {llm_performance.get('output_tokens', 0)}")
                    print(f"   é¦–Tokenæ—¶å»¶: {llm_performance.get('time_to_first_token_ms', 0):.1f}ms")
                    print(f"   æ€»ç”Ÿæˆæ—¶é—´: {llm_performance.get('total_time_ms', 0):.1f}ms")
                    print(f"   æœ‰æ•ˆç”Ÿæˆé€Ÿåº¦: {llm_performance.get('tokens_per_second', 0):.2f} tokens/s")
            
            logger.info(f"ç­”æ¡ˆè´¨é‡è¯„ä¼°:")
            logger.info(f"  BLEU: {metrics['bleu_avg']:.3f} (BLEU-1: {metrics['bleu_1']:.3f}, BLEU-2: {metrics['bleu_2']:.3f})")
            logger.info(f"  ROUGE: {metrics['rouge_avg']:.3f} (ROUGE-1: {metrics['rouge_1']:.3f}, ROUGE-2: {metrics['rouge_2']:.3f}, ROUGE-L: {metrics['rouge_l']:.3f})")
            logger.info(f"  å­—ç¬¦çº§: ç²¾ç¡®ç‡={metrics['char_precision']:.3f}, å¬å›ç‡={metrics['char_recall']:.3f}, F1={metrics['char_f1']:.3f}")
            logger.info(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_sim:.3f}")
            logger.info(f"  é•¿åº¦æ¯”: {metrics['length_ratio']:.2f}")
            if llm_performance:
                logger.info(f"  LLMæ€§èƒ½: è¾“å…¥Token={llm_performance.get('input_tokens', 0)}, "
                           f"è¾“å‡ºToken={llm_performance.get('output_tokens', 0)}, "
                           f"é¦–Tokenæ—¶å»¶={llm_performance.get('time_to_first_token_ms', 0):.1f}ms, "
                           f"ç”Ÿæˆé€Ÿåº¦={llm_performance.get('tokens_per_second', 0):.2f} tokens/s")
            
            return metrics
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆè¯„ä¼°å¤±è´¥: {e}")
            return {
                'bleu_1': 0.0, 'bleu_2': 0.0, 'bleu_avg': 0.0,
                'rouge_1': 0.0, 'rouge_2': 0.0, 'rouge_l': 0.0, 'rouge_avg': 0.0,
                'char_precision': 0.0, 'char_recall': 0.0, 'char_f1': 0.0,
                'semantic_similarity': 0.0, 'length_ratio': 0.0,
                'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0,
                'time_to_first_token_ms': 0, 'total_time_ms': 0, 
                'generation_time_ms': 0, 'tokens_per_second': 0
            }

    def evaluate_retrieval_cases(
        self,
        test_cases: List[Dict],
        limit: Optional[int] = None,
        enable_rerank: bool = True,
        reranker_type: str = "bge",
        verbose: bool = True,
    ) -> List[Dict]:
        """è¯„ä¼°æ£€ç´¢æµ‹è¯•ç”¨ä¾‹
        
        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            limit: é™åˆ¶æµ‹è¯•æ•°é‡
            enable_rerank: æ˜¯å¦å¯ç”¨é‡æ’åº
            reranker_type: é‡æ’åºå™¨ç±»å‹
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
        """
        if limit:
            test_cases = test_cases[:limit]

        print(f"\nğŸ§ª è¿è¡Œæ£€ç´¢æµ‹è¯• ({len(test_cases)} æ¡):")
        if enable_rerank:
            print(f"ğŸ“ˆ é‡æ’åº: {reranker_type.upper()}")
        else:
            print("ğŸ“Š é‡æ’åº: ç¦ç”¨")
        print("-" * 80)

        results = []
        for i, case in enumerate(test_cases, 1):
            query = case["query"]
            keywords = case.get("expected_keywords", [])

            print(f"\n[{i:2d}/{len(test_cases)}] {query[:45]}...", end=" ")
            print(f"[{case.get('difficulty', 'unknown')}]")
            print("=" * 80)

            case_info = {
                "id": case["id"],
                "category": case.get("category", "unknown"),
                "difficulty": case.get("difficulty", "unknown"),
                "description": case.get("description", ""),
                "ground_truth": case.get("ground_truth", []),
            }

            try:
                result = self.run_retrieval_test(
                    query, keywords, case_info, enable_rerank, reranker_type, verbose
                )
                results.append(result)

                metrics = result["metrics"]
                keyword_analysis = result["keyword_analysis"]
                model_info = result["model_info"]

                status = (
                    "âœ…"
                    if keyword_analysis["hit_rate"] >= 0.6
                    and metrics["ndcg_at_5"] >= 0.5
                    else "âš ï¸"
                    if keyword_analysis["hit_rate"] >= 0.4
                    and metrics["ndcg_at_5"] >= 0.3
                    else "âŒ"
                )

                rerank_indicator = (
                    "ğŸ”„" if model_info.get("reranker_enabled", False) else "ğŸ“Š"
                )

                print(
                    f"\nğŸ“Š ç»“æœæ‘˜è¦: {rerank_indicator}{status} {result['response_time_ms']:.1f}ms | "
                    f"P@1:{metrics['precision_at_1']:.2f} | "
                    f"NDCG:{metrics['ndcg_at_5']:.2f} | "
                    f"å…³é”®è¯:{keyword_analysis['hit_rate']:.0%}"
                )

                if len(keyword_analysis["missed"]) > 0:
                    print(f"     æœªå‘½ä¸­å…³é”®è¯: {', '.join(keyword_analysis['missed'][:3])}")

            except Exception as e:
                print(f"\n     âŒ æµ‹è¯•å¤±è´¥: {str(e)[:50]}")
                logger.error(f"æµ‹è¯•å¤±è´¥ {case['id']}: {e}", exc_info=True)
                results.append(
                    {"case_info": case_info, "query": query, "error": str(e)}
                )
            
            print("-" * 80)

        return results

    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        valid_results = [r for r in results if "metrics" in r]

        if not valid_results:
            return {"error": "æ— æœ‰æ•ˆæµ‹è¯•ç»“æœ"}

        times = [r["response_time_ms"] for r in valid_results]
        p1s = [r["metrics"]["precision_at_1"] for r in valid_results]
        p3s = [r["metrics"]["precision_at_3"] for r in valid_results]
        p5s = [r["metrics"]["precision_at_5"] for r in valid_results]
        mrrs = [r["metrics"]["mrr"] for r in valid_results]
        hit_rates = [r["keyword_analysis"]["hit_rate"] for r in valid_results]

        recalls = [r["metrics"]["recall_at_5"] for r in valid_results]
        f1s = [r["metrics"]["f1_at_5"] for r in valid_results]
        ndcgs = [r["metrics"]["ndcg_at_5"] for r in valid_results]
        semantic_sims = [
            r["metrics"]["semantic_similarity"]
            for r in valid_results
            if r["metrics"]["semantic_similarity"] > 0
        ]
        topic_coverage_rates = [
            r["topic_coverage"]["coverage_rate"]
            for r in valid_results
            if r["topic_coverage"]["coverage_rate"] > 0
        ]
        
        # æ”¶é›† LLM æ€§èƒ½æŒ‡æ ‡
        llm_input_tokens = [
            r["answer_metrics"]["input_tokens"]
            for r in valid_results
            if r.get("answer_metrics") and r["answer_metrics"].get("input_tokens", 0) > 0
        ]
        llm_output_tokens = [
            r["answer_metrics"]["output_tokens"]
            for r in valid_results
            if r.get("answer_metrics") and r["answer_metrics"].get("output_tokens", 0) > 0
        ]
        llm_time_to_first_token = [
            r["answer_metrics"]["time_to_first_token_ms"]
            for r in valid_results
            if r.get("answer_metrics") and r["answer_metrics"].get("time_to_first_token_ms", 0) > 0
        ]
        llm_generation_time = [
            r["answer_metrics"]["generation_time_ms"]
            for r in valid_results
            if r.get("answer_metrics") and r["answer_metrics"].get("generation_time_ms", 0) > 0
        ]
        llm_tokens_per_second = [
            r["answer_metrics"]["tokens_per_second"]
            for r in valid_results
            if r.get("answer_metrics") and r["answer_metrics"].get("tokens_per_second", 0) > 0
        ]

        by_difficulty = {}
        by_category = {}

        for r in valid_results:
            diff = r["case_info"]["difficulty"]
            cat = r["case_info"]["category"]

            if diff not in by_difficulty:
                by_difficulty[diff] = {
                    "p1": [],
                    "hit": [],
                    "mrr": [],
                    "time": [],
                    "recall": [],
                    "f1": [],
                    "ndcg": [],
                    "llm_input_tokens": [],
                    "llm_output_tokens": [],
                    "llm_ttft": [],
                    "llm_generation_time": [],
                    "llm_tokens_per_second": [],
                }
            by_difficulty[diff]["p1"].append(r["metrics"]["precision_at_1"])
            by_difficulty[diff]["hit"].append(r["keyword_analysis"]["hit_rate"])
            by_difficulty[diff]["mrr"].append(r["metrics"]["mrr"])
            by_difficulty[diff]["time"].append(r["response_time_ms"])
            by_difficulty[diff]["recall"].append(r["metrics"]["recall_at_5"])
            by_difficulty[diff]["f1"].append(r["metrics"]["f1_at_5"])
            by_difficulty[diff]["ndcg"].append(r["metrics"]["ndcg_at_5"])
            
            # æ”¶é›†LLMæ€§èƒ½æ•°æ®
            if r.get("answer_metrics"):
                am = r["answer_metrics"]
                if am.get("input_tokens", 0) > 0:
                    by_difficulty[diff]["llm_input_tokens"].append(am["input_tokens"])
                if am.get("output_tokens", 0) > 0:
                    by_difficulty[diff]["llm_output_tokens"].append(am["output_tokens"])
                if am.get("time_to_first_token_ms", 0) > 0:
                    by_difficulty[diff]["llm_ttft"].append(am["time_to_first_token_ms"])
                if am.get("generation_time_ms", 0) > 0:
                    by_difficulty[diff]["llm_generation_time"].append(am["generation_time_ms"])
                if am.get("tokens_per_second", 0) > 0:
                    by_difficulty[diff]["llm_tokens_per_second"].append(am["tokens_per_second"])

            if cat not in by_category:
                by_category[cat] = {
                    "p1": [],
                    "hit": [],
                    "mrr": [],
                    "recall": [],
                    "f1": [],
                    "ndcg": [],
                    "llm_input_tokens": [],
                    "llm_output_tokens": [],
                    "llm_ttft": [],
                    "llm_generation_time": [],
                    "llm_tokens_per_second": [],
                }
            by_category[cat]["p1"].append(r["metrics"]["precision_at_1"])
            by_category[cat]["hit"].append(r["keyword_analysis"]["hit_rate"])
            by_category[cat]["mrr"].append(r["metrics"]["mrr"])
            by_category[cat]["recall"].append(r["metrics"]["recall_at_5"])
            by_category[cat]["f1"].append(r["metrics"]["f1_at_5"])
            by_category[cat]["ndcg"].append(r["metrics"]["ndcg_at_5"])
            
            # æ”¶é›†LLMæ€§èƒ½æ•°æ®
            if r.get("answer_metrics"):
                am = r["answer_metrics"]
                if am.get("input_tokens", 0) > 0:
                    by_category[cat]["llm_input_tokens"].append(am["input_tokens"])
                if am.get("output_tokens", 0) > 0:
                    by_category[cat]["llm_output_tokens"].append(am["output_tokens"])
                if am.get("time_to_first_token_ms", 0) > 0:
                    by_category[cat]["llm_ttft"].append(am["time_to_first_token_ms"])
                if am.get("generation_time_ms", 0) > 0:
                    by_category[cat]["llm_generation_time"].append(am["generation_time_ms"])
                if am.get("tokens_per_second", 0) > 0:
                    by_category[cat]["llm_tokens_per_second"].append(am["tokens_per_second"])

        poor_cases = [
            r
            for r in valid_results
            if r["keyword_analysis"]["hit_rate"] < 0.4
            or r["metrics"]["ndcg_at_5"] < 0.3
        ]
        good_cases = [
            r
            for r in valid_results
            if r["keyword_analysis"]["hit_rate"] >= 0.8
            and r["metrics"]["ndcg_at_5"] >= 0.6
        ]
        failed_cases = [r for r in results if "error" in r]

        return {
            "total_tests": len(results),
            "valid_tests": len(valid_results),
            "failed_tests": len(failed_cases),
            "statistics": {
                "avg_response_time_ms": round(statistics.mean(times), 1)
                if times
                else 0,
                "p95_response_time_ms": round(sorted(times)[int(len(times) * 0.95)])
                if times
                else 0,
                "avg_precision_at_1": round(statistics.mean(p1s), 3) if p1s else 0,
                "avg_precision_at_3": round(statistics.mean(p3s), 3) if p3s else 0,
                "avg_precision_at_5": round(statistics.mean(p5s), 3) if p5s else 0,
                "avg_mrr": round(statistics.mean(mrrs), 3) if mrrs else 0,
                "avg_keyword_hit_rate": round(statistics.mean(hit_rates), 3)
                if hit_rates
                else 0,
                "avg_recall_at_5": round(statistics.mean(recalls), 3) if recalls else 0,
                "avg_f1_at_5": round(statistics.mean(f1s), 3) if f1s else 0,
                "avg_ndcg_at_5": round(statistics.mean(ndcgs), 3) if ndcgs else 0,
                "avg_semantic_similarity": round(statistics.mean(semantic_sims), 3)
                if semantic_sims
                else 0,
                "avg_topic_coverage": round(statistics.mean(topic_coverage_rates), 3)
                if topic_coverage_rates
                else 0,
                # LLM æ€§èƒ½æŒ‡æ ‡
                "avg_llm_input_tokens": round(statistics.mean(llm_input_tokens), 1)
                if llm_input_tokens
                else 0,
                "avg_llm_output_tokens": round(statistics.mean(llm_output_tokens), 1)
                if llm_output_tokens
                else 0,
                "avg_time_to_first_token_ms": round(statistics.mean(llm_time_to_first_token), 1)
                if llm_time_to_first_token
                else 0,
                "avg_generation_time_ms": round(statistics.mean(llm_generation_time), 1)
                if llm_generation_time
                else 0,
                "avg_tokens_per_second": round(statistics.mean(llm_tokens_per_second), 2)
                if llm_tokens_per_second
                else 0,
            },
            "by_difficulty": {
                diff: {
                    "count": len(stats["p1"]),
                    "avg_precision_at_1": round(
                        statistics.mean(stats["p1"]) if stats["p1"] else 0, 3
                    ),
                    "avg_keyword_hit_rate": round(
                        statistics.mean(stats["hit"]) if stats["hit"] else 0, 3
                    ),
                    "avg_mrr": round(
                        statistics.mean(stats["mrr"]) if stats["mrr"] else 0, 3
                    ),
                    "avg_response_time_ms": round(
                        statistics.mean(stats["time"]) if stats["time"] else 0, 1
                    ),
                    "avg_recall_at_5": round(
                        statistics.mean(stats["recall"]) if stats["recall"] else 0, 3
                    ),
                    "avg_f1_at_5": round(
                        statistics.mean(stats["f1"]) if stats["f1"] else 0, 3
                    ),
                    "avg_ndcg_at_5": round(
                        statistics.mean(stats["ndcg"]) if stats["ndcg"] else 0, 3
                    ),
                    # LLM æ€§èƒ½æŒ‡æ ‡
                    "avg_llm_input_tokens": round(
                        statistics.mean(stats["llm_input_tokens"]) if stats["llm_input_tokens"] else 0, 1
                    ),
                    "avg_llm_output_tokens": round(
                        statistics.mean(stats["llm_output_tokens"]) if stats["llm_output_tokens"] else 0, 1
                    ),
                    "avg_llm_tokens_per_second": round(
                        statistics.mean(stats["llm_tokens_per_second"]) if stats["llm_tokens_per_second"] else 0, 2
                    ),
                    "avg_llm_ttft_ms": round(
                        statistics.mean(stats["llm_ttft"]) if stats["llm_ttft"] else 0, 1
                    ),
                }
                for diff, stats in by_difficulty.items()
            },
            "by_category": {
                cat: {
                    "count": len(stats["p1"]),
                    "avg_precision_at_1": round(
                        statistics.mean(stats["p1"]) if stats["p1"] else 0, 3
                    ),
                    "avg_keyword_hit_rate": round(
                        statistics.mean(stats["hit"]) if stats["hit"] else 0, 3
                    ),
                    "avg_mrr": round(
                        statistics.mean(stats["mrr"]) if stats["mrr"] else 0, 3
                    ),
                    "avg_recall_at_5": round(
                        statistics.mean(stats["recall"]) if stats["recall"] else 0, 3
                    ),
                    "avg_f1_at_5": round(
                        statistics.mean(stats["f1"]) if stats["f1"] else 0, 3
                    ),
                    "avg_ndcg_at_5": round(
                        statistics.mean(stats["ndcg"]) if stats["ndcg"] else 0, 3
                    ),
                    # LLM æ€§èƒ½æŒ‡æ ‡
                    "avg_llm_input_tokens": round(
                        statistics.mean(stats["llm_input_tokens"]) if stats["llm_input_tokens"] else 0, 1
                    ),
                    "avg_llm_output_tokens": round(
                        statistics.mean(stats["llm_output_tokens"]) if stats["llm_output_tokens"] else 0, 1
                    ),
                    "avg_llm_tokens_per_second": round(
                        statistics.mean(stats["llm_tokens_per_second"]) if stats["llm_tokens_per_second"] else 0, 2
                    ),
                    "avg_llm_ttft_ms": round(
                        statistics.mean(stats["llm_ttft"]) if stats["llm_ttft"] else 0, 1
                    ),
                }
                for cat, stats in by_category.items()
            },
            "problem_cases": [
                {
                    "id": r["case_info"]["id"],
                    "query": r["query"],
                    "hit_rate": r["keyword_analysis"]["hit_rate"],
                    "missed_keywords": r["keyword_analysis"]["missed"],
                    "precision_at_1": r["metrics"]["precision_at_1"],
                    "ndcg_at_5": r["metrics"]["ndcg_at_5"],
                    "topic_coverage": r["topic_coverage"]["coverage_rate"],
                }
                for r in poor_cases
            ],
            "good_cases": [
                {
                    "id": r["case_info"]["id"],
                    "query": r["query"],
                    "hit_rate": r["keyword_analysis"]["hit_rate"],
                    "precision_at_1": r["metrics"]["precision_at_1"],
                    "ndcg_at_5": r["metrics"]["ndcg_at_5"],
                    "topic_coverage": r["topic_coverage"]["coverage_rate"],
                }
                for r in good_cases
            ],
            "failed_cases": [
                {"id": r["case_info"]["id"], "query": r["query"], "error": r["error"]}
                for r in failed_cases
            ],
        }

    def calculate_score(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        stats = analysis.get("statistics", {})

        score = 0
        grade_descriptions = []

        avg_p1 = stats.get("avg_precision_at_1", 0)
        if avg_p1 >= 0.7:
            score += 20
            grade_descriptions.append("ğŸŸ¢ P@1 ç²¾ç¡®ç‡ä¼˜ç§€ (+20)")
        elif avg_p1 >= 0.5:
            score += 15
            grade_descriptions.append("ğŸŸ¡ P@1 ç²¾ç¡®ç‡è‰¯å¥½ (+15)")
        elif avg_p1 >= 0.3:
            score += 8
            grade_descriptions.append("ğŸŸ  P@1 ç²¾ç¡®ç‡ä¸€èˆ¬ (+8)")

        avg_ndcg = stats.get("avg_ndcg_at_5", 0)
        if avg_ndcg >= 0.7:
            score += 20
            grade_descriptions.append("ğŸŸ¢ NDCG@5 æ’åºè´¨é‡ä¼˜ç§€ (+20)")
        elif avg_ndcg >= 0.5:
            score += 15
            grade_descriptions.append("ğŸŸ¡ NDCG@5 æ’åºè´¨é‡è‰¯å¥½ (+15)")
        elif avg_ndcg >= 0.3:
            score += 8
            grade_descriptions.append("ğŸŸ  NDCG@5 æ’åºè´¨é‡ä¸€èˆ¬ (+8)")

        avg_f1 = stats.get("avg_f1_at_5", 0)
        if avg_f1 >= 0.7:
            score += 20
            grade_descriptions.append("ğŸŸ¢ F1@5 å¹³è¡¡æŒ‡æ ‡ä¼˜ç§€ (+20)")
        elif avg_f1 >= 0.5:
            score += 15
            grade_descriptions.append("ğŸŸ¡ F1@5 å¹³è¡¡æŒ‡æ ‡è‰¯å¥½ (+15)")
        elif avg_f1 >= 0.3:
            score += 8
            grade_descriptions.append("ğŸŸ  F1@5 å¹³è¡¡æŒ‡æ ‡ä¸€èˆ¬ (+8)")

        avg_hit = stats.get("avg_keyword_hit_rate", 0)
        if avg_hit >= 0.8:
            score += 15
            grade_descriptions.append("ğŸŸ¢ å…³é”®è¯å‘½ä¸­ç‡ä¼˜ç§€ (+15)")
        elif avg_hit >= 0.6:
            score += 10
            grade_descriptions.append("ğŸŸ¡ å…³é”®è¯å‘½ä¸­ç‡è‰¯å¥½ (+10)")
        elif avg_hit >= 0.4:
            score += 5
            grade_descriptions.append("ğŸŸ  å…³é”®è¯å‘½ä¸­ç‡ä¸€èˆ¬ (+5)")

        avg_topic = stats.get("avg_topic_coverage", 0)
        if avg_topic >= 0.8:
            score += 10
            grade_descriptions.append("ğŸŸ¢ ä¸»é¢˜è¦†ç›–ç‡ä¼˜ç§€ (+10)")
        elif avg_topic >= 0.6:
            score += 7
            grade_descriptions.append("ğŸŸ¡ ä¸»é¢˜è¦†ç›–ç‡è‰¯å¥½ (+7)")
        elif avg_topic >= 0.4:
            score += 3
            grade_descriptions.append("ğŸŸ  ä¸»é¢˜è¦†ç›–ç‡ä¸€èˆ¬ (+3)")

        avg_semantic = stats.get("avg_semantic_similarity", 0)
        if avg_semantic >= 0.8:
            score += 5
            grade_descriptions.append("ğŸŸ¢ è¯­ä¹‰ç›¸ä¼¼åº¦ä¼˜ç§€ (+5)")
        elif avg_semantic >= 0.6:
            score += 3
            grade_descriptions.append("ğŸŸ¡ è¯­ä¹‰ç›¸ä¼¼åº¦è‰¯å¥½ (+3)")

        avg_mrr = stats.get("avg_mrr", 0)
        if avg_mrr >= 0.5:
            score += 5
            grade_descriptions.append("ğŸŸ¢ MRR ä¼˜ç§€ (+5)")

        avg_time = stats.get("avg_response_time_ms", 0)
        if avg_time <= 100:
            score += 5
            grade_descriptions.append("ğŸŸ¢ å“åº”é€Ÿåº¦ä¼˜ç§€ (+5)")
        elif avg_time <= 500:
            score += 3
            grade_descriptions.append("ğŸŸ¡ å“åº”é€Ÿåº¦è‰¯å¥½ (+3)")

        if score >= 85:
            grade = "ğŸŸ¢ ä¼˜ç§€"
        elif score >= 70:
            grade = "ğŸŸ¡ è‰¯å¥½"
        elif score >= 55:
            grade = "ğŸŸ  ä¸€èˆ¬"
        else:
            grade = "ğŸ”´ éœ€æ”¹è¿›"

        return {
            "total_score": score,
            "max_score": 100,
            "grade": grade,
            "grade_descriptions": grade_descriptions,
        }

    def save_report(
        self,
        results: List[Dict],
        analysis: Dict[str, Any],
        score: Dict[str, Any],
        test_data_info: Dict[str, Any],
        test_file: str,
        baseline_results: Optional[List[Dict]] = None,
        baseline_analysis: Optional[Dict[str, Any]] = None,
        baseline_score: Optional[Dict[str, Any]] = None,
    ) -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = self.evaluation_time.strftime("%Y%m%d_%H%M%S")

        reranker_enabled = False
        if results and results[0]["model_info"].get("reranker_enabled"):
            reranker_enabled = True

        report_data = {
            "evaluation_info": {
                "timestamp": self.evaluation_time.isoformat(),
                "test_file": str(test_file),
                "evaluator": "enhanced_eval.py",
                "version": "2.1-autodl",
                "reranker_enabled": reranker_enabled,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "dataset_info": test_data_info,
            "score_info": score,
            "analysis": analysis,
            "detailed_results": results,
        }

        if baseline_results:
            report_data["baseline_results"] = baseline_results
            report_data["baseline_analysis"] = baseline_analysis
            report_data["baseline_score"] = baseline_score

        json_file = self.output_dir / f"rag_evaluation_report_{timestamp}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        md_file = self.output_dir / f"rag_evaluation_summary_{timestamp}.md"
        md_content = self.generate_markdown_report(
            results,
            analysis,
            score,
            test_data_info,
            timestamp,
            baseline_results=baseline_results,
            baseline_analysis=baseline_analysis,
            baseline_score=baseline_score,
        )
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(md_content)

        return str(json_file)

    def generate_markdown_report(
        self,
        results: List[Dict],
        analysis: Dict[str, Any],
        score: Dict[str, Any],
        test_data_info: Dict[str, Any],
        timestamp: str,
        baseline_results: Optional[List[Dict]] = None,
        baseline_analysis: Optional[Dict[str, Any]] = None,
        baseline_score: Optional[Dict[str, Any]] = None,
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        stats = analysis["statistics"]

        reranker_enabled = False
        reranker_info = ""
        if results and results[0]["model_info"].get("reranker_enabled"):
            reranker_enabled = True
            reranker_info = (
                f" ({results[0]['model_info'].get('reranker_type', 'unknown')} é‡æ’åº)"
            )

        device_info = "GPU" if torch.cuda.is_available() else "CPU"

        md = f"""# RAGç³»ç»Ÿæµ‹è¯„æŠ¥å‘Š{reranker_info}

## ğŸ“Š æµ‹è¯„æ¦‚è§ˆ

- **æµ‹è¯„æ—¶é—´**: {self.evaluation_time.strftime("%Y-%m-%d %H:%M:%S")}
- **è¿è¡Œè®¾å¤‡**: {device_info}
- **æ•°æ®é›†ç‰ˆæœ¬**: {test_data_info.get("version", "unknown")}
- **æµ‹è¯•ç”¨ä¾‹æ€»æ•°**: {analysis["total_tests"]}
- **æœ‰æ•ˆæµ‹è¯•**: {analysis["valid_tests"]}
- **å¤±è´¥æµ‹è¯•**: {analysis["failed_tests"]}
- **é‡æ’åº**: {"å¯ç”¨" if reranker_enabled else "ç¦ç”¨"}

## ğŸ† ç»¼åˆè¯„åˆ†

**{score["grade"]} - {score["total_score"]}/100**

"""

        for desc in score["grade_descriptions"]:
            md += f"- {desc}\n"

        if baseline_analysis and baseline_score:
            baseline_stats = baseline_analysis["statistics"]
            md += f"""
### ğŸ”„ é‡æ’åºæ•ˆæœå¯¹æ¯”

| æŒ‡æ ‡ | åŸºç¡€æ£€ç´¢ | é‡æ’åº | æ”¹è¿› |
|------|----------|--------|------|
| **ç»¼åˆè¯„åˆ†** | {baseline_score["total_score"]}/100 | {score["total_score"]}/100 | **{score["total_score"] - baseline_score["total_score"]:+d}** |
| **è¯„çº§** | {baseline_score["grade"]} | {score["grade"]} | {"â¬†ï¸ æå‡" if score["total_score"] > baseline_score["total_score"] else "â¬‡ï¸ ä¸‹é™" if score["total_score"] < baseline_score["total_score"] else "â¡ï¸ æŒå¹³"} |
| P@1 ç²¾ç¡®ç‡ | {baseline_stats["avg_precision_at_1"]:.3f} | {stats["avg_precision_at_1"]:.3f} | {stats["avg_precision_at_1"] - baseline_stats["avg_precision_at_1"]:+.3f} |
| NDCG@5 | {baseline_stats.get("avg_ndcg_at_5", 0):.3f} | {stats.get("avg_ndcg_at_5", 0):.3f} | {stats.get("avg_ndcg_at_5", 0) - baseline_stats.get("avg_ndcg_at_5", 0):+.3f} |
| F1@5 | {baseline_stats.get("avg_f1_at_5", 0):.3f} | {stats.get("avg_f1_at_5", 0):.3f} | {stats.get("avg_f1_at_5", 0) - baseline_stats.get("avg_f1_at_5", 0):+.3f} |
| å…³é”®è¯å‘½ä¸­ç‡ | {baseline_stats["avg_keyword_hit_rate"]:.1%} | {stats["avg_keyword_hit_rate"]:.1%} | {stats["avg_keyword_hit_rate"] - baseline_stats["avg_keyword_hit_rate"]:+.1%} |
| å“åº”æ—¶é—´ | {baseline_stats["avg_response_time_ms"]:.1f}ms | {stats["avg_response_time_ms"]:.1f}ms | {stats["avg_response_time_ms"] - baseline_stats["avg_response_time_ms"]:+.1f}ms |

"""

        md += f"""
## ğŸ“ˆ å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯„ä»· |
|------|------|------|
| å¹³å‡å“åº”æ—¶é—´ | {stats["avg_response_time_ms"]:.1f}ms | {"ä¼˜ç§€" if stats["avg_response_time_ms"] <= 100 else "è‰¯å¥½" if stats["avg_response_time_ms"] <= 500 else "ä¸€èˆ¬"} |
| P@1 ç²¾ç¡®ç‡ | {stats["avg_precision_at_1"]:.3f} | {"ä¼˜ç§€" if stats["avg_precision_at_1"] >= 0.7 else "è‰¯å¥½" if stats["avg_precision_at_1"] >= 0.5 else "ä¸€èˆ¬"} |
| P@3 ç²¾ç¡®ç‡ | {stats["avg_precision_at_3"]:.3f} | {"ä¼˜ç§€" if stats["avg_precision_at_3"] >= 0.7 else "è‰¯å¥½" if stats["avg_precision_at_3"] >= 0.5 else "ä¸€èˆ¬"} |
| P@5 ç²¾ç¡®ç‡ | {stats["avg_precision_at_5"]:.3f} | {"ä¼˜ç§€" if stats["avg_precision_at_5"] >= 0.8 else "è‰¯å¥½" if stats["avg_precision_at_5"] >= 0.6 else "ä¸€èˆ¬"} |
| **Recall@5** | **{stats.get("avg_recall_at_5", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_recall_at_5", 0) >= 0.7 else "è‰¯å¥½" if stats.get("avg_recall_at_5", 0) >= 0.5 else "ä¸€èˆ¬"} |
| **F1@5** | **{stats.get("avg_f1_at_5", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_f1_at_5", 0) >= 0.7 else "è‰¯å¥½" if stats.get("avg_f1_at_5", 0) >= 0.5 else "ä¸€èˆ¬"} |
| **NDCG@5** | **{stats.get("avg_ndcg_at_5", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_ndcg_at_5", 0) >= 0.7 else "è‰¯å¥½" if stats.get("avg_ndcg_at_5", 0) >= 0.5 else "ä¸€èˆ¬"} |
| MRR | {stats["avg_mrr"]:.3f} | {"ä¼˜ç§€" if stats["avg_mrr"] >= 0.5 else "è‰¯å¥½" if stats["avg_mrr"] >= 0.3 else "ä¸€èˆ¬"} |
| å…³é”®è¯å‘½ä¸­ç‡ | {stats["avg_keyword_hit_rate"]:.1%} | {"ä¼˜ç§€" if stats["avg_keyword_hit_rate"] >= 0.8 else "è‰¯å¥½" if stats["avg_keyword_hit_rate"] >= 0.6 else "ä¸€èˆ¬"} |
| **ä¸»é¢˜è¦†ç›–ç‡** | **{stats.get("avg_topic_coverage", 0):.1%}** | {"ä¼˜ç§€" if stats.get("avg_topic_coverage", 0) >= 0.8 else "è‰¯å¥½" if stats.get("avg_topic_coverage", 0) >= 0.6 else "ä¸€èˆ¬"} |
| **è¯­ä¹‰ç›¸ä¼¼åº¦** | **{stats.get("avg_semantic_similarity", 0):.3f}** | {"ä¼˜ç§€" if stats.get("avg_semantic_similarity", 0) >= 0.8 else "è‰¯å¥½" if stats.get("avg_semantic_similarity", 0) >= 0.6 else "ä¸€èˆ¬"} |

### ğŸ†• æ–°å¢æŒ‡æ ‡è¯´æ˜

- **Recall@5**: å¬å›ç‡@5ï¼Œè¡¡é‡æ£€ç´¢ç»“æœçš„å®Œæ•´æ€§
- **F1@5**: F1åˆ†æ•°@5ï¼Œç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **NDCG@5**: å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š@5ï¼Œè€ƒè™‘æ’åºè´¨é‡
- **ä¸»é¢˜è¦†ç›–ç‡**: expected_topicsçš„è¦†ç›–æƒ…å†µåˆ†æ
- **è¯­ä¹‰ç›¸ä¼¼åº¦**: ä½¿ç”¨embeddingæ¨¡å‹è®¡ç®—ç­”æ¡ˆç›¸ä¼¼åº¦

## âš¡ LLM ç”Ÿæˆæ€§èƒ½

| æŒ‡æ ‡ | æ•°å€¼ | è¯„ä»· |
|------|------|------|
| å¹³å‡è¾“å…¥ Token | {stats.get('avg_llm_input_tokens', 0):.1f} | prompt é•¿åº¦ |
| å¹³å‡è¾“å‡º Token | {stats.get('avg_llm_output_tokens', 0):.1f} | ç”Ÿæˆé•¿åº¦ |
| é¦– Token æ—¶å»¶ | {stats.get('avg_time_to_first_token_ms', 0):.1f}ms | {"ä¼˜ç§€" if stats.get('avg_time_to_first_token_ms', 0) < 1000 else "è‰¯å¥½" if stats.get('avg_time_to_first_token_ms', 0) < 2000 else "ä¸€èˆ¬"} |
| å¹³å‡ç”Ÿæˆæ—¶é—´ | {stats.get('avg_generation_time_ms', 0):.1f}ms | çº¯ç”Ÿæˆé˜¶æ®µè€—æ—¶ |
| **ç”Ÿæˆé€Ÿåº¦** | **{stats.get('avg_tokens_per_second', 0):.2f} tokens/s** | {"ä¼˜ç§€" if stats.get('avg_tokens_per_second', 0) > 25 else "è‰¯å¥½" if stats.get('avg_tokens_per_second', 0) > 15 else "ä¸€èˆ¬"} |

### ğŸ“ æ€§èƒ½æŒ‡æ ‡è¯´æ˜

- **è¾“å…¥ Token**: é€å…¥æ¨¡å‹çš„ prompt token æ•°é‡
- **è¾“å‡º Token**: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ token æ•°é‡
- **é¦– Token æ—¶å»¶ (TTFT)**: ä»è¯·æ±‚å‘é€åˆ°é¦–ä¸ª token ç”Ÿæˆçš„æ—¶é—´ï¼ˆåæ˜ æ¨¡å‹åŠ è½½å’Œé¢„çƒ­é€Ÿåº¦ï¼‰
- **ç”Ÿæˆæ—¶é—´**: ä»é¦–ä¸ª token åˆ°ç”Ÿæˆç»“æŸçš„çº¯ç”Ÿæˆé˜¶æ®µæ—¶é—´
- **ç”Ÿæˆé€Ÿåº¦**: output_tokens / generation_timeï¼Œæ¨¡å‹è§£ç æ•ˆç‡çš„æ ¸å¿ƒæŒ‡æ ‡

### ğŸ¯ æ€§èƒ½è¯„ä»·æ ‡å‡†

| æŒ‡æ ‡ | ä¼˜ç§€ | è‰¯å¥½ | ä¸€èˆ¬ |
|------|------|------|------|
| ç”Ÿæˆé€Ÿåº¦ | > 25 tokens/s | 15-25 tokens/s | < 15 tokens/s |
| é¦– Token æ—¶å»¶ | < 1000ms | 1000-2000ms | > 2000ms |
| è¾“å‡º Token æ•° | 100-300 | 50-100 æˆ– 300-500 | < 50 æˆ– > 500 |

## ğŸ“Š æŒ‰éš¾åº¦åˆ†æ

"""

        for diff, diff_stats in analysis["by_difficulty"].items():
            md += f"### {diff.upper()}\n"
            md += f"- æµ‹è¯•æ•°é‡: {diff_stats['count']}\n"
            md += f"- P@1 ç²¾ç¡®ç‡: {diff_stats['avg_precision_at_1']:.3f}\n"
            md += f"- **NDCG@5**: {diff_stats['avg_ndcg_at_5']:.3f}\n"
            md += f"- **F1@5**: {diff_stats['avg_f1_at_5']:.3f}\n"
            md += f"- **Recall@5**: {diff_stats['avg_recall_at_5']:.3f}\n"
            md += f"- å…³é”®è¯å‘½ä¸­ç‡: {diff_stats['avg_keyword_hit_rate']:.1%}\n"
            md += f"- å¹³å‡å“åº”æ—¶é—´: {diff_stats['avg_response_time_ms']:.1f}ms\n"
            # æ·»åŠ LLMæ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
            if diff_stats.get('avg_llm_tokens_per_second', 0) > 0:
                md += f"- **ç”Ÿæˆé€Ÿåº¦**: {diff_stats['avg_llm_tokens_per_second']:.2f} tokens/s\n"
                md += f"- é¦–Tokenæ—¶å»¶: {diff_stats.get('avg_llm_ttft_ms', 0):.1f}ms\n"
                md += f"- å¹³å‡è¾“å‡ºToken: {diff_stats.get('avg_llm_output_tokens', 0):.1f}\n"
            md += "\n"

        if analysis["problem_cases"]:
            md += "## âš ï¸ é—®é¢˜ç”¨ä¾‹åˆ†æ\n\n"
            for case in analysis["problem_cases"][:5]:
                md += f"### {case['id']}\n"
                md += f"**æŸ¥è¯¢**: {case['query']}\n"
                md += f"**å…³é”®è¯å‘½ä¸­ç‡**: {case['hit_rate']:.1%}\n"
                md += f"**NDCG@5**: {case['ndcg_at_5']:.3f}\n"
                md += f"**ä¸»é¢˜è¦†ç›–ç‡**: {case['topic_coverage']:.1%}\n"
                md += f"**æœªå‘½ä¸­å…³é”®è¯**: {', '.join(case['missed_keywords'])}\n"
                md += f"**P@1**: {case['precision_at_1']:.3f}\n\n"

        md += """
## ğŸ“ æ–‡ä»¶è¯´æ˜

- `rag_evaluation_report_{timestamp}.json`: å®Œæ•´æµ‹è¯•æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
- `rag_evaluation_summary_{timestamp}.md`: æœ¬æ‘˜è¦æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

"""

        if stats["avg_precision_at_1"] < 0.6:
            md += "- ğŸ”´ æ£€ç´¢ç²¾ç¡®ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–åµŒå…¥æ¨¡å‹æˆ–é‡æ’åºç­–ç•¥\n"
        if stats.get("avg_ndcg_at_5", 0) < 0.5:
            md += "- ğŸ”´ æ’åºè´¨é‡ä¸ä½³(NDCG@5)ï¼Œå»ºè®®å¼•å…¥é‡æ’åºæ¨¡å‹æˆ–ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—\n"
        if stats.get("avg_f1_at_5", 0) < 0.5:
            md += "- ğŸŸ¡ F1åˆ†æ•°åä½ï¼Œéœ€è¦å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼Œè°ƒæ•´æ£€ç´¢å‚æ•°\n"
        if stats.get("avg_recall_at_5", 0) < 0.5:
            md += "- ğŸŸ¡ å¬å›ç‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ top_kæˆ–æ‰©å±•æ–‡æ¡£åº“\n"
        if stats["avg_keyword_hit_rate"] < 0.7:
            md += "- ğŸŸ¡ å…³é”®è¯è¦†ç›–ç‡ä¸è¶³ï¼Œå»ºè®®æ‰©å±•æ–‡æ¡£å†…å®¹æˆ–ä¼˜åŒ–æŸ¥è¯¢ç†è§£\n"
        if stats.get("avg_topic_coverage", 0) < 0.6:
            md += "- ğŸŸ¡ ä¸»é¢˜è¦†ç›–ç‡åä½ï¼Œå»ºè®®ä¸°å¯Œå„ä¸»é¢˜ç›¸å…³æ–‡æ¡£\n"
        if (
            stats.get("avg_semantic_similarity", 0) < 0.6
            and stats.get("avg_semantic_similarity", 0) > 0
        ):
            md += "- ğŸŸ¡ è¯­ä¹‰ç›¸ä¼¼åº¦ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–embeddingæ¨¡å‹æˆ–ç­”æ¡ˆç”Ÿæˆè´¨é‡\n"
        if stats["avg_mrr"] < 0.3:
            md += "- ğŸŸ¡ MRRåä½ï¼Œå»ºè®®ä¼˜åŒ–æ’åºç®—æ³•ç¡®ä¿æœ€ç›¸å…³ç»“æœæ’åœ¨é¦–ä½\n"

        return md

    def run_evaluation(
        self,
        test_file: str,
        limit: Optional[int] = None,
        enable_rerank: bool = True,
        reranker_type: str = "bge",
        compare_with_baseline: bool = True,
    ) -> str:
        """è¿è¡Œå®Œæ•´æµ‹è¯„"""
        print("\n" + "=" * 80)
        print("ğŸš€ RAGç³»ç»Ÿå¢å¼ºæµ‹è¯„ (AutoDLç‰ˆ)")
        print(f"ğŸ’» è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        if enable_rerank:
            print(f"ğŸ”„ é‡æ’åº: {reranker_type.upper()}")
        else:
            print("ğŸ“Š åŸºç¡€æ£€ç´¢æ¨¡å¼")
        print("=" * 80)
        print(f"æµ‹è¯„æ—¶é—´: {self.evaluation_time.strftime('%Y-%m-%d %H:%M:%S')}")

        if not self.init_services(
            enable_rerank=enable_rerank, reranker_type=reranker_type
        ):
            raise RuntimeError("æœåŠ¡åˆå§‹åŒ–å¤±è´¥")

        test_data = self.load_test_data(test_file)

        metadata = test_data.get("metadata", {})
        retrieval_cases = test_data.get("retrieval_test_cases", []) + test_data.get(
            "retrieval_test_cases_part2", []
        )

        print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   ç‰ˆæœ¬: {metadata.get('version', 'unknown')}")
        print(f"   æè¿°: {metadata.get('description', 'N/A')}")
        print(f"   æ£€ç´¢æµ‹è¯•: {len(retrieval_cases)} æ¡")

        print(f"\nğŸ¯ ä¸»è¦æµ‹è¯•: {'å¯ç”¨' if enable_rerank else 'ç¦ç”¨'}é‡æ’åº")
        results = self.evaluate_retrieval_cases(
            retrieval_cases,
            limit,
            enable_rerank=enable_rerank,
            reranker_type=reranker_type,
        )

        baseline_results = None
        if compare_with_baseline and enable_rerank:
            print(f"\nğŸ“Š å¯¹æ¯”æµ‹è¯•: ç¦ç”¨é‡æ’åº")
            baseline_results = self.evaluate_retrieval_cases(
                retrieval_cases, limit, enable_rerank=False
            )

        print("\nğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
        analysis = self.analyze_results(results)

        baseline_analysis = None
        if baseline_results:
            print("ğŸ“Š åˆ†æåŸºçº¿æµ‹è¯•ç»“æœ...")
            baseline_analysis = self.analyze_results(baseline_results)

        print("ğŸ† è®¡ç®—ç»¼åˆè¯„åˆ†...")
        score = self.calculate_score(analysis)
        baseline_score = None
        if baseline_analysis:
            baseline_score = self.calculate_score(baseline_analysis)

        print("ğŸ’¾ ä¿å­˜æµ‹è¯•æŠ¥å‘Š...")
        report_file = self.save_report(
            results,
            analysis,
            score,
            metadata,
            test_file,
            baseline_results=baseline_results,
            baseline_analysis=baseline_analysis,
            baseline_score=baseline_score,
        )

        self.print_summary(analysis, score, baseline_analysis, baseline_score)
        
        # æµ‹è¯•å®Œæˆåï¼Œå¦‚æœLLMæ¨¡å‹å¸¸é©»æ˜¾å­˜ï¼Œåˆ™å¸è½½é‡Šæ”¾èµ„æº
        if self.keep_llm_loaded and self._llm_client is not None:
            print("\nğŸ§¹ æ¸…ç†èµ„æºï¼šå¸è½½LLMæ¨¡å‹...")
            self.unload_llm_model()

        return report_file

    def print_summary(
        self,
        analysis: Dict[str, Any],
        score: Dict[str, Any],
        baseline_analysis: Optional[Dict[str, Any]] = None,
        baseline_score: Optional[Dict[str, Any]] = None,
    ):
        """æ‰“å°æµ‹è¯„æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯„æ‘˜è¦")
        if baseline_analysis:
            print("ğŸ”„ åŒ…å«é‡æ’åºæ•ˆæœå¯¹æ¯”")
        print("=" * 80)

        stats = analysis["statistics"]
        device = "GPU" if torch.cuda.is_available() else "CPU"

        print(f"\nğŸ’» è¿è¡Œè®¾å¤‡: {device}")
        print(f"ğŸ” æ•´ä½“æ€§èƒ½:")
        print(
            f"   æµ‹è¯•æ•°é‡: {analysis['total_tests']} (æœ‰æ•ˆ:{analysis['valid_tests']}, å¤±è´¥:{analysis['failed_tests']})"
        )
        print(f"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time_ms']:.1f}ms")
        print(f"   P@1 ç²¾ç¡®ç‡: {stats['avg_precision_at_1']:.3f}")
        print(f"   P@3 ç²¾ç¡®ç‡: {stats['avg_precision_at_3']:.3f}")
        print(f"   P@5 ç²¾ç¡®ç‡: {stats['avg_precision_at_5']:.3f}")
        print(f"   ğŸ†• Recall@5: {stats.get('avg_recall_at_5', 0):.3f}")
        print(f"   ğŸ†• F1@5: {stats.get('avg_f1_at_5', 0):.3f}")
        print(f"   ğŸ†• NDCG@5: {stats.get('avg_ndcg_at_5', 0):.3f}")
        print(f"   MRR: {stats['avg_mrr']:.3f}")
        print(f"   å…³é”®è¯å‘½ä¸­ç‡: {stats['avg_keyword_hit_rate']:.1%}")
        print(f"   ğŸ†• ä¸»é¢˜è¦†ç›–ç‡: {stats.get('avg_topic_coverage', 0):.1%}")
        print(f"   ğŸ†• è¯­ä¹‰ç›¸ä¼¼åº¦: {stats.get('avg_semantic_similarity', 0):.3f}")
        
        # LLM æ€§èƒ½æŒ‡æ ‡
        if stats.get('avg_llm_tokens_per_second', 0) > 0:
            print(f"\n   âš¡ LLMç”Ÿæˆæ€§èƒ½:")
            print(f"      å¹³å‡è¾“å…¥Token: {stats.get('avg_llm_input_tokens', 0):.1f}")
            print(f"      å¹³å‡è¾“å‡ºToken: {stats.get('avg_llm_output_tokens', 0):.1f}")
            print(f"      é¦–Tokenæ—¶å»¶: {stats.get('avg_time_to_first_token_ms', 0):.1f}ms")
            print(f"      å¹³å‡ç”Ÿæˆæ—¶é—´: {stats.get('avg_generation_time_ms', 0):.1f}ms")
            print(f"      âš¡ ç”Ÿæˆé€Ÿåº¦: {stats.get('avg_llm_tokens_per_second', 0):.2f} tokens/s")
        
        # æ‰“å° LLM æ€§èƒ½æŒ‡æ ‡
        if stats.get('avg_tokens_per_second', 0) > 0:
            print(f"\nâš¡ LLMç”Ÿæˆæ€§èƒ½:")
            print(f"   å¹³å‡è¾“å…¥Token: {stats.get('avg_llm_input_tokens', 0):.1f}")
            print(f"   å¹³å‡è¾“å‡ºToken: {stats.get('avg_llm_output_tokens', 0):.1f}")
            print(f"   é¦–Tokenæ—¶å»¶: {stats.get('avg_time_to_first_token_ms', 0):.1f}ms")
            print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {stats.get('avg_generation_time_ms', 0):.1f}ms")
            print(f"   âš¡ ç”Ÿæˆé€Ÿåº¦: {stats.get('avg_tokens_per_second', 0):.2f} tokens/s")

        if baseline_analysis:
            baseline_stats = baseline_analysis["statistics"]
            print(f"\nğŸ”„ é‡æ’åºæ•ˆæœå¯¹æ¯”:")
            score_improvement = score["total_score"] - baseline_score["total_score"]
            print(
                f"   ç»¼åˆè¯„åˆ†: {baseline_score['total_score']} â†’ {score['total_score']} ({score_improvement:+d}åˆ†)"
            )
            print(
                f"   P@1: {baseline_stats['avg_precision_at_1']:.3f} â†’ {stats['avg_precision_at_1']:.3f} ({stats['avg_precision_at_1'] - baseline_stats['avg_precision_at_1']:+.3f})"
            )
            print(
                f"   NDCG@5: {baseline_stats.get('avg_ndcg_at_5', 0):.3f} â†’ {stats.get('avg_ndcg_at_5', 0):.3f} ({stats.get('avg_ndcg_at_5', 0) - baseline_stats.get('avg_ndcg_at_5', 0):+.3f})"
            )
            print(
                f"   F1@5: {baseline_stats.get('avg_f1_at_5', 0):.3f} â†’ {stats.get('avg_f1_at_5', 0):.3f} ({stats.get('avg_f1_at_5', 0) - baseline_stats.get('avg_f1_at_5', 0):+.3f})"
            )
            print(
                f"   å“åº”æ—¶é—´: {baseline_stats['avg_response_time_ms']:.1f}ms â†’ {stats['avg_response_time_ms']:.1f}ms ({stats['avg_response_time_ms'] - baseline_stats['avg_response_time_ms']:+.1f}ms)"
            )

        print(f"\nğŸ“ˆ æŒ‰éš¾åº¦åˆ†æ:")
        for diff, diff_stats in analysis["by_difficulty"].items():
            print(
                f"   {diff:6s}: P@1={diff_stats['avg_precision_at_1']:.2f}, "
                f"NDCG5={diff_stats['avg_ndcg_at_5']:.2f}, "
                f"F1@5={diff_stats['avg_f1_at_5']:.2f}, "
                f"å…³é”®è¯={diff_stats['avg_keyword_hit_rate']:.0%} ({diff_stats['count']}æ¡)"
            )

        if analysis["problem_cases"]:
            print(f"\nâš ï¸  é—®é¢˜ç”¨ä¾‹: {len(analysis['problem_cases'])} æ¡")
            for case in analysis["problem_cases"][:2]:
                print(f"   â€¢ {case['query'][:40]}... (å‘½ä¸­:{case['hit_rate']:.0%})")

        print(f"\nğŸ† ç»¼åˆè¯„åˆ†: {score['grade']} - {score['total_score']}/100")

        for desc in score["grade_descriptions"]:
            print(f"   {desc}")

        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿå¢å¼ºæµ‹è¯„è„šæœ¬ (AutoDLç‰ˆ)")
    parser.add_argument(
        "--test-file",
        type=str,
        default=str(TEST_DATASET_PATH),
        help=f"æµ‹è¯•æ•°æ®æ–‡ä»¶ (é»˜è®¤: {TEST_DATASET_PATH})",
    )
    parser.add_argument("--limit", type=int, help="é™åˆ¶æµ‹è¯•æ•°é‡")
    parser.add_argument(
        "--output-dir", type=str, default="test_reports", help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--enable-rerank",
        action="store_true",
        default=True,
        help="å¯ç”¨é‡æ’åºï¼ˆé»˜è®¤å¯ç”¨ï¼‰",
    )
    parser.add_argument("--disable-rerank", action="store_true", help="ç¦ç”¨é‡æ’åº")
    parser.add_argument(
        "--reranker-type",
        type=str,
        default="bge",
        choices=["bge", "cross-encoder", "none"],
        help="é‡æ’åºå™¨ç±»å‹",
    )
    parser.add_argument(
        "--compare", action="store_true", default=True, help="ä¸åŸºçº¿å¯¹æ¯”ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument("--no-compare", action="store_true", help="ç¦ç”¨åŸºçº¿å¯¹æ¯”")
    parser.add_argument(
        "--vector-db-dir",
        type=str,
        default=str(VECTOR_DB_DIR),
        help=f"å‘é‡æ•°æ®åº“ç›®å½• (é»˜è®¤: {VECTOR_DB_DIR})",
    )

    args = parser.parse_args()

    enable_rerank = args.enable_rerank and not args.disable_rerank
    compare_with_baseline = args.compare and not args.no_compare

    evaluator = RAGEvaluator(
        output_dir=args.output_dir,
        vector_db_path=args.vector_db_dir,
    )

    try:
        report_file = evaluator.run_evaluation(
            args.test_file,
            args.limit,
            enable_rerank=enable_rerank,
            reranker_type=args.reranker_type,
            compare_with_baseline=compare_with_baseline,
        )
        print(f"\nâœ… æµ‹è¯„å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯„å¤±è´¥: {e}")
        logger.error(f"æµ‹è¯„å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
