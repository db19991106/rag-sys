#!/usr/bin/env python3
"""
RAGASæœ¬åœ°æ¨¡å‹è¯„ä¼° - å®Œå…¨ç¦»çº¿ç‰ˆ
ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹ä½œä¸ºè¯„ä¼°å™¨
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# é…ç½®RAGASä½¿ç”¨æœ¬åœ°æ¨¡å‹
import os

os.environ["OPENAI_API_KEY"] = "dummy-key-for-ragas"  # è™šæ‹Ÿkeyé¿å…æŠ¥é”™

try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    )
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    from datasets import Dataset

    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    print("âš ï¸  RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")

try:
    from langchain_community.llms import Ollama
    from langchain_community.embeddings import OllamaEmbeddings

    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸  langchain-communityæœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain-community")

# å¯¼å…¥RAGæœåŠ¡
from services.rag_generator import rag_generator
from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from models import RetrievalConfig, GenerationConfig, EmbeddingConfig, VectorDBConfig
from models import EmbeddingModelType, VectorDBType


class OfflineRAGASIntegration:
    """å®Œå…¨ç¦»çº¿çš„RAGASè¯„ä¼° - ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹"""

    def __init__(self, ollama_model: str = "qwen2.5:0.5b"):
        self.ollama_model = ollama_model
        self.evaluation_history = []
        self.initialized = False
        self.ragas_llm = None

        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")

        if not LANGCHAIN_AVAILABLE:
            raise ImportError(
                "langchain-communityæœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain-community"
            )

        # åŸºç¡€æŒ‡æ ‡
        self.metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]

    def initialize_ollama_llm(self):
        """åˆå§‹åŒ–Ollama LLMç”¨äºRAGASè¯„ä¼°"""
        if self.ragas_llm is not None:
            return True

        print(f"ğŸ¤– åˆå§‹åŒ–RAGASè¯„ä¼°å™¨ï¼ˆOllama: {self.ollama_model}ï¼‰")
        try:
            # åˆ›å»ºOllama LLM
            ollama_llm = Ollama(
                model=self.ollama_model,
                base_url="http://localhost:11434",
                temperature=0.0,  # è¯„ä¼°æ—¶ä¿æŒç¡®å®šæ€§
            )

            # åŒ…è£…ä¸ºRAGAS LLM
            self.ragas_llm = LangchainLLMWrapper(ollama_llm)

            print(f"   âœ… RAGASè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"\nğŸ’¡ è¯·ç¡®ä¿:")
            print(f"   1. OllamaæœåŠ¡å·²å¯åŠ¨: ollama serve")
            print(f"   2. æ¨¡å‹å·²ä¸‹è½½: ollama pull {self.ollama_model}")
            return False

    def test_ollama_connection(self):
        """æµ‹è¯•Ollamaè¿æ¥"""
        print("ğŸ§ª æµ‹è¯•Ollamaè¿æ¥...")
        try:
            import requests

            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m["name"] for m in models]
                print(f"   âœ… Ollamaè¿æ¥æ­£å¸¸")
                print(f"   ğŸ“¦ å¯ç”¨æ¨¡å‹: {', '.join(model_names)}")

                if self.ollama_model not in model_names:
                    print(f"\nâš ï¸  æ¨¡å‹ {self.ollama_model} æœªæ‰¾åˆ°")
                    print(f"   è¯·è¿è¡Œ: ollama pull {self.ollama_model}")
                    return False
                return True
            else:
                print(f"   âŒ Ollamaè¿”å›é”™è¯¯: {response.status_code}")
                return False
        except Exception as e:
            print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
            print(f"\nğŸ’¡ è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨:")
            print(f"   ollama serve")
            return False

    def initialize_services(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        if self.initialized:
            return True

        print("ğŸš€ åˆå§‹åŒ–RAGæœåŠ¡...")

        try:
            # 1. åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if not embedding_service.is_loaded():
                print("   ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹...")
                config = EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name="BAAI/bge-small-zh-v1.5",
                    device="cpu",
                    batch_size=8,
                )
                response = embedding_service.load_model(config)
                if response.status != "success":
                    print(f"   âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {response.message}")
                    return False
                print(f"   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (ç»´åº¦: {response.dimension})")
            else:
                print("   âœ… åµŒå…¥æ¨¡å‹å·²åŠ è½½")

            # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            if vector_db_manager.db is None:
                print("   ğŸ“¥ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
                dimension = embedding_service.get_dimension()
                config = VectorDBConfig(
                    db_type=VectorDBType.FAISS, dimension=dimension, index_type="HNSW"
                )
                success = vector_db_manager.initialize(config)
                if not success:
                    print("   âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                    return False
                print("   âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("   âœ… å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–")

            # 3. æ£€æŸ¥å‘é‡åº“çŠ¶æ€
            status = vector_db_manager.get_status()
            print(f"   ğŸ“Š å‘é‡åº“çŠ¶æ€: {status.total_vectors} ä¸ªå‘é‡")

            if status.total_vectors == 0:
                print("   âš ï¸  è­¦å‘Š: å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ process_document.py")
                return False

            self.initialized = True
            print("âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ\n")
            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def evaluate_single_query(
        self,
        query: str,
        ground_truth: str = None,
        retrieval_config: RetrievalConfig = None,
        generation_config: GenerationConfig = None,
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""

        # æµ‹è¯•Ollamaè¿æ¥
        if not self.test_ollama_connection():
            return {
                "query": query,
                "error": "Ollamaè¿æ¥å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        # åˆå§‹åŒ–RAGASè¯„ä¼°å™¨
        if not self.initialize_ollama_llm():
            return {
                "query": query,
                "error": "RAGASè¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        # åˆå§‹åŒ–RAGæœåŠ¡
        if not self.initialize_services():
            return {
                "query": query,
                "error": "RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        print(f"ğŸ” è¯„ä¼°æŸ¥è¯¢: {query[:50]}...")

        # ä½¿ç”¨é»˜è®¤é…ç½®
        if retrieval_config is None:
            retrieval_config = RetrievalConfig(top_k=5)
        if generation_config is None:
            generation_config = GenerationConfig(
                llm_provider="local", temperature=0.7, max_tokens=500
            )

        # è¿è¡ŒRAGç³»ç»Ÿ
        start_time = time.time()
        try:
            response = rag_generator.generate(
                query=query,
                retrieval_config=retrieval_config,
                generation_config=generation_config,
            )
            rag_time = time.time() - start_time

            # æå–ä¿¡æ¯
            answer = response.answer
            contexts = (
                [chunk.content for chunk in response.context_chunks]
                if response.context_chunks
                else []
            )

            print(f"   âœ“ RAGæ‰§è¡Œå®Œæˆ ({rag_time:.2f}s)")
            print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            print(f"   ğŸ’¬ å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")

        except Exception as e:
            print(f"   âŒ RAGæ‰§è¡Œå¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

        # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°
        if not contexts:
            print("   âš ï¸  æ— æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°")
            return {
                "query": query,
                "answer": answer,
                "error": "æ— æ£€ç´¢ä¸Šä¸‹æ–‡",
                "timestamp": datetime.now().isoformat(),
            }

        # å‡†å¤‡RAGASæ•°æ®
        data_dict = {
            "question": [query],
            "answer": [answer],
            "contexts": [contexts],
        }

        if ground_truth:
            data_dict["ground_truth"] = [ground_truth]

        dataset = Dataset.from_dict(data_dict)

        # è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨æœ¬åœ°Ollamaï¼‰
        print("   ğŸ§ª è¿è¡ŒRAGASè¯„ä¼°ï¼ˆæœ¬åœ°Ollamaï¼‰...")
        print("   â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        try:
            eval_start = time.time()
            result = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                llm=self.ragas_llm,  # ä½¿ç”¨æœ¬åœ°Ollama
                raise_exceptions=False,
            )
            eval_time = time.time() - eval_start

            # è½¬æ¢ç»“æœ
            scores = {
                k: float(v[0]) if hasattr(v, "__getitem__") else float(v)
                for k, v in result.items()
            }

            print(f"   âœ“ è¯„ä¼°å®Œæˆ ({eval_time:.2f}s)")

            # æ„å»ºç»“æœ
            evaluation_result = {
                "query": query,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": ground_truth,
                "scores": scores,
                "rag_time": rag_time,
                "eval_time": eval_time,
                "timestamp": datetime.now().isoformat(),
            }

            # æ·»åŠ åˆ°å†å²
            self.evaluation_history.append(evaluation_result)

            # æ‰“å°ç»“æœ
            self._print_scores(scores)

            return evaluation_result

        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "answer": answer,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def evaluate_test_dataset(
        self, test_file: str, max_samples: int = None
    ) -> Dict[str, Any]:
        """è¯„ä¼°æµ‹è¯•æ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_file}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_path = Path(test_file)
        if not test_path.exists():
            test_path = Path(__file__).parent / "test_data" / test_file

        if not test_path.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return {"error": f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"}

        try:
            with open(test_path, "r", encoding="utf-8") as f:
                test_data = json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            return {"error": f"åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}"}

        # è·å–æµ‹è¯•ç”¨ä¾‹
        test_cases = test_data.get("end_to_end_test_cases", [])
        if not test_cases:
            test_cases = test_data.get("retrieval_test_cases", [])

        if not test_cases:
            print("âŒ æµ‹è¯•æ•°æ®ä¸ºç©º")
            return {"error": "æµ‹è¯•æ•°æ®ä¸ºç©º"}

        if max_samples:
            test_cases = test_cases[:max_samples]

        print(f"ğŸ§ª å¼€å§‹è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹\n")
        print("âš ï¸  æ³¨æ„: ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¯„ä¼°è¾ƒæ…¢ï¼Œæ¯ä¸ªæŸ¥è¯¢çº¦éœ€1-3åˆ†é’Ÿ\n")

        # æ‰¹é‡è¯„ä¼°
        results = []
        for i, case in enumerate(test_cases, 1):
            print(f"[{i}/{len(test_cases)}] ", end="")

            query = case.get("query")
            expected = case.get("expected_answer_contains", [])
            ground_truth = (
                " ".join(expected) if isinstance(expected, list) else str(expected)
            )

            result = self.evaluate_single_query(
                query=query, ground_truth=ground_truth if ground_truth else None
            )
            results.append(result)
            print()

            # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡è½½
            if i < len(test_cases):
                time.sleep(2)

        # è®¡ç®—ç»Ÿè®¡
        successful_evals = [r for r in results if "scores" in r]
        if successful_evals:
            avg_scores = {}
            for metric in successful_evals[0]["scores"].keys():
                values = [
                    r["scores"][metric]
                    for r in successful_evals
                    if metric in r["scores"]
                ]
                avg_scores[metric] = sum(values) / len(values) if values else 0

            summary = {
                "total_cases": len(test_cases),
                "successful_evals": len(successful_evals),
                "failed_evals": len(test_cases) - len(successful_evals),
                "average_scores": avg_scores,
                "ollama_model": self.ollama_model,
                "timestamp": datetime.now().isoformat(),
            }
        else:
            summary = {
                "total_cases": len(test_cases),
                "successful_evals": 0,
                "failed_evals": len(test_cases),
                "timestamp": datetime.now().isoformat(),
            }

        # ä¿å­˜ç»“æœ
        self._save_results(results, summary)
        self._print_summary(summary)

        return {"results": results, "summary": summary}

    def _print_scores(self, scores: Dict[str, float]):
        """æ‰“å°è¯„åˆ†"""
        for metric, score in scores.items():
            if score >= 0.8:
                status = "ğŸŸ¢"
            elif score >= 0.6:
                status = "ğŸŸ¡"
            else:
                status = "ğŸ”´"
            print(f"   {status} {metric}: {score:.3f}")

    def _print_summary(self, summary: Dict):
        """æ‰“å°æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ“Š è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {summary['total_cases']}")
        print(f"æˆåŠŸè¯„ä¼°: {summary['successful_evals']} âœ…")
        print(f"å¤±è´¥: {summary['failed_evals']} âŒ")

        if "average_scores" in summary and summary["average_scores"]:
            print("\nå¹³å‡æŒ‡æ ‡å¾—åˆ†:")
            for metric, score in summary["average_scores"].items():
                if score >= 0.8:
                    status = "ğŸŸ¢ ä¼˜ç§€"
                elif score >= 0.6:
                    status = "ğŸŸ¡ è‰¯å¥½"
                else:
                    status = "ğŸ”´ éœ€ä¼˜åŒ–"

                print(f"  {metric}: {score:.3f} {status}")

        print("=" * 70)

    def _save_results(self, results: List[Dict], summary: Dict):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_dir = Path(__file__).parent / "evaluation_results"
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = (
            output_dir
            / f"ragas_eval_ollama_{self.ollama_model.replace(':', '_')}_{timestamp}.json"
        )

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                {"summary": summary, "results": results},
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="RAGASç¦»çº¿è¯„ä¼°å·¥å…·ï¼ˆOllamaï¼‰")
    parser.add_argument(
        "--model",
        "-m",
        type=str,
        default="qwen2.5:0.5b",
        help="Ollamaæ¨¡å‹åç§° (é»˜è®¤: qwen2.5:0.5b)",
    )
    parser.add_argument("--test", "-t", action="store_true", help="æµ‹è¯•Ollamaè¿æ¥")
    parser.add_argument(
        "--mode", choices=["single", "batch"], default="batch", help="è¯„ä¼°æ¨¡å¼"
    )
    parser.add_argument("--query", "-q", type=str, help="å•ä¸ªæŸ¥è¯¢")
    parser.add_argument("--ground-truth", "-g", type=str, help="æœŸæœ›ç­”æ¡ˆ")
    parser.add_argument(
        "--test-file",
        "-f",
        type=str,
        default="test_data/test_dataset.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶",
    )
    parser.add_argument("--max-samples", "-n", type=int, help="æœ€å¤§æ ·æœ¬æ•°")

    args = parser.parse_args()

    if not RAGAS_AVAILABLE or not LANGCHAIN_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–ï¼Œè¯·å®‰è£…:")
        print("   pip install ragas langchain-community")
        return

    evaluator = OfflineRAGASIntegration(ollama_model=args.model)

    if args.test:
        # ä»…æµ‹è¯•è¿æ¥
        print("ğŸ”§ æµ‹è¯•Ollamaè¿æ¥")
        print("=" * 70)
        if evaluator.test_ollama_connection():
            if evaluator.initialize_ollama_llm():
                print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œè¯„ä¼°")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")

    elif args.mode == "single":
        if not args.query:
            print("âŒ è¯·æä¾›æŸ¥è¯¢: --query 'ä½ çš„é—®é¢˜'")
            return

        result = evaluator.evaluate_single_query(
            query=args.query, ground_truth=args.ground_truth
        )

        if evaluator.evaluation_history:
            print("\n" + evaluator.get_evaluation_report())

    elif args.mode == "batch":
        evaluator.evaluate_test_dataset(
            test_file=args.test_file, max_samples=args.max_samples
        )


if __name__ == "__main__":
    main()
