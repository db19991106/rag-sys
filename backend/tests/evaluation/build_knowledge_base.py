#!/usr/bin/env python3
"""
ç¦»çº¿çŸ¥è¯†åº“æ„å»ºè„šæœ¬
ç”¨äºæ‰¹é‡å¤„ç†æ–‡æ¡£ã€åˆ‡åˆ†ã€å‘é‡åŒ–å¹¶å­˜å…¥å‘é‡æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python build_knowledge_base.py
    python build_knowledge_base.py --config financial
    python build_knowledge_base.py --docs-dir /path/to/docs --chunking-method financial_v2
"""

import sys
import os
import argparse
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
# å½“å‰æ–‡ä»¶è·¯å¾„: backend/tests/evaluation/build_knowledge_base.py
# éœ€è¦ä» evaluation -> tests -> backend
backend_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_dir))

# ç¦ç”¨stdouté‡å®šå‘ï¼Œé¿å…æ—¥å¿—é€’å½’é—®é¢˜
import os
os.environ['RAG_DISABLE_STDOUT_REDIRECT'] = 'true'

# å¯¼å…¥é…ç½®
from eval_config import (
    CURRENT_CONFIG,
    get_config,
    DOCS_DIR,
    VECTOR_DB_DIR,
    MODELS_DIR,
    TEST_DATASET_PATH,
    SUPPORTED_EXTENSIONS,
)

# å¯¼å…¥æœåŠ¡
from services.document_parser import DocumentParser
from services.chunker import Chunker, ChunkType
from services.financial_chunker_v2 import FinancialDocumentChunker
from services.embedding import embedding_service
from services.vector_db import vector_db_manager, VectorDBConfig
from models import EmbeddingConfig, EmbeddingModelType, VectorDBType
from config import settings
# from utils.logger import logger  # æ³¨é‡Šæ‰ï¼Œé¿å…å†²çª

# åˆ›å»º chunker å®ä¾‹
chunker = Chunker()


class KnowledgeBaseBuilder:
    """çŸ¥è¯†åº“æ„å»ºå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ„å»ºå™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_docs": 0,
            "processed_docs": 0,
            "failed_docs": 0,
            "total_chunks": 0,
            "total_vectors": 0,
        }
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get("log_config", {})
        log_level = getattr(logging, log_config.get("log_level", "INFO"))
        log_file = log_config.get("log_file")

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        if log_file:
            log_file = Path(log_file)
            log_file.parent.mkdir(parents=True, exist_ok=True)

        # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
        logging.basicConfig(
            level=log_level,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(str(log_file), encoding="utf-8")
                if log_file
                else logging.NullHandler(),
            ],
            force=True,  # å¼ºåˆ¶é‡æ–°é…ç½®
        )

        self.logger = logging.getLogger(__name__)
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ ç¦»çº¿çŸ¥è¯†åº“æ„å»ºè„šæœ¬å¯åŠ¨")
        self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        self.logger.info("=" * 80)

    def scan_documents(self, docs_dir: Path) -> List[Path]:
        """
        æ‰«ææ–‡æ¡£ç›®å½•

        Args:
            docs_dir: æ–‡æ¡£ç›®å½•

        Returns:
            æ–‡æ¡£è·¯å¾„åˆ—è¡¨
        """
        self.logger.info(f"\nğŸ“ æ‰«ææ–‡æ¡£ç›®å½•: {docs_dir}")

        if not docs_dir.exists():
            self.logger.error(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
            return []

        supported_exts = self.config.get("supported_extensions", SUPPORTED_EXTENSIONS)
        doc_files = []

        for ext in supported_exts:
            doc_files.extend(docs_dir.glob(f"**/*{ext}"))

        # å»é‡å¹¶æ’åº
        doc_files = sorted(list(set(doc_files)))

        self.logger.info(f"âœ… æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£")
        for doc_file in doc_files:
            self.logger.info(f"   - {doc_file.name}")

        self.stats["total_docs"] = len(doc_files)
        return doc_files

    def parse_document(self, doc_path: Path) -> Optional[str]:
        """
        è§£ææ–‡æ¡£

        Args:
            doc_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ–‡æ¡£å†…å®¹
        """
        try:
            self.logger.info(f"\nğŸ“„ è§£ææ–‡æ¡£: {doc_path.name}")

            # ä½¿ç”¨æ–‡æ¡£è§£æå™¨
            content = DocumentParser.parse(str(doc_path))

            if not content:
                self.logger.warning(f"âš ï¸ æ–‡æ¡£ä¸ºç©º: {doc_path.name}")
                return None

            self.logger.info(f"   âœ… è§£ææˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            return content

        except Exception as e:
            self.logger.error(f"   âŒ è§£æå¤±è´¥: {str(e)}")
            return None

    def chunk_document(self, content: str, doc_path: Path) -> List[Dict[str, Any]]:
        """
        åˆ‡åˆ†æ–‡æ¡£

        Args:
            content: æ–‡æ¡£å†…å®¹
            doc_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        chunking_method = self.config.get("chunking_method", "financial_v2")
        self.logger.info(f"\nâœ‚ï¸  åˆ‡åˆ†æ–‡æ¡£ (æ–¹æ³•: {chunking_method})")

        try:
            chunks = []

            if chunking_method == "financial_v2":
                # ä½¿ç”¨è´¢åŠ¡æŠ¥é”€åˆ¶åº¦åˆ‡åˆ†å™¨V2
                chunker_v2 = FinancialDocumentChunker(
                    max_chunk_size=self.config.get("chunking_config", {}).get(
                        "max_chunk_size", 1000
                    )
                )
                chunk_objects = chunker_v2.chunk_document(content, doc_id=doc_path.stem)

                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                for i, chunk_obj in enumerate(chunk_objects):
                    chunks.append(
                        {
                            "id": f"{doc_path.stem}_chunk_{i + 1}",
                            "content": chunk_obj.content,
                            "metadata": chunk_obj.metadata,
                            "chunk_type": chunk_obj.chunk_type,
                        }
                    )

            elif chunking_method == "intelligent":
                # ä½¿ç”¨æ™ºèƒ½åˆ‡åˆ†
                chunk_result = chunker.chunk(
                    content=content,
                    chunk_type=ChunkType.INTELLIGENT,
                    doc_id=doc_path.stem,
                )

                for i, chunk in enumerate(chunk_result.chunks):
                    chunks.append(
                        {
                            "id": chunk.id,
                            "content": chunk.content,
                            "metadata": chunk.metadata,
                            "chunk_type": chunk.chunk_type.value,
                        }
                    )

            else:
                # ä½¿ç”¨é»˜è®¤åˆ‡åˆ†
                chunk_result = chunker.chunk(
                    content=content,
                    chunk_type=ChunkType.NAIVE,
                    doc_id=doc_path.stem,
                )

                for i, chunk in enumerate(chunk_result.chunks):
                    chunks.append(
                        {
                            "id": chunk.id,
                            "content": chunk.content,
                            "metadata": chunk.metadata,
                            "chunk_type": chunk.chunk_type.value,
                        }
                    )

            self.logger.info(f"   âœ… ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")
            return chunks

        except Exception as e:
            self.logger.error(f"   âŒ åˆ‡åˆ†å¤±è´¥: {str(e)}")
            import traceback

            self.logger.error(traceback.format_exc())
            return []

    def init_embedding_service(self) -> bool:
        """
        åˆå§‹åŒ–åµŒå…¥æœåŠ¡

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("\nğŸ”§ åˆå§‹åŒ–åµŒå…¥æœåŠ¡...")

        try:
            embedding_type = self.config.get("embedding_model_type", "bge")
            embedding_config = self.config.get("embedding_config", {})

            if embedding_type == "bge":
                model_path = embedding_config.get("model_path")
                if not model_path or not Path(model_path).exists():
                    self.logger.error(f"âŒ BGEæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                    return False

                config = EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name=model_path,
                    device=embedding_config.get("device", "cuda"),
                )
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„åµŒå…¥æ¨¡å‹ç±»å‹: {embedding_type}")
                return False

            embedding_service.load_model(config)

            if embedding_service.is_loaded():
                self.logger.info(f"   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
                self.logger.info(f"   ğŸ“Š æ¨¡å‹ç»´åº¦: {embedding_service.get_dimension()}")
                return True
            else:
                self.logger.error("âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–åµŒå…¥æœåŠ¡å¤±è´¥: {str(e)}")
            return False

    def init_vector_db(self) -> bool:
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("\nğŸ’¾ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")

        try:
            db_type = self.config.get("vector_db_type", "faiss")
            vector_db_config = self.config.get("vector_db_config", {})

            if db_type == "faiss":
                # æ£€æŸ¥å‘é‡åº“ç›®å½•
                vector_db_dir = self.config.get("vector_db_dir", VECTOR_DB_DIR)
                vector_db_path = Path(vector_db_dir)
                vector_db_path.mkdir(parents=True, exist_ok=True)

                config = VectorDBConfig(
                    db_type=VectorDBType.FAISS,
                    dimension=embedding_service.get_dimension(),
                    index_type=vector_db_config.get("index_type", "HNSW"),
                    index_path=str(vector_db_path),
                )
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„å‘é‡æ•°æ®åº“ç±»å‹: {db_type}")
                return False

            success = vector_db_manager.initialize(config)

            if success:
                status = vector_db_manager.get_status()
                self.logger.info(f"   âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
                self.logger.info(f"   ğŸ“Š å½“å‰å‘é‡æ•°: {status.total_vectors}")
                return True
            else:
                self.logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

    def embed_and_store(self, chunks: List[Dict[str, Any]], doc_path: Path) -> bool:
        """
        å‘é‡åŒ–å¹¶å­˜å‚¨

        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
            doc_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not chunks:
            return True

        self.logger.info(f"\nğŸ”¢ å‘é‡åŒ–å¹¶å­˜å‚¨...")

        try:
            # å‡†å¤‡æ–‡æœ¬
            texts = [chunk["content"] for chunk in chunks]

            # æ‰¹é‡å‘é‡åŒ–
            batch_size = self.config.get("batch_config", {}).get(
                "embedding_batch_size", 32
            )
            all_vectors = []

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i : i + batch_size]
                self.logger.info(
                    f"   å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(texts) - 1) // batch_size + 1} ({len(batch_texts)} æ¡)"
                )

                vectors = embedding_service.encode(batch_texts)
                all_vectors.extend(vectors)

            # å‡†å¤‡å…ƒæ•°æ®
            metadata = []
            for i, chunk in enumerate(chunks):
                meta = {
                    "chunk_id": chunk["id"],
                    "document_id": doc_path.stem,
                    "document_name": doc_path.name,
                    "chunk_num": i + 1,
                    "content": chunk["content"],
                    "chunk_type": chunk.get("chunk_type", "text"),
                    "metadata": chunk.get("metadata", {}),
                }
                metadata.append(meta)

            # å­˜å…¥å‘é‡æ•°æ®åº“
            import numpy as np

            vectors_array = np.array(all_vectors, dtype=np.float32)
            vector_db_manager.add_vectors(vectors_array, metadata)

            self.logger.info(f"   âœ… å­˜å‚¨æˆåŠŸ: {len(chunks)} ä¸ªå‘é‡")
            self.stats["total_vectors"] += len(chunks)
            return True

        except Exception as e:
            self.logger.error(f"   âŒ å‘é‡åŒ–æˆ–å­˜å‚¨å¤±è´¥: {str(e)}")
            import traceback

            self.logger.error(traceback.format_exc())
            return False

    def process_document(self, doc_path: Path) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£

        Args:
            doc_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info(f"\n{'=' * 80}")
        self.logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_path.name}")
        self.logger.info(f"{'=' * 80}")

        try:
            # 1. è§£ææ–‡æ¡£
            content = self.parse_document(doc_path)
            if not content:
                self.stats["failed_docs"] += 1
                return False

            # 2. åˆ‡åˆ†æ–‡æ¡£
            chunks = self.chunk_document(content, doc_path)
            if not chunks:
                self.logger.warning(f"âš ï¸ æ²¡æœ‰ç”Ÿæˆç‰‡æ®µ: {doc_path.name}")
                self.stats["failed_docs"] += 1
                return False

            self.stats["total_chunks"] += len(chunks)

            # 3. å‘é‡åŒ–å¹¶å­˜å‚¨
            success = self.embed_and_store(chunks, doc_path)
            if not success:
                self.stats["failed_docs"] += 1
                return False

            self.stats["processed_docs"] += 1
            self.logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_path.name}")
            return True

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥: {doc_path.name} - {str(e)}")
            import traceback

            self.logger.error(traceback.format_exc())
            self.stats["failed_docs"] += 1
            return False

    def build(self) -> bool:
        """
        æ„å»ºçŸ¥è¯†åº“

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.stats["start_time"] = datetime.now()
        self.logger.info(f"\nğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“ - {self.stats['start_time']}")

        # 1. åˆå§‹åŒ–æœåŠ¡
        if not self.init_embedding_service():
            self.logger.error("âŒ åµŒå…¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œåœæ­¢æ„å»º")
            return False

        if not self.init_vector_db():
            self.logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œåœæ­¢æ„å»º")
            return False

        # 2. æ‰«ææ–‡æ¡£
        docs_dir = self.config.get("docs_dir", DOCS_DIR)
        doc_files = self.scan_documents(Path(docs_dir))

        if not doc_files:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
            return False

        # 3. å¤„ç†æ–‡æ¡£
        self.logger.info(f"\n{'=' * 80}")
        self.logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(doc_files)} ä¸ªæ–‡æ¡£")
        self.logger.info(f"{'=' * 80}")

        for i, doc_file in enumerate(doc_files, 1):
            self.logger.info(
                f"\nğŸ“Š è¿›åº¦: {i}/{len(doc_files)} ({i / len(doc_files) * 100:.1f}%)"
            )
            self.process_document(doc_file)

        # 4. ä¿å­˜å‘é‡æ•°æ®åº“
        self.logger.info("\nğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“...")
        try:
            if hasattr(vector_db_manager.db, "save"):
                vector_db_manager.db.save()
                self.logger.info("âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")

        # 5. è¾“å‡ºç»Ÿè®¡
        self.stats["end_time"] = datetime.now()
        self.print_stats()

        return self.stats["failed_docs"] == 0

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()

        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ“Š æ„å»ºç»Ÿè®¡")
        self.logger.info("=" * 80)
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        self.logger.info(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {self.stats['total_docs']}")
        self.logger.info(f"âœ… æˆåŠŸå¤„ç†: {self.stats['processed_docs']}")
        self.logger.info(f"âŒ å¤±è´¥æ–‡æ¡£: {self.stats['failed_docs']}")
        self.logger.info(f"âœ‚ï¸  æ€»ç‰‡æ®µæ•°: {self.stats['total_chunks']}")
        self.logger.info(f"ğŸ”¢ æ€»å‘é‡æ•°: {self.stats['total_vectors']}")

        if self.stats["total_docs"] > 0:
            success_rate = (
                self.stats["processed_docs"] / self.stats["total_docs"]
            ) * 100
            self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")

        self.logger.info("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç¦»çº¿çŸ¥è¯†åº“æ„å»ºè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # ä½¿ç”¨é»˜è®¤é…ç½®æ„å»º
    python build_knowledge_base.py
    
    # ä½¿ç”¨è´¢åŠ¡æŠ¥é”€åˆ¶åº¦é…ç½®
    python build_knowledge_base.py --config financial
    
    # æŒ‡å®šæ–‡æ¡£ç›®å½•å’Œåˆ‡åˆ†æ–¹æ³•
    python build_knowledge_base.py --docs-dir /path/to/docs --chunking-method financial_v2
    
    # æŒ‡å®šå‘é‡åº“ç›®å½•
    python build_knowledge_base.py --vector-db-dir /path/to/vector_db
        """,
    )

    parser.add_argument(
        "--config",
        type=str,
        default="default",
        choices=["default", "financial", "general"],
        help="ä½¿ç”¨é¢„å®šä¹‰é…ç½®æ–¹æ¡ˆ (default: default)",
    )

    parser.add_argument(
        "--docs-dir",
        type=str,
        help=f"æ–‡æ¡£ç›®å½• (é»˜è®¤: {DOCS_DIR})",
    )

    parser.add_argument(
        "--vector-db-dir",
        type=str,
        help=f"å‘é‡åº“ç›®å½• (é»˜è®¤: {VECTOR_DB_DIR})",
    )

    parser.add_argument(
        "--chunking-method",
        type=str,
        choices=["financial_v2", "financial", "intelligent", "naive", "enhanced"],
        help="åˆ‡åˆ†æ–¹æ³•",
    )

    parser.add_argument(
        "--embedding-model",
        type=str,
        help="åµŒå…¥æ¨¡å‹è·¯å¾„",
    )

    parser.add_argument(
        "--device",
        type=str,
        choices=["cuda", "cpu"],
        help="è¿è¡Œè®¾å¤‡",
    )

    parser.add_argument(
        "--log-level",
        type=str,
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)",
    )

    args = parser.parse_args()

    # è·å–é…ç½®
    config = get_config(args.config)

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.docs_dir:
        config["docs_dir"] = Path(args.docs_dir)
    if args.vector_db_dir:
        config["vector_db_dir"] = Path(args.vector_db_dir)
    if args.chunking_method:
        config["chunking_method"] = args.chunking_method
    if args.embedding_model:
        config["embedding_config"]["model_path"] = args.embedding_model
    if args.device:
        config["embedding_config"]["device"] = args.device
    if args.log_level:
        # ç¡®ä¿log_configå­˜åœ¨
        if "log_config" not in config:
            config["log_config"] = {}
        config["log_config"]["log_level"] = args.log_level

    # åˆ›å»ºæ„å»ºå™¨å¹¶è¿è¡Œ
    builder = KnowledgeBaseBuilder(config)
    success = builder.build()

    if success:
        print("\nâœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œä½†éƒ¨åˆ†æ–‡æ¡£å¤„ç†å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
