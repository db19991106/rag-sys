#!/usr/bin/env python3
"""
RAGASè¯„ä¼°é›†æˆæ¨¡å— - ä¿®å¤ç‰ˆ
ä¸ä½ çš„RAGç³»ç»Ÿæ— ç¼é›†æˆ
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        context_entity_recall,
        answer_similarity,
        answer_correctness,
    )
    from datasets import Dataset

    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    print("âš ï¸  RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")
    print("   æˆ–ä½¿ç”¨ç®€åŒ–ç‰ˆè¯„ä¼°å™¨: python ragas_integration.py --mode simple")

# å¯¼å…¥RAGæœåŠ¡
from services.rag_generator import rag_generator
from services.embedding import embedding_service
from services.vector_db import vector_db_manager
from services.retriever import retriever
from models import RetrievalConfig, GenerationConfig, EmbeddingConfig, VectorDBConfig
from models import EmbeddingModelType, VectorDBType
from utils.logger import logger


class RAGASIntegration:
    """RAGASè¯„ä¼°é›†æˆç±»"""

    def __init__(self, use_ground_truth: bool = True):
        self.use_ground_truth = use_ground_truth
        self.evaluation_history = []
        self.initialized = False

        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install ragas")

        # åŸºç¡€æŒ‡æ ‡
        self.basic_metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]

        # é«˜çº§æŒ‡æ ‡ï¼ˆéœ€è¦ground truthï¼‰
        self.advanced_metrics = [
            answer_correctness,
            answer_similarity,
            context_entity_recall,
        ]

    def initialize_services(self):
        """åˆå§‹åŒ–RAGæœåŠ¡ï¼ˆåµŒå…¥æ¨¡å‹å’Œå‘é‡æ•°æ®åº“ï¼‰"""
        if self.initialized:
            return True

        print("ğŸš€ åˆå§‹åŒ–RAGæœåŠ¡...")

        try:
            # 1. åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if not embedding_service.is_loaded():
                print("   ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹...")
                config = EmbeddingConfig(
                    model_type=EmbeddingModelType.BGE,
                    model_name="BAAI/bge-small-zh-v1.5",
                    device="cpu",
                    batch_size=8,
                )
                response = embedding_service.load_model(config)
                if response.status != "success":
                    print(f"   âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {response.message}")
                    return False
                print(f"   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (ç»´åº¦: {response.dimension})")
            else:
                print("   âœ… åµŒå…¥æ¨¡å‹å·²åŠ è½½")

            # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            if vector_db_manager.db is None:
                print("   ğŸ“¥ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
                dimension = embedding_service.get_dimension()
                config = VectorDBConfig(
                    db_type=VectorDBType.FAISS, dimension=dimension, index_type="HNSW"
                )
                success = vector_db_manager.initialize(config)
                if not success:
                    print("   âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                    return False
                print("   âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("   âœ… å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–")

            # 3. æ£€æŸ¥å‘é‡åº“çŠ¶æ€
            status = vector_db_manager.get_status()
            print(f"   ğŸ“Š å‘é‡åº“çŠ¶æ€: {status.total_vectors} ä¸ªå‘é‡")

            if status.total_vectors == 0:
                print("   âš ï¸  è­¦å‘Š: å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
                return False

            self.initialized = True
            print("âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ\n")
            return True

        except Exception as e:
            print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def evaluate_single_query(
        self,
        query: str,
        ground_truth: str = None,
        retrieval_config: RetrievalConfig = None,
        generation_config: GenerationConfig = None,
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢é—®é¢˜
            ground_truth: æœŸæœ›ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
            retrieval_config: æ£€ç´¢é…ç½®
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
        if not self.initialize_services():
            return {
                "query": query,
                "error": "RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        print(f"ğŸ” è¯„ä¼°æŸ¥è¯¢: {query[:50]}...")

        # ä½¿ç”¨é»˜è®¤é…ç½®
        if retrieval_config is None:
            retrieval_config = RetrievalConfig(top_k=5)
        if generation_config is None:
            generation_config = GenerationConfig(temperature=0.7, max_tokens=500)

        # è¿è¡ŒRAGç³»ç»Ÿ
        start_time = time.time()
        try:
            response = rag_generator.generate(
                query=query,
                retrieval_config=retrieval_config,
                generation_config=generation_config,
            )
            rag_time = time.time() - start_time

            # æå–ä¿¡æ¯ï¼ˆä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å±æ€§åï¼‰
            answer = response.answer
            # RAGResponseä¸­ä½¿ç”¨context_chunksè€Œä¸æ˜¯sources
            contexts = (
                [chunk.content for chunk in response.context_chunks]
                if response.context_chunks
                else []
            )

            print(f"   âœ“ RAGæ‰§è¡Œå®Œæˆ ({rag_time:.2f}s)")
            print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            print(f"   ğŸ’¬ å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")

        except Exception as e:
            print(f"   âŒ RAGæ‰§è¡Œå¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

        # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°
        if not contexts:
            print("   âš ï¸  æ— æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯„ä¼°")
            return {
                "query": query,
                "answer": answer,
                "error": "æ— æ£€ç´¢ä¸Šä¸‹æ–‡",
                "timestamp": datetime.now().isoformat(),
            }

        # å‡†å¤‡RAGASæ•°æ®
        data_dict = {
            "question": [query],
            "answer": [answer],
            "contexts": [contexts],
        }

        if ground_truth:
            data_dict["ground_truth"] = [ground_truth]

        dataset = Dataset.from_dict(data_dict)

        # é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
        metrics = self.basic_metrics.copy()
        if ground_truth and self.use_ground_truth:
            metrics.extend(self.advanced_metrics)

        # è¿è¡Œè¯„ä¼°
        print("   ğŸ§ª è¿è¡ŒRAGASè¯„ä¼°...")
        try:
            eval_start = time.time()
            result = evaluate(dataset=dataset, metrics=metrics, raise_exceptions=False)
            eval_time = time.time() - eval_start

            # è½¬æ¢ç»“æœ
            scores = {
                k: float(v[0]) if hasattr(v, "__getitem__") else float(v)
                for k, v in result.items()
            }

            print(f"   âœ“ è¯„ä¼°å®Œæˆ ({eval_time:.2f}s)")

            # æ„å»ºç»“æœ
            evaluation_result = {
                "query": query,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": ground_truth,
                "scores": scores,
                "rag_time": rag_time,
                "eval_time": eval_time,
                "timestamp": datetime.now().isoformat(),
            }

            # æ·»åŠ åˆ°å†å²
            self.evaluation_history.append(evaluation_result)

            # æ‰“å°ç»“æœ
            self._print_scores(scores)

            return evaluation_result

        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return {
                "query": query,
                "answer": answer,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def evaluate_test_dataset(
        self, test_file: str, max_samples: int = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æµ‹è¯•æ•°æ®é›†

        Args:
            test_file: æµ‹è¯•æ•°æ®JSONæ–‡ä»¶è·¯å¾„
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ‰¹é‡è¯„ä¼°ç»“æœ
        """
        # ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
        if not self.initialize_services():
            return {
                "error": "RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥",
                "timestamp": datetime.now().isoformat(),
            }

        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_file}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_path = Path(test_file)
        if not test_path.exists():
            # å°è¯•åœ¨test_dataç›®å½•ä¸‹æŸ¥æ‰¾
            test_path = Path(__file__).parent / "test_data" / test_file

        if not test_path.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return {
                "error": f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}",
                "timestamp": datetime.now().isoformat(),
            }

        try:
            with open(test_path, "r", encoding="utf-8") as f:
                test_data = json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            return {
                "error": f"åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}",
                "timestamp": datetime.now().isoformat(),
            }

        # è·å–æµ‹è¯•ç”¨ä¾‹
        test_cases = test_data.get("end_to_end_test_cases", [])
        if not test_cases:
            test_cases = test_data.get("retrieval_test_cases", [])

        if not test_cases:
            print("âŒ æµ‹è¯•æ•°æ®ä¸ºç©º")
            return {"error": "æµ‹è¯•æ•°æ®ä¸ºç©º", "timestamp": datetime.now().isoformat()}

        if max_samples:
            test_cases = test_cases[:max_samples]

        print(f"ğŸ§ª å¼€å§‹è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹\n")

        # æ‰¹é‡è¯„ä¼°
        results = []
        for i, case in enumerate(test_cases, 1):
            print(f"[{i}/{len(test_cases)}] ", end="")

            query = case.get("query")
            expected = case.get("expected_answer_contains", [])
            ground_truth = (
                " ".join(expected) if isinstance(expected, list) else str(expected)
            )

            result = self.evaluate_single_query(
                query=query, ground_truth=ground_truth if ground_truth else None
            )
            results.append(result)

            print()

        # è®¡ç®—ç»Ÿè®¡
        successful_evals = [r for r in results if "scores" in r]
        if successful_evals:
            avg_scores = {}
            for metric in successful_evals[0]["scores"].keys():
                values = [
                    r["scores"][metric]
                    for r in successful_evals
                    if metric in r["scores"]
                ]
                avg_scores[metric] = sum(values) / len(values) if values else 0

            summary = {
                "total_cases": len(test_cases),
                "successful_evals": len(successful_evals),
                "failed_evals": len(test_cases) - len(successful_evals),
                "average_scores": avg_scores,
                "timestamp": datetime.now().isoformat(),
            }
        else:
            summary = {
                "total_cases": len(test_cases),
                "successful_evals": 0,
                "failed_evals": len(test_cases),
                "timestamp": datetime.now().isoformat(),
            }

        # ä¿å­˜ç»“æœ
        self._save_results(results, summary)

        # æ‰“å°æ€»ç»“
        self._print_summary(summary)

        return {"results": results, "summary": summary}

    def _print_scores(self, scores: Dict[str, float]):
        """æ‰“å°è¯„åˆ†"""
        for metric, score in scores.items():
            # æ ¹æ®åˆ†æ•°æ˜¾ç¤ºé¢œè‰²/è¡¨æƒ…
            if score >= 0.8:
                status = "ğŸŸ¢"
            elif score >= 0.6:
                status = "ğŸŸ¡"
            else:
                status = "ğŸ”´"

            print(f"   {status} {metric}: {score:.3f}")

    def _print_summary(self, summary: Dict):
        """æ‰“å°æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ“Š è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {summary['total_cases']}")
        print(f"æˆåŠŸè¯„ä¼°: {summary['successful_evals']} âœ…")
        print(f"å¤±è´¥: {summary['failed_evals']} âŒ")

        if "average_scores" in summary and summary["average_scores"]:
            print("\nå¹³å‡æŒ‡æ ‡å¾—åˆ†:")
            for metric, score in summary["average_scores"].items():
                if score >= 0.8:
                    status = "ğŸŸ¢ ä¼˜ç§€"
                elif score >= 0.6:
                    status = "ğŸŸ¡ è‰¯å¥½"
                else:
                    status = "ğŸ”´ éœ€ä¼˜åŒ–"

                print(f"  {metric}: {score:.3f} {status}")

        print("=" * 70)

    def _save_results(self, results: List[Dict], summary: Dict):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_dir = Path(__file__).parent / "evaluation_results"
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"ragas_eval_{timestamp}.json"

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                {"summary": summary, "results": results},
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")


class SimpleRAGEvaluator:
    """
    ç®€åŒ–ç‰ˆRAGè¯„ä¼°å™¨ï¼ˆæ— éœ€RAGASä¾èµ–ï¼‰
    ä½¿ç”¨åŸºäºè§„åˆ™çš„è¯„ä¼°
    """

    def __init__(self):
        self.results = []

    def evaluate_retrieval_quality(
        self, query: str, expected_keywords: List[str], retrieved_contexts: List[str]
    ) -> Dict[str, float]:
        """è¯„ä¼°æ£€ç´¢è´¨é‡ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        if not retrieved_contexts:
            return {
                "keyword_hit_rate": 0.0,
                "context_coverage": 0.0,
                "avg_context_length": 0,
            }

        # å…³é”®è¯å‘½ä¸­æ£€æŸ¥
        hits = 0
        for keyword in expected_keywords:
            if any(keyword in ctx for ctx in retrieved_contexts):
                hits += 1

        keyword_hit_rate = hits / len(expected_keywords) if expected_keywords else 0

        # è®¡ç®—å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦
        avg_length = sum(len(ctx) for ctx in retrieved_contexts) / len(
            retrieved_contexts
        )

        return {
            "keyword_hit_rate": keyword_hit_rate,
            "num_contexts": len(retrieved_contexts),
            "avg_context_length": avg_length,
        }


# ==================== å‘½ä»¤è¡Œå…¥å£ ====================


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="RAGASè¯„ä¼°å·¥å…·")
    parser.add_argument(
        "--mode",
        choices=["single", "batch", "simple"],
        default="single",
        help="è¯„ä¼°æ¨¡å¼",
    )
    parser.add_argument("--query", "-q", type=str, help="å•ä¸ªæŸ¥è¯¢")
    parser.add_argument("--ground-truth", "-g", type=str, help="æœŸæœ›ç­”æ¡ˆ")
    parser.add_argument(
        "--test-file",
        "-f",
        type=str,
        default="test_data/test_dataset.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶",
    )
    parser.add_argument("--max-samples", "-n", type=int, help="æœ€å¤§æ ·æœ¬æ•°")

    args = parser.parse_args()

    if args.mode == "single":
        if not args.query:
            print("âŒ è¯·æä¾›æŸ¥è¯¢: --query 'ä½ çš„é—®é¢˜'")
            return

        if not RAGAS_AVAILABLE:
            print("âš ï¸  RAGASæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–è¯„ä¼°å™¨")
            print("   å®‰è£…å‘½ä»¤: pip install ragas")
            evaluator = SimpleRAGEvaluator()
            print("ç®€åŒ–è¯„ä¼°æ¨¡å¼ - ä»…æ”¯æŒåŸºç¡€åŠŸèƒ½")
        else:
            evaluator = RAGASIntegration()
            result = evaluator.evaluate_single_query(
                query=args.query, ground_truth=args.ground_truth
            )
            if evaluator.evaluation_history:
                print("\n" + evaluator.get_evaluation_report())

    elif args.mode == "batch":
        if not RAGAS_AVAILABLE:
            print("âŒ æ‰¹é‡è¯„ä¼°éœ€è¦RAGASï¼Œè¯·å…ˆå®‰è£…: pip install ragas")
            return

        evaluator = RAGASIntegration()
        evaluator.evaluate_test_dataset(
            test_file=args.test_file, max_samples=args.max_samples
        )

    elif args.mode == "simple":
        print("ä½¿ç”¨ç®€åŒ–è¯„ä¼°å™¨ï¼ˆæ— éœ€RAGASï¼‰")
        evaluator = SimpleRAGEvaluator()
        print("ç®€åŒ–è¯„ä¼°æ¨¡å¼ - æ‰§è¡ŒåŸºç¡€å…³é”®è¯åŒ¹é…è¯„ä¼°")


if __name__ == "__main__":
    main()
