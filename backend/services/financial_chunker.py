"""
è´¢åŠ¡åˆ¶åº¦æ–‡æ¡£æ™ºèƒ½åˆ‡åˆ†å™¨
ä¸“ä¸ºè´¢åŠ¡æŠ¥é”€åˆ¶åº¦ç±»æ–‡æ¡£è®¾è®¡ï¼Œæ”¯æŒèŒçº§å·®å¼‚è¡¨æ ¼çš„æ™ºèƒ½å±•å¼€
"""

import re
import json
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, field


@dataclass
class Chunk:
    """æ–‡æ¡£ç‰‡æ®µ"""

    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    chunk_type: str = "text"  # text, table, procedure
    level: str = ""  # èŒçº§æ ‡ç­¾
    expense_type: str = ""  # è´¹ç”¨ç±»å‹


class FinancialDocumentChunker:
    """è´¢åŠ¡åˆ¶åº¦æ–‡æ¡£åˆ‡åˆ†å™¨"""

    # èŒçº§å…³é”®è¯æ˜ å°„
    LEVEL_KEYWORDS = {
        "8-9çº§": [
            "8-9çº§",
            "æ™®é€šå‘˜å·¥",
            "è½¯ä»¶ç ”å‘å·¥ç¨‹å¸ˆ",
            "æœºæ¢°ç ”å‘å·¥ç¨‹å¸ˆ",
            "å·¥è‰ºå·¥ç¨‹å¸ˆ",
            "å®æ–½å·¥ç¨‹å¸ˆ",
        ],
        "10-11çº§": ["10-11çº§", "ç»ç†çº§", "ç»ç†"],
        "12çº§åŠä»¥ä¸Š": ["12çº§åŠä»¥ä¸Š", "æ€»ç›‘", "ä¸“å®¶çº§", "ä¸“å®¶"],
    }

    # è´¹ç”¨ç±»å‹å…³é”®è¯
    EXPENSE_TYPES = {
        "å·®æ—…è´¹": ["å·®æ—…", "å‡ºå·®", "äº¤é€šå·¥å…·", "ä½å®¿", "è¡¥è´´"],
        "ä¸šåŠ¡æ‹›å¾…è´¹": ["æ‹›å¾…", "å®´è¯·", "å®¢æˆ·", "ç¤¼å“"],
        "é€šè®¯è´¹": ["é€šè®¯", "ç”µè¯", "æ‰‹æœº"],
        "åŠå…¬è´¹": ["åŠå…¬", "æ–‡å…·", "ä¹¦ç±", "è½¯ä»¶"],
        "åŸ¹è®­ä¼šè®®è´¹": ["åŸ¹è®­", "ä¼šè®®"],
        "å€Ÿæ¬¾": ["å€Ÿæ¬¾", "å¤‡ç”¨é‡‘"],
    }

    def __init__(self, max_chunk_size: int = 800, overlap: int = 100):
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap

    def chunk_document(self, document_text: str, doc_id: str = "") -> List[Chunk]:
        """
        ä¸»åˆ‡åˆ†æ–¹æ³•

        Args:
            document_text: æ–‡æ¡£å®Œæ•´æ–‡æœ¬
            doc_id: æ–‡æ¡£ID

        Returns:
            List[Chunk]: åˆ‡åˆ†åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        chunks = []

        # ç¬¬ä¸€æ­¥ï¼šæŒ‰ä¸€çº§æ ‡é¢˜åˆ‡åˆ†
        sections = self._split_by_headers(document_text)

        for section_title, section_content in sections:
            # ç¬¬äºŒæ­¥ï¼šè¯†åˆ«å¹¶å¤„ç†ç‰¹æ®ŠåŒºåŸŸ
            section_chunks = self._process_section(section_title, section_content)
            chunks.extend(section_chunks)

        # ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ å…¨å±€metadata
        for i, chunk in enumerate(chunks):
            chunk.metadata.update(
                {"doc_id": doc_id, "chunk_index": i, "total_chunks": len(chunks)}
            )

        return chunks

    def _split_by_headers(self, text: str) -> List[Tuple[str, str]]:
        """æŒ‰ä¸€çº§æ ‡é¢˜(#)åˆ‡åˆ†æ–‡æ¡£"""
        # åŒ¹é… # å¼€å¤´çš„æ ‡é¢˜
        pattern = r"(^|\n)#\s+(.+?)(?=\n#\s|\Z)"
        matches = list(re.finditer(pattern, text, re.MULTILINE | re.DOTALL))

        sections = []
        for i, match in enumerate(matches):
            title = match.group(2).strip()
            content = match.group(0).strip()
            sections.append((title, content))

        return sections

    def _process_section(self, title: str, content: str) -> List[Chunk]:
        """å¤„ç†å•ä¸ªç« èŠ‚"""
        chunks = []

        # è¯†åˆ«ç« èŠ‚ç±»å‹
        if self._is_table_section(content):
            # åŒ…å«èŒçº§è¡¨æ ¼çš„ç« èŠ‚
            table_chunks = self._process_table_section(title, content)
            chunks.extend(table_chunks)
        elif self._is_procedure_section(content):
            # æµç¨‹ç±»ç« èŠ‚
            procedure_chunk = self._create_procedure_chunk(title, content)
            chunks.append(procedure_chunk)
        else:
            # æ™®é€šæ–‡æœ¬ç« èŠ‚
            text_chunks = self._split_text_section(title, content)
            chunks.extend(text_chunks)

        return chunks

    def _is_table_section(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åŒ…å«èŒçº§å·®å¼‚è¡¨æ ¼"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«markdownè¡¨æ ¼ä¸”æåˆ°èŒçº§
        has_table = "|" in content and "---" in content
        has_level = any(
            keyword in content
            for keywords in self.LEVEL_KEYWORDS.values()
            for keyword in keywords
        )
        return has_table and has_level

    def _is_procedure_section(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æµç¨‹ç±»ç« èŠ‚"""
        procedure_keywords = ["æµç¨‹", "æ­¥éª¤", "å®¡æ‰¹æµ", "ç”³è¯·äºº", "å®¡æ‰¹"]
        return any(keyword in content for keyword in procedure_keywords) and (
            "â†’" in content or "```" in content
        )

    def _process_table_section(self, title: str, content: str) -> List[Chunk]:
        """å¤„ç†åŒ…å«èŒçº§è¡¨æ ¼çš„ç« èŠ‚ï¼ŒæŒ‰èŒçº§å±•å¼€"""
        chunks = []

        # æå–è¡¨æ ¼
        tables = self._extract_tables(content)

        for table in tables:
            # åˆ¤æ–­è¡¨æ ¼æ˜¯å¦åŒ…å«èŒçº§å·®å¼‚
            if self._contains_level_differences(table):
                # æŒ‰èŒçº§å±•å¼€è¡¨æ ¼
                level_chunks = self._expand_table_by_level(title, table, content)
                chunks.extend(level_chunks)
            else:
                # æ™®é€šè¡¨æ ¼ä½œä¸ºä¸€ä¸ªchunk
                chunk = Chunk(
                    content=f"## {title}\n\n{table}",
                    chunk_type="table",
                    metadata={"section": title},
                )
                chunk.level = self._detect_level(table)
                chunk.expense_type = self._detect_expense_type(table)
                chunks.append(chunk)

        # å¤„ç†è¡¨æ ¼å¤–çš„æ–‡æœ¬
        non_table_content = self._remove_tables(content, tables)
        if non_table_content.strip():
            text_chunks = self._split_text_section(title, non_table_content)
            chunks.extend(text_chunks)

        return chunks

    def _extract_tables(self, content: str) -> List[str]:
        """æå–markdownè¡¨æ ¼"""
        tables = []
        # åŒ¹é…markdownè¡¨æ ¼ï¼ˆ|å¼€å¤´ï¼ŒåŒ…å«åˆ†éš”è¡Œ|---|ï¼‰
        pattern = r"\|[^\n]+\|\n\|[-:|\s]+\|\n(?:\|[^\n]+\|\n?)+"
        matches = re.finditer(pattern, content)

        for match in matches:
            tables.append(match.group(0))

        return tables

    def _contains_level_differences(self, table: str) -> bool:
        """åˆ¤æ–­è¡¨æ ¼æ˜¯å¦åŒ…å«èŒçº§å·®å¼‚"""
        return any(
            level in table
            for level in ["8-9çº§", "10-11çº§", "12çº§åŠä»¥ä¸Š", "æ™®é€šå‘˜å·¥", "ç»ç†", "æ€»ç›‘"]
        )

    def _expand_table_by_level(
        self, title: str, table: str, context: str
    ) -> List[Chunk]:
        """æŒ‰èŒçº§å±•å¼€è¡¨æ ¼"""
        chunks = []

        # è§£æè¡¨æ ¼ç»“æ„
        rows = [row.strip() for row in table.strip().split("\n") if row.strip()]
        if len(rows) < 3:  # è¡¨å¤´+åˆ†éš”è¡Œ+è‡³å°‘ä¸€è¡Œæ•°æ®
            return [Chunk(content=f"## {title}\n\n{table}", chunk_type="table")]

        header = rows[0]
        separator = rows[1]
        data_rows = rows[2:]

        # ä¸ºæ¯ä¸ªèŒçº§åˆ›å»ºç‹¬ç«‹çš„chunk
        for level_name, keywords in self.LEVEL_KEYWORDS.items():
            level_rows = []
            for row in data_rows:
                if any(keyword in row for keyword in keywords):
                    level_rows.append(row)

            if level_rows:
                # æ„å»ºè¯¥èŒçº§çš„ä¸“å±è¡¨æ ¼
                level_table = f"{header}\n{separator}\n" + "\n".join(level_rows)

                # æ·»åŠ ç›¸å…³ä¸Šä¸‹æ–‡è¯´æ˜
                context_info = self._extract_context_for_level(context, level_name)

                chunk_content = (
                    f"## {title} - {level_name}\n\n{context_info}\n\n{level_table}"
                )

                chunk = Chunk(
                    content=chunk_content,
                    chunk_type="table",
                    level=level_name,
                    metadata={
                        "section": title,
                        "level": level_name,
                        "table_type": "level_specific",
                    },
                )
                chunk.expense_type = self._detect_expense_type(table)
                chunks.append(chunk)

        return chunks

    def _extract_context_for_level(self, content: str, level: str) -> str:
        """æå–ä¸ç‰¹å®šèŒçº§ç›¸å…³çš„ä¸Šä¸‹æ–‡è¯´æ˜"""
        # æå–è¯¥èŒçº§ç›¸å…³çš„æ³¨é‡Šå’Œè¯´æ˜
        context_parts = []

        # æŸ¥æ‰¾åŒ…å«èŒçº§å…³é”®è¯çš„æ®µè½
        paragraphs = content.split("\n\n")
        for para in paragraphs:
            if level.replace("çº§", "") in para or any(
                keyword in para for keyword in self.LEVEL_KEYWORDS.get(level, [])
            ):
                if "æ³¨ï¼š" in para or "è¯´æ˜ï¼š" in para or "æ³¨æ„" in para:
                    context_parts.append(para.strip())

        return "\n".join(context_parts) if context_parts else ""

    def _create_procedure_chunk(self, title: str, content: str) -> Chunk:
        """åˆ›å»ºæµç¨‹ç±»chunkï¼Œä¿æŒå®Œæ•´æ€§"""
        chunk = Chunk(
            content=f"## {title}\n\n{content}",
            chunk_type="procedure",
            metadata={"section": title, "type": "procedure"},
        )
        chunk.level = self._detect_level(content)
        chunk.expense_type = self._detect_expense_type(content)
        return chunk

    def _split_text_section(self, title: str, content: str) -> List[Chunk]:
        """åˆ‡åˆ†æ™®é€šæ–‡æœ¬ç« èŠ‚"""
        chunks = []

        # æŒ‰äºŒçº§æ ‡é¢˜(##)è¿›ä¸€æ­¥åˆ‡åˆ†
        subsections = re.split(r"\n##\s+", content)

        for subsection in subsections:
            if not subsection.strip():
                continue

            # å¦‚æœå†…å®¹å¤ªé•¿ï¼ŒæŒ‰æ®µè½åˆ‡åˆ†
            if len(subsection) > self.max_chunk_size:
                paragraph_chunks = self._split_by_paragraphs(title, subsection)
                chunks.extend(paragraph_chunks)
            else:
                chunk = Chunk(
                    content=f"## {title}\n\n{subsection.strip()}",
                    chunk_type="text",
                    metadata={"section": title},
                )
                chunk.level = self._detect_level(subsection)
                chunk.expense_type = self._detect_expense_type(subsection)
                chunks.append(chunk)

        return chunks

    def _split_by_paragraphs(self, title: str, content: str) -> List[Chunk]:
        """æŒ‰æ®µè½åˆ‡åˆ†é•¿æ–‡æœ¬"""
        chunks = []
        paragraphs = content.split("\n\n")

        current_chunk = f"## {title}\n\n"
        current_size = len(current_chunk)

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            para_size = len(para) + 2  # +2 for \n\n

            if (
                current_size + para_size > self.max_chunk_size
                and current_chunk.strip() != f"## {title}"
            ):
                # ä¿å­˜å½“å‰chunk
                chunk = Chunk(
                    content=current_chunk.strip(),
                    chunk_type="text",
                    metadata={"section": title},
                )
                chunk.level = self._detect_level(current_chunk)
                chunk.expense_type = self._detect_expense_type(current_chunk)
                chunks.append(chunk)

                # å¼€å§‹æ–°chunkï¼Œä¿ç•™é‡å 
                overlap_text = self._get_overlap(current_chunk)
                current_chunk = f"## {title}\n\n{overlap_text}\n\n{para}"
                current_size = len(current_chunk)
            else:
                current_chunk += f"\n\n{para}"
                current_size += para_size

        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk.strip() != f"## {title}":
            chunk = Chunk(
                content=current_chunk.strip(),
                chunk_type="text",
                metadata={"section": title},
            )
            chunk.level = self._detect_level(current_chunk)
            chunk.expense_type = self._detect_expense_type(current_chunk)
            chunks.append(chunk)

        return chunks

    def _get_overlap(self, text: str) -> str:
        """è·å–æ–‡æœ¬æœ«å°¾ä½œä¸ºé‡å éƒ¨åˆ†"""
        lines = text.strip().split("\n")
        overlap_lines = (
            lines[-3:] if len(lines) > 3 else lines[-2:] if len(lines) > 1 else []
        )
        return "\n".join(overlap_lines)

    def _remove_tables(self, content: str, tables: List[str]) -> str:
        """ä»å†…å®¹ä¸­ç§»é™¤è¡¨æ ¼"""
        result = content
        for table in tables:
            result = result.replace(table, "")
        return result

    def _detect_level(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬æ¶‰åŠçš„èŒçº§"""
        levels = []
        for level_name, keywords in self.LEVEL_KEYWORDS.items():
            if any(keyword in text for keyword in keywords):
                levels.append(level_name)
        return ",".join(levels) if levels else ""

    def _detect_expense_type(self, text: str) -> str:
        """æ£€æµ‹è´¹ç”¨ç±»å‹"""
        types = []
        for type_name, keywords in self.EXPENSE_TYPES.items():
            if any(keyword in text for keyword in keywords):
                types.append(type_name)
        return ",".join(types) if types else ""


class ChunkingService:
    """åˆ‡åˆ†æœåŠ¡å°è£…"""

    @staticmethod
    def chunk_financial_document(file_path: str, doc_id: str = "") -> List[Dict]:
        """
        åˆ‡åˆ†è´¢åŠ¡åˆ¶åº¦æ–‡æ¡£

        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            doc_id: æ–‡æ¡£ID

        Returns:
            List[Dict]: åˆ‡åˆ†ç»“æœåˆ—è¡¨
        """
        # è¯»å–æ–‡æ¡£
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # åˆ›å»ºåˆ‡åˆ†å™¨
        chunker = FinancialDocumentChunker(max_chunk_size=800, overlap=100)

        # æ‰§è¡Œåˆ‡åˆ†
        chunks = chunker.chunk_document(content, doc_id)

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = []
        for i, chunk in enumerate(chunks):
            result.append(
                {
                    "chunk_id": f"{doc_id}_chunk_{i:03d}",
                    "content": chunk.content,
                    "metadata": {
                        **chunk.metadata,
                        "chunk_type": chunk.chunk_type,
                        "level": chunk.level,
                        "expense_type": chunk.expense_type,
                        "char_count": len(chunk.content),
                    },
                }
            )

        return result

    @staticmethod
    def save_chunks(chunks: List[Dict], output_path: str):
        """ä¿å­˜åˆ‡åˆ†ç»“æœåˆ°JSONæ–‡ä»¶"""
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µï¼Œå·²ä¿å­˜åˆ°: {output_path}")


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šåˆ‡åˆ†baoxiao.md
    file_path = "/root/autodl-tmp/rag/backend/data/docs/baoxiao.md"
    doc_id = "baoxiao_001"
    output_path = "/root/autodl-tmp/rag/backend/data/chunks/baoxiao_chunks.json"

    # æ‰§è¡Œåˆ‡åˆ†
    chunks = ChunkingService.chunk_financial_document(file_path, doc_id)

    # ä¿å­˜ç»“æœ
    ChunkingService.save_chunks(chunks, output_path)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š åˆ‡åˆ†ç»Ÿè®¡ï¼š")
    print(f"æ€»è®¡ç”Ÿæˆ: {len(chunks)} ä¸ªchunk")

    # æŒ‰ç±»å‹ç»Ÿè®¡
    type_count = {}
    level_count = {}
    for chunk in chunks:
        chunk_type = chunk["metadata"]["chunk_type"]
        level = chunk["metadata"]["level"]

        type_count[chunk_type] = type_count.get(chunk_type, 0) + 1
        if level:
            for l in level.split(","):
                level_count[l] = level_count.get(l, 0) + 1

    print("\næŒ‰ç±»å‹åˆ†å¸ƒï¼š")
    for t, count in type_count.items():
        print(f"  {t}: {count}ä¸ª")

    print("\næŒ‰èŒçº§åˆ†å¸ƒï¼š")
    for l, count in level_count.items():
        print(f"  {l}: {count}ä¸ª")

    # æ‰“å°å‰3ä¸ªchunkç¤ºä¾‹
    print("\nğŸ“ å‰3ä¸ªChunkç¤ºä¾‹ï¼š")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"ç±»å‹: {chunk['metadata']['chunk_type']}")
        print(f"èŒçº§: {chunk['metadata']['level']}")
        print(f"è´¹ç”¨ç±»å‹: {chunk['metadata']['expense_type']}")
        print(f"å†…å®¹é¢„è§ˆ: {chunk['content'][:150]}...")
