"""
è´¢åŠ¡åˆ¶åº¦æ–‡æ¡£æ™ºèƒ½åˆ‡åˆ†å™¨ - ä¼˜åŒ–ç‰ˆ
ä¸“ä¸ºè´¢åŠ¡æŠ¥é”€åˆ¶åº¦ç±»æ–‡æ¡£è®¾è®¡
"""

import re
import json
from typing import List, Dict, Any
from dataclasses import dataclass, field


@dataclass
class Chunk:
    """æ–‡æ¡£ç‰‡æ®µ"""

    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    chunk_type: str = "text"  # text, table, procedure


class FinancialDocumentChunker:
    """è´¢åŠ¡åˆ¶åº¦æ–‡æ¡£åˆ‡åˆ†å™¨"""

    def __init__(self, max_chunk_size: int = 1000):
        self.max_chunk_size = max_chunk_size

    def chunk_document(self, document_text: str, doc_id: str = "") -> List[Chunk]:
        """ä¸»åˆ‡åˆ†æ–¹æ³•"""
        chunks = []

        # æ­¥éª¤1ï¼šæŒ‰ä¸€çº§æ ‡é¢˜(#)åˆ‡åˆ†å¤§ç« èŠ‚
        sections = self._split_by_level1_headers(document_text)

        for section_title, section_content in sections:
            # æ­¥éª¤2ï¼šå¤„ç†æ¯ä¸ªç« èŠ‚
            section_chunks = self._process_section(section_title, section_content)
            chunks.extend(section_chunks)

        # æ·»åŠ å…¨å±€metadata
        for i, chunk in enumerate(chunks):
            chunk.metadata.update(
                {"doc_id": doc_id, "chunk_index": i, "total_chunks": len(chunks)}
            )

        return chunks

    def _split_by_level1_headers(self, text: str) -> List[tuple]:
        """æŒ‰ä¸€çº§æ ‡é¢˜(# )åˆ‡åˆ†æ–‡æ¡£"""
        # åŒ¹é…ä»¥#å¼€å¤´ä¸”åé¢æœ‰ç©ºæ ¼çš„æ ‡é¢˜
        pattern = r"\n#\s+([^\n]+)\n"

        # æ‰¾åˆ°æ‰€æœ‰ä¸€çº§æ ‡é¢˜ä½ç½®
        matches = list(re.finditer(pattern, "\n" + text))

        sections = []
        for i, match in enumerate(matches):
            title = match.group(1).strip()
            start = match.end()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            content = text[start:end].strip()
            sections.append((title, content))

        return sections

    def _process_section(self, title: str, content: str) -> List[Chunk]:
        """å¤„ç†å•ä¸ªç« èŠ‚"""
        chunks = []

        # æå–è¯¥ç« èŠ‚çš„å¼•è¨€ï¼ˆå¦‚æœæœ‰ï¼‰
        intro_match = re.match(r"^([^#].*?)(?=\n##|\Z)", content, re.DOTALL)
        intro = intro_match.group(1).strip() if intro_match else ""

        # æŒ‰äºŒçº§æ ‡é¢˜(## )åˆ‡åˆ†
        subsections = self._split_by_level2_headers(content)

        if not subsections:
            # æ²¡æœ‰äºŒçº§æ ‡é¢˜ï¼Œæ•´ä¸ªç« èŠ‚ä½œä¸ºä¸€ä¸ªchunk
            chunk_content = f"# {title}\n\n{content}".strip()
            chunks.append(self._create_chunk(chunk_content, "text", title))
        else:
            for sub_title, sub_content in subsections:
                # å¤„ç†å­ç« èŠ‚
                sub_chunks = self._process_subsection(
                    title, sub_title, sub_content, intro
                )
                chunks.extend(sub_chunks)

        return chunks

    def _split_by_level2_headers(self, text: str) -> List[tuple]:
        """æŒ‰äºŒçº§æ ‡é¢˜(## )åˆ‡åˆ†"""
        pattern = r"\n##\s+([^\n]+)\n"
        matches = list(re.finditer(pattern, "\n" + text))

        subsections = []
        for i, match in enumerate(matches):
            title = match.group(1).strip()
            start = match.end()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            content = text[start:end].strip()
            subsections.append((title, content))

        return subsections

    def _process_subsection(
        self, section_title: str, sub_title: str, content: str, intro: str = ""
    ) -> List[Chunk]:
        """å¤„ç†äºŒçº§å­ç« èŠ‚"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
        if "|" in content and "---" in content:
            return self._process_with_table(section_title, sub_title, content, intro)
        else:
            # æ™®é€šæ–‡æœ¬
            chunk_content = f"# {section_title}\n\n## {sub_title}\n\n{content}"
            return [self._create_chunk(chunk_content, "text", section_title, sub_title)]

    def _process_with_table(
        self, section_title: str, sub_title: str, content: str, intro: str = ""
    ) -> List[Chunk]:
        """å¤„ç†åŒ…å«è¡¨æ ¼çš„å†…å®¹"""
        chunks = []

        # æå–è¡¨æ ¼
        tables = self._extract_tables(content)

        # æå–è¡¨æ ¼å‰åçš„æ–‡æœ¬
        non_table_text = content
        for table in tables:
            non_table_text = non_table_text.replace(table, "[TABLE]")

        text_parts = [p.strip() for p in non_table_text.split("[TABLE]") if p.strip()]

        # ä¸ºæ¯ä¸ªè¡¨æ ¼åˆ›å»ºä¸€ä¸ªchunkï¼Œé™„å¸¦ä¸Šä¸‹æ–‡
        for i, table in enumerate(tables):
            context_before = text_parts[i] if i < len(text_parts) else ""
            context_after = text_parts[i + 1] if i + 1 < len(text_parts) else ""

            # åˆ¤æ–­æ˜¯å¦æ˜¯èŒçº§å·®å¼‚è¡¨æ ¼
            if self._is_level_table(table):
                # æŒ‰èŒçº§å±•å¼€
                level_chunks = self._expand_by_level(
                    section_title, sub_title, table, context_before, intro
                )
                chunks.extend(level_chunks)
            else:
                # æ™®é€šè¡¨æ ¼
                chunk_content = f"# {section_title}\n\n## {sub_title}\n\n{context_before}\n\n{table}\n\n{context_after}".strip()
                chunks.append(
                    self._create_chunk(chunk_content, "table", section_title, sub_title)
                )

        # å¦‚æœæ²¡æœ‰è¡¨æ ¼æˆ–æ–‡æœ¬éƒ¨åˆ†è¿˜æœ‰å‰©ä½™
        if not tables and text_parts:
            chunk_content = f"# {section_title}\n\n## {sub_title}\n\n{text_parts[0]}"
            chunks.append(
                self._create_chunk(chunk_content, "text", section_title, sub_title)
            )

        return chunks

    def _extract_tables(self, content: str) -> List[str]:
        """æå–markdownè¡¨æ ¼"""
        tables = []
        # åŒ¹é…markdownè¡¨æ ¼
        pattern = r"\|[^\n]+\|\n\|[-:\|\s]+\|\n(?:\|[^\n]+\|\n?)+"
        matches = re.finditer(pattern, content)

        for match in matches:
            tables.append(match.group(0).strip())

        return tables

    def _is_level_table(self, table: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯åŒ…å«èŒçº§å·®å¼‚çš„è¡¨æ ¼"""
        level_keywords = [
            "8-9çº§",
            "10-11çº§",
            "12çº§åŠä»¥ä¸Š",
            "æ™®é€šå‘˜å·¥",
            "ç»ç†",
            "æ€»ç›‘",
            "ä¸“å®¶",
        ]
        return any(keyword in table for keyword in level_keywords)

    def _expand_by_level(
        self,
        section_title: str,
        sub_title: str,
        table: str,
        context: str,
        intro: str = "",
    ) -> List[Chunk]:
        """æŒ‰èŒçº§å±•å¼€è¡¨æ ¼"""
        chunks = []

        # è§£æè¡¨æ ¼
        lines = [line.strip() for line in table.split("\n") if line.strip()]
        if len(lines) < 3:
            # è¡¨æ ¼æ ¼å¼ä¸å¯¹ï¼Œç›´æ¥è¿”å›
            chunk_content = f"# {section_title}\n\n## {sub_title}\n\n{table}"
            return [
                self._create_chunk(chunk_content, "table", section_title, sub_title)
            ]

        header_line = lines[0]
        separator = lines[1]
        data_lines = lines[2:]

        # èŒçº§å…³é”®è¯æ˜ å°„
        level_keywords = {
            "æ™®é€šå‘˜å·¥/8-9çº§": ["8-9çº§", "æ™®é€šå‘˜å·¥"],
            "ç»ç†/10-11çº§": ["10-11çº§", "ç»ç†"],
            "æ€»ç›‘åŠä»¥ä¸Š/12çº§": ["12çº§åŠä»¥ä¸Š", "æ€»ç›‘", "ä¸“å®¶"],
        }

        # ä¸ºæ¯ä¸ªèŒçº§åˆ›å»ºç‹¬ç«‹chunk
        for level_name, keywords in level_keywords.items():
            level_data = []
            for line in data_lines:
                if any(keyword in line for keyword in keywords):
                    level_data.append(line)

            if level_data:
                # æ„å»ºè¯¥èŒçº§çš„è¡¨æ ¼
                level_table = f"{header_line}\n{separator}\n" + "\n".join(level_data)

                # æ„å»ºchunkå†…å®¹
                chunk_parts = [f"# {section_title}"]
                if intro:
                    chunk_parts.append(intro)
                chunk_parts.extend(
                    [f"## {sub_title} - {level_name}", context, level_table]
                )

                chunk_content = "\n\n".join(chunk_parts)
                chunk = self._create_chunk(
                    chunk_content, "table", section_title, sub_title
                )
                chunk.metadata["level"] = level_name
                chunks.append(chunk)

        return chunks

    def _create_chunk(
        self, content: str, chunk_type: str, section: str, subsection: str = ""
    ) -> Chunk:
        """åˆ›å»ºChunkå¯¹è±¡"""
        chunk = Chunk(
            content=content.strip(),
            chunk_type=chunk_type,
            metadata={
                "section": section,
                "subsection": subsection,
                "char_count": len(content),
            },
        )

        # è‡ªåŠ¨æ£€æµ‹èŒçº§å’Œè´¹ç”¨ç±»å‹
        chunk.metadata["level"] = self._detect_level(content)
        chunk.metadata["expense_type"] = self._detect_expense_type(content)

        return chunk

    def _detect_level(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬æ¶‰åŠçš„èŒçº§"""
        levels = []
        if "8-9çº§" in text or "æ™®é€šå‘˜å·¥" in text:
            levels.append("8-9çº§")
        if "10-11çº§" in text or "ç»ç†" in text:
            levels.append("10-11çº§")
        if "12çº§åŠä»¥ä¸Š" in text or "æ€»ç›‘" in text or "ä¸“å®¶" in text:
            levels.append("12çº§åŠä»¥ä¸Š")
        return ",".join(levels)

    def _detect_expense_type(self, text: str) -> str:
        """æ£€æµ‹è´¹ç”¨ç±»å‹"""
        types = []
        if any(kw in text for kw in ["å·®æ—…", "å‡ºå·®", "äº¤é€šå·¥å…·", "ä½å®¿", "è¡¥è´´"]):
            types.append("å·®æ—…è´¹")
        if any(kw in text for kw in ["æ‹›å¾…", "å®´è¯·", "å®¢æˆ·", "ç¤¼å“"]):
            types.append("ä¸šåŠ¡æ‹›å¾…è´¹")
        if any(kw in text for kw in ["é€šè®¯", "ç”µè¯", "æ‰‹æœº"]):
            types.append("é€šè®¯è´¹")
        if any(kw in text for kw in ["åŠå…¬", "æ–‡å…·", "ä¹¦ç±", "è½¯ä»¶"]):
            types.append("åŠå…¬è´¹")
        if any(kw in text for kw in ["åŸ¹è®­", "ä¼šè®®"]):
            types.append("åŸ¹è®­ä¼šè®®è´¹")
        return ",".join(types)


class ChunkingService:
    """åˆ‡åˆ†æœåŠ¡å°è£…"""

    @staticmethod
    def chunk_financial_document(file_path: str, doc_id: str = "") -> List[Dict]:
        """åˆ‡åˆ†è´¢åŠ¡åˆ¶åº¦æ–‡æ¡£"""
        # è¯»å–æ–‡æ¡£
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # åˆ›å»ºåˆ‡åˆ†å™¨
        chunker = FinancialDocumentChunker(max_chunk_size=1000)

        # æ‰§è¡Œåˆ‡åˆ†
        chunks = chunker.chunk_document(content, doc_id)

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = []
        for i, chunk in enumerate(chunks):
            result.append(
                {
                    "chunk_id": f"{doc_id}_chunk_{i:03d}",
                    "content": chunk.content,
                    "metadata": {**chunk.metadata, "chunk_type": chunk.chunk_type},
                }
            )

        return result

    @staticmethod
    def save_chunks(chunks: List[Dict], output_path: str):
        """ä¿å­˜åˆ‡åˆ†ç»“æœ"""
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    import os

    file_path = "/root/autodl-tmp/rag/backend/data/docs/baoxiao.md"
    doc_id = "baoxiao_001"
    output_dir = "/root/autodl-tmp/rag/backend/data/chunks"
    output_path = os.path.join(output_dir, "baoxiao_chunks_v2.json")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # æ‰§è¡Œåˆ‡åˆ†
    chunks = ChunkingService.chunk_financial_document(file_path, doc_id)

    # ä¿å­˜ç»“æœ
    ChunkingService.save_chunks(chunks, output_path)

    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š åˆ‡åˆ†ç»Ÿè®¡ï¼š")
    print(f"æ€»è®¡ç”Ÿæˆ: {len(chunks)} ä¸ªchunk")

    # æŒ‰ç±»å‹ç»Ÿè®¡
    type_count = {}
    level_chunks = {"8-9çº§": [], "10-11çº§": [], "12çº§åŠä»¥ä¸Š": []}

    for chunk in chunks:
        chunk_type = chunk["metadata"]["chunk_type"]
        type_count[chunk_type] = type_count.get(chunk_type, 0) + 1

        level = chunk["metadata"].get("level", "")
        for lv in level_chunks.keys():
            if lv in level:
                level_chunks[lv].append(chunk["chunk_id"])

    print("\næŒ‰ç±»å‹åˆ†å¸ƒï¼š")
    for t, count in type_count.items():
        print(f"  {t}: {count}ä¸ª")

    print("\næŒ‰èŒçº§åˆ†å¸ƒï¼š")
    for lv, ids in level_chunks.items():
        print(f"  {lv}: {len(ids)}ä¸ªchunk")

    # æ‰“å°ç¤ºä¾‹
    print("\nğŸ“ ç¤ºä¾‹Chunkï¼ˆæ™®é€šå‘˜å·¥-å·®æ—…è´¹ï¼‰ï¼š")
    for chunk in chunks:
        if "8-9çº§" in chunk["metadata"].get("level", "") and "å·®æ—…" in chunk["content"]:
            print(f"\nChunk ID: {chunk['chunk_id']}")
            print(f"èŒçº§: {chunk['metadata']['level']}")
            print(f"è´¹ç”¨ç±»å‹: {chunk['metadata']['expense_type']}")
            print(f"å†…å®¹:\n{chunk['content'][:300]}...")
            break
