#!/usr/bin/env python3
"""
ä¸‹è½½å’Œé…ç½®æœ¬åœ° LLM æ¨¡å‹ - ä¿®å¤ç‰ˆ
æ”¯æŒå¤šç§ä¸‹è½½æ–¹å¼
"""

import os
from pathlib import Path


def download_qwen_model():
    """ä¸‹è½½ Qwen2.5-0.5B-Instruct æ¨¡å‹"""
    print("=" * 60)
    print("ä¸‹è½½ Qwen2.5-0.5B-Instruct æ¨¡å‹")
    print("=" * 60)
    print()
    print("è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ä¸­æ–‡æ¨¡å‹ï¼Œé€‚åˆåœ¨ CPU ä¸Šè¿è¡Œ")
    print("æ¨¡å‹å¤§å°: çº¦ 1GB")
    print()

    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    model_dir = Path("./data/models/Qwen/Qwen2.5-0.5B-Instruct")
    model_dir.mkdir(parents=True, exist_ok=True)

    print(f"æ¨¡å‹å°†ä¸‹è½½åˆ°: {model_dir.absolute()}")
    print()

    try:
        from huggingface_hub import snapshot_download
        import huggingface_hub

        print("å¼€å§‹ä¸‹è½½...")

        # æ£€æŸ¥huggingface_hubç‰ˆæœ¬
        version = huggingface_hub.__version__
        print(f"huggingface_hub ç‰ˆæœ¬: {version}")

        # æ–°ç‰ˆæœ¬(>=0.20.0)ä¸å†æ”¯æŒtrust_remote_codeå‚æ•°
        try:
            # å°è¯•æ–°ç‰ˆæœ¬API
            snapshot_download(
                repo_id="Qwen/Qwen2.5-0.5B-Instruct",
                local_dir=str(model_dir),
                local_dir_use_symlinks=False,
                # ç§»é™¤trust_remote_codeå‚æ•°
            )
        except TypeError as e:
            if "unexpected keyword argument" in str(e):
                # æ—§ç‰ˆæœ¬API
                snapshot_download(
                    repo_id="Qwen/Qwen2.5-0.5B-Instruct",
                    local_dir=str(model_dir),
                    local_dir_use_symlinks=False,
                )
            else:
                raise

        print()
        print("=" * 60)
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        print("=" * 60)
        print()
        print("ä¸‹ä¸€æ­¥:")
        print("1. æ›´æ–° .env æ–‡ä»¶ï¼Œè®¾ç½®:")
        print("   LLM_PROVIDER=local")
        print("   LLM_MODEL=Qwen2.5-0.5B-Instruct")
        print("   LOCAL_LLM_MODEL_PATH=./data/models/Qwen/Qwen2.5-0.5B-Instruct")
        print()
        print("2. æˆ–è€…ç›´æ¥ä¿®æ”¹ config.py ä¸­çš„é…ç½®")
        print()
        print("3. é‡å¯åç«¯æœåŠ¡")
        print()

    except ImportError:
        print("âŒ é”™è¯¯: éœ€è¦å®‰è£… huggingface_hub")
        print("è¯·è¿è¡Œ: pip install huggingface_hub")
        return False
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        print()
        print("ğŸ’¡ å°è¯•å¤‡é€‰æ–¹æ¡ˆ:")
        print("   1. ä½¿ç”¨ModelScope(å›½å†…é•œåƒ): python download_model_modelscope.py")
        print("   2. ä½¿ç”¨Ollama: ollama pull qwen2.5:0.5b")
        print(
            "   3. æ‰‹åŠ¨ä¸‹è½½: è®¿é—® https://modelscope.cn/models/qwen/Qwen2.5-0.5B-Instruct"
        )
        return False

    return True


def download_from_modelscope():
    """ä»ModelScopeä¸‹è½½ï¼ˆå›½å†…é•œåƒï¼Œæ›´å¿«ï¼‰"""
    print("=" * 60)
    print("ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼ˆå›½å†…é•œåƒï¼‰")
    print("=" * 60)
    print()

    model_dir = Path("./data/models/Qwen/Qwen2.5-0.5B-Instruct")
    model_dir.mkdir(parents=True, exist_ok=True)

    print(f"æ¨¡å‹å°†ä¸‹è½½åˆ°: {model_dir.absolute()}")
    print()

    try:
        from modelscope import snapshot_download

        print("å¼€å§‹ä¸‹è½½...")
        snapshot_download(
            model_id="qwen/Qwen2.5-0.5B-Instruct",
            local_dir=str(model_dir),
            local_dir_use_symlinks=False,
        )

        print()
        print("=" * 60)
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        print("=" * 60)
        return True

    except ImportError:
        print("âŒ é”™è¯¯: éœ€è¦å®‰è£… modelscope")
        print("è¯·è¿è¡Œ: pip install modelscope")
        return False
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        return False


def install_requirements():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("=" * 60)
    print("å®‰è£…å¿…è¦çš„ä¾èµ–")
    print("=" * 60)
    print()

    requirements = [
        "transformers>=4.35.0",
        "torch>=2.0.0",
        "huggingface_hub>=0.19.0",
        "accelerate>=0.24.0",
    ]

    import subprocess

    for req in requirements:
        print(f"å®‰è£… {req}...")
        try:
            subprocess.run(
                ["pip", "install", req], check=True, capture_output=True, text=True
            )
            print(f"âœ… {req} å®‰è£…æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ {req} å®‰è£…å¤±è´¥")
            print(e.stderr)
            return False

    print()
    print("âœ… æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆ!")
    return True


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    print()
    print("æ£€æŸ¥ä¾èµ–...")
    print()

    all_ok = True

    # æ£€æŸ¥ transformers
    try:
        import transformers

        print(f"âœ… transformers {transformers.__version__}")
    except ImportError:
        print("âŒ transformers æœªå®‰è£…")
        all_ok = False

    # æ£€æŸ¥ torch
    try:
        import torch

        cuda_available = torch.cuda.is_available()
        print(f"âœ… torch {torch.__version__}")
        print(f"   CUDA å¯ç”¨: {cuda_available}")
    except ImportError:
        print("âŒ torch æœªå®‰è£…")
        all_ok = False

    # æ£€æŸ¥ huggingface_hub
    try:
        import huggingface_hub

        print(f"âœ… huggingface_hub {huggingface_hub.__version__}")
    except ImportError:
        print("âŒ huggingface_hub æœªå®‰è£…")
        all_ok = False

    print()
    return all_ok


def main():
    """ä¸»å‡½æ•°"""
    print()
    print("â•”" + "â•" * 56 + "â•—")
    print("â•‘" + " " * 10 + "RAG æœ¬åœ° LLM æ¨¡å‹é…ç½®å·¥å…·" + " " * 21 + "â•‘")
    print("â•š" + "â•" * 56 + "â•")
    print()

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–")
        response = input("æ˜¯å¦ç°åœ¨å®‰è£…ä¾èµ–? (y/n): ")
        if response.lower() == "y":
            if not install_requirements():
                print("âŒ ä¾èµ–å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…")
                return
        else:
            print("è¯·å…ˆå®‰è£…ä¾èµ–åå†è¿è¡Œæ­¤è„šæœ¬")
            return

    # è¯¢é—®ä¸‹è½½æ–¹å¼
    print()
    print("é€‰æ‹©ä¸‹è½½æ–¹å¼:")
    print("1. HuggingFace (å›½é™…æº)")
    print("2. ModelScope (å›½å†…é•œåƒï¼Œæ¨è)")
    print()

    choice = input("è¯·é€‰æ‹© (1/2): ").strip()

    if choice == "1":
        success = download_qwen_model()
    elif choice == "2":
        success = download_from_modelscope()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return

    if success:
        print()
        print("é…ç½®è¯´æ˜:")
        print("-" * 60)
        print()
        print("åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ :")
        print()
        print("LLM_PROVIDER=local")
        print("LLM_MODEL=Qwen2.5-0.5B-Instruct")
        print("LOCAL_LLM_MODEL_PATH=./data/models/Qwen/Qwen2.5-0.5B-Instruct")
        print("LOCAL_LLM_DEVICE=cpu  # å¦‚æœæœ‰ GPUï¼Œæ”¹ä¸º cuda")
        print()
        print("æˆ–è€…ç›´æ¥ä¿®æ”¹ config.py ä¸­çš„é»˜è®¤é…ç½®")
        print()


if __name__ == "__main__":
    main()
