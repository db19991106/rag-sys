#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGçŸ¥è¯†åº“æ–‡æ¡£å¤„ç†å™¨ - æ··åˆåˆ‡åˆ†ç‰ˆæœ¬
åŠŸèƒ½ï¼šæ•°æ®æ¸…æ´— â†’ ç»“æ„åˆ‡åˆ† â†’ è¯­ä¹‰åˆ‡åˆ† â†’ åµŒå…¥è®¡ç®— â†’ ç»“æ„åŒ–JSONè¾“å‡º

å¤„ç†æµç¨‹ï¼š
1. æ•°æ®æ¸…æ´—ï¼šè¯»å–å¤šæ ¼å¼æ–‡æ¡£ï¼Œå»é™¤æ— æ•ˆå­—ç¬¦ï¼Œè§„æ•´è¡¨æ ¼ï¼Œç»Ÿä¸€ç¼–ç 
2. ç»“æ„åˆ‡åˆ†ï¼šæŒ‰ç« èŠ‚/å­æ ‡é¢˜/æ¡æ¬¾æ‹†åˆ†ï¼Œä¿ç•™è¡¨æ ¼ç»“æ„
3. è¯­ä¹‰åˆ‡åˆ†ï¼šä½¿ç”¨BGEæ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç›¸ä¼¼åº¦â‰¥0.8åˆå¹¶ï¼Œ<0.8æ‹†åˆ†
4. ç»“æ„åŒ–è¾“å‡ºï¼šç”ŸæˆæŒ‡å®šæ ¼å¼çš„JSONæ–‡ä»¶
"""

import json
import os
import re
import math
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, field, asdict
import warnings
warnings.filterwarnings('ignore')

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


# ==================== æ•°æ®ç±»å®šä¹‰ ====================

@dataclass
class TableData:
    """è¡¨æ ¼æ•°æ®ç»“æ„"""
    table_id: str
    rows: List[List[str]]
    chapter: str = ""
    subtitle: str = ""
    
    def to_dict(self) -> Dict:
        return {
            "table_id": self.table_id,
            "rows": self.rows,
            "chapter": self.chapter,
            "subtitle": self.subtitle
        }


@dataclass
class StructuredUnit:
    """ç»“æ„åˆ‡åˆ†åçš„åŸºæœ¬å•å…ƒ"""
    chapter: str = ""  # ç« èŠ‚åç§°
    subtitle: str = ""  # å­æ ‡é¢˜
    articles: List[str] = field(default_factory=list)  # æ¡æ¬¾åˆ—è¡¨
    content: str = ""  # çº¯æ–‡æœ¬å†…å®¹
    tables: List[TableData] = field(default_factory=list)  # è¡¨æ ¼æ•°æ®
    char_count: int = 0  # å­—ç¬¦æ•°


@dataclass
class Chunk:
    """æœ€ç»ˆåˆ‡åˆ†åçš„chunk"""
    chunk_id: str
    chapter: str
    subtitle: str
    articles: List[str]
    content: str
    tables: List[Dict]
    metadata: Dict[str, Any]
    embeddings: Optional[np.ndarray] = None  # ä¸´æ—¶å­˜å‚¨ï¼Œä¸åºåˆ—åŒ–
    
    def to_dict(self) -> Dict:
        return {
            "chunk_id": self.chunk_id,
            "chapter": self.chapter,
            "subtitle": self.subtitle,
            "articles": self.articles,
            "content": self.content,
            "tables": self.tables,
            "metadata": self.metadata
        }


# ==================== 1. æ•°æ®æ¸…æ´—æ¨¡å— ====================

class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨ï¼šå¤„ç†å„ç§æ ¼å¼æ–‡æ¡£ï¼Œå»é™¤æ— æ•ˆå­—ç¬¦ï¼Œè§„æ•´è¡¨æ ¼"""
    
    def __init__(self):
        self.encoding_errors = []
        
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬å†…å®¹
        - å»é™¤ä¹±ç å’Œæ— æ•ˆå­—ç¬¦
        - è§„æ•´ç©ºç™½ç¬¦
        - ç»Ÿä¸€ç¼–ç 
        """
        if not text:
            return ""
        
        # 1. ç»Ÿä¸€è½¬æ¢ä¸ºUTF-8ï¼ˆå·²è¯»å–ä¸ºstrï¼Œæ— éœ€é‡å¤è§£ç ï¼‰
        # 2. å»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦ï¼‰
        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f]', '', text)
        
        # 3. å»é™¤é›¶å®½å­—ç¬¦
        text = re.sub(r'[\u200b-\u200f\ufeff]', '', text)
        
        # 4. è§„æ•´ç©ºç™½ç¬¦ï¼šå¤šä¸ªç©ºæ ¼/åˆ¶è¡¨ç¬¦è½¬ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 5. è§„æ•´æ¢è¡Œï¼šå¤šä¸ªè¿ç»­æ¢è¡Œä¿ç•™æœ€å¤šä¸¤ä¸ª
        text = re.sub(r'\n{4,}', '\n\n\n', text)
        
        # 6. å»é™¤æ¯è¡Œé¦–å°¾ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        # 7. å»é™¤é¡µçœ‰é¡µè„šç±»å†…å®¹
        text = re.sub(r'\d+\s*/\s*\d+\s*é¡µ', '', text)
        text = re.sub(r'ç¬¬\s*\d+\s*é¡µ', '', text)
        text = re.sub(r'Page\s*\d+\s*(of|/)\s*\d+', '', text, flags=re.IGNORECASE)
        
        # 8. å»é™¤å¸¸è§çš„é¡µçœ‰å†…å®¹
        page_header_patterns = [
            r'WORDæ ¼å¼å¯ç¼–è¾‘',
            r'ä¸“ä¸šçŸ¥è¯†æ•´ç†åˆ†äº«',
            r'Wordæ–‡æ¡£',
            r'å¯ç¼–è¾‘',
        ]
        for pattern in page_header_patterns:
            text = re.sub(pattern, '', text)
        
        # 9. å»é™¤æ— å…³å†…å®¹ï¼ˆæ–‡æ¡£æœ«å°¾çš„åƒåœ¾å†…å®¹ï¼‰
        garbage_patterns = [
            r'å•çº¯çš„è¯¾æœ¬å†…å®¹ï¼Œå¹¶ä¸èƒ½æ»¡è¶³',
            r'å„¿ç«¥ç”»',
            r'ç‹ç‹¸å’Œé¸¡',
            r'å°é¸­å­å­¦æ¸¸æ³³',
            r'åæ‚”ä¹Ÿæ¥ä¸åŠ',
            r'æ‘˜è‰è“çš„å°å§‘å¨˜',
            r'å­¦ç”Ÿçš„éœ€è¦ï¼Œé€šè¿‡è¡¥å……',
            r'å„¿ç«¥æ„æ„¿ç”»',
            r'å„¿ç«¥æ‰©æ•£æ€§æ€ç»´',
            r'æƒ…èŠ‚ï¼Œéƒ½æ˜¯ä¸€åˆ™æœ‰è¶£çš„å°æ•…äº‹',
        ]
        for pattern in garbage_patterns:
            text = re.sub(pattern, '', text)
        
        # 10. å»é™¤å•ç‹¬å­˜åœ¨çš„çŸ­å¥ï¼ˆå¯èƒ½æ˜¯é¡µçœ‰é¡µè„šæ®‹ç•™ï¼‰
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            # è·³è¿‡å¤ªçŸ­çš„è¡Œï¼ˆé™¤éæ˜¯æœ‰æ•ˆçš„ç¼–å·ï¼‰
            if len(line) < 5 and not re.match(r'^[ï¼ˆ(]?\d+[ï¼‰)]?\s*$', line):
                continue
            cleaned_lines.append(line)
        text = '\n'.join(cleaned_lines)
        
        # 10. å»é™¤URLå’Œæ°´å°
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        return text.strip()
    
    def extract_tables_from_text(self, text: str) -> Tuple[str, List[TableData]]:
        """
        ä»æ–‡æœ¬ä¸­æå–è¡¨æ ¼
        æ”¯æŒMarkdownè¡¨æ ¼ã€ASCIIè¡¨æ ¼ã€åˆ¶è¡¨ç¬¦åˆ†éš”ç­‰æ ¼å¼
        """
        tables = []
        table_id_counter = 1
        
        # æ¨¡å¼1: Markdownè¡¨æ ¼
        md_pattern = r'\|(.+)\|\n\|[-:\s|]+\|\n((?:\|.+\|\n?)+)'
        md_matches = list(re.finditer(md_pattern, text))
        
        for match in md_matches:
            header_line = match.group(1)
            body_lines = match.group(2).strip().split('\n')
            
            # è§£æè¡¨å¤´
            headers = [cell.strip() for cell in header_line.split('|') if cell.strip()]
            
            # è§£æè¡Œ
            rows = [headers]
            for line in body_lines:
                cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                if cells:
                    rows.append(cells)
            
            table = TableData(
                table_id=f"T1_{table_id_counter:03d}",
                rows=rows
            )
            tables.append(table)
            table_id_counter += 1
            
            # ä»æ–‡æœ¬ä¸­ç§»é™¤è¡¨æ ¼
            text = text.replace(match.group(0), f"\n[TABLE_{table.table_id}]\n")
        
        # æ¨¡å¼2: ç®€å•ASCIIè¡¨æ ¼ï¼ˆç©ºæ ¼/åˆ¶è¡¨ç¬¦å¯¹é½ï¼‰
        # æ£€æµ‹è¿ç»­å¤šè¡Œå…·æœ‰ç›¸ä¼¼ç»“æ„çš„æ–‡æœ¬
        lines = text.split('\n')
        table_ranges = []
        in_table = False
        table_start = 0
        
        for i, line in enumerate(lines):
            # æ£€æµ‹è¡¨æ ¼è¡Œï¼šåŒ…å«å¤šä¸ªç©ºæ ¼åˆ†éš”çš„å†…å®¹
            if re.match(r'^[\s\u4e00-\u9fa5a-zA-Z0-9]+(\s{2,}[\s\u4e00-\u9fa5a-zA-Z0-9]+){2,}$', line):
                if not in_table:
                    in_table = True
                    table_start = i
            else:
                if in_table and i - table_start >= 2:  # è‡³å°‘2è¡Œæ‰è®¤ä¸ºæ˜¯è¡¨æ ¼
                    table_ranges.append((table_start, i))
                in_table = False
        
        # å¤„ç†æ£€æµ‹åˆ°çš„è¡¨æ ¼
        for start, end in reversed(table_ranges):  # åå‘å¤„ç†ä»¥ä¾¿åˆ é™¤
            table_lines = lines[start:end]
            if len(table_lines) >= 2:
                # ç®€å•åˆ†å‰²
                rows = []
                for line in table_lines:
                    cells = [cell.strip() for cell in re.split(r'\s{2,}', line) if cell.strip()]
                    if cells:
                        rows.append(cells)
                
                if rows:
                    table = TableData(
                        table_id=f"T1_{table_id_counter:03d}",
                        rows=rows
                    )
                    tables.append(table)
                    table_id_counter += 1
                    
                    # æ›¿æ¢ä¸ºå ä½ç¬¦
                    lines[start:end] = [f"[TABLE_{table.table_id}]"]
        
        text = '\n'.join(lines)
        
        return text, tables
    
    def load_document(self, filepath: Path) -> Tuple[str, List[TableData], List[int]]:
        """
        åŠ è½½æ–‡æ¡£ï¼Œæ”¯æŒTXTã€DOCã€DOCXã€PDFæ ¼å¼
        è¿”å›ï¼š(æ¸…æ´—åçš„æ–‡æœ¬, è¡¨æ ¼åˆ—è¡¨, è¡¨æ ¼æ®µè½ç´¢å¼•åˆ—è¡¨)
        """
        suffix = filepath.suffix.lower()
        
        if suffix == '.txt':
            text, tables = self._load_txt(filepath)
            return text, tables, []
        elif suffix == '.doc':
            return self._load_doc(filepath)
        elif suffix == '.docx':
            return self._load_docx(filepath)
        elif suffix == '.pdf':
            text, tables = self._load_pdf(filepath)
            return text, tables, []
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
    
    def _load_txt(self, filepath: Path) -> Tuple[str, List[TableData]]:
        """åŠ è½½TXTæ–‡ä»¶"""
        # å°è¯•ä¸åŒç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'utf-16', 'latin1']
        raw_text = None
        
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    raw_text = f.read()
                break
            except UnicodeDecodeError:
                continue
        
        if raw_text is None:
            raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶: {filepath}")
        
        # æ¸…æ´—æ–‡æœ¬
        cleaned_text = self.clean_text(raw_text)
        
        # æå–è¡¨æ ¼
        text, tables = self.extract_tables_from_text(cleaned_text)
        
        return text, tables
    
    def _load_doc(self, filepath: Path) -> Tuple[str, List[TableData], List[int]]:
        """åŠ è½½DOCæ–‡ä»¶ï¼ˆæ—§ç‰ˆWordæ ¼å¼æˆ–ä¼ªè£…æˆdocçš„docxï¼‰"""
        # æ–¹æ³•1ï¼šå°è¯•ç”¨python-docxç›´æ¥è¯»å–ï¼ˆå¾ˆå¤š.docå®é™…æ˜¯docxæ ¼å¼ï¼‰
        try:
            from docx import Document
            doc = Document(filepath)
            
            # åˆ›å»ºæ®µè½å’Œè¡¨æ ¼å¯¹è±¡çš„IDæ˜ å°„
            para_by_elem = {p._element: p for p in doc.paragraphs}
            table_by_elem = {t._element: t for t in doc.tables}
            
            result_elements = []  # (type, content)
            tables = []
            table_counter = 1
            
            # éå†bodyçš„æ‰€æœ‰å­å…ƒç´ ï¼Œæ ¹æ®tagåˆ¤æ–­ç±»å‹
            for child in doc.element.body:
                tag = child.tag.lower()
                
                if tag.endswith('p'):  # æ®µè½
                    para = para_by_elem.get(child)
                    if para:
                        text = para.text.strip()
                        if text:
                            result_elements.append(('para', text))
                elif tag.endswith('tbl'):  # è¡¨æ ¼
                    table = table_by_elem.get(child)
                    if table:
                        rows = []
                        for row in table.rows:
                            cells = [cell.text.strip() for cell in row.cells]
                            rows.append(cells)
                        
                        if rows:
                            table_id = f"T1_{table_counter:03d}"
                            table_data = TableData(
                                table_id=table_id,
                                rows=rows
                            )
                            tables.append(table_data)
                            result_elements.append(('table', table_id))
                            table_counter += 1
            
            # æ„å»ºæ–‡æœ¬ï¼šæ®µè½å’Œè¡¨æ ¼äº¤æ›¿
            text_parts = []
            for elem_type, content in result_elements:
                if elem_type == 'para':
                    text_parts.append(content)
                elif elem_type == 'table':
                    text_parts.append(f"[TABLE_{content}]")
            
            text = '\n'.join(text_parts)
            cleaned_text = self.clean_text(text)
            
            # è¿”å›è¡¨æ ¼ç´¢å¼•
            table_indices = []
            para_count = 0
            for elem_type, _ in result_elements:
                if elem_type == 'para':
                    para_count += 1
                elif elem_type == 'table':
                    table_indices.append(para_count)
            
            return cleaned_text, tables, table_indices
        except Exception as e:
            pass
        
        # æ–¹æ³•2ï¼šå°è¯•ä½¿ç”¨antiwordå‘½ä»¤
        try:
            import subprocess
            result = subprocess.run(['antiword', str(filepath)], 
                                  capture_output=True, text=True, timeout=30)
            if result.returncode == 0:
                raw_text = result.stdout
                cleaned_text = self.clean_text(raw_text)
                text, tables = self.extract_tables_from_text(cleaned_text)
                return text, tables, []
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
            pass
        
        # æ–¹æ³•3ï¼šå°è¯•ä½¿ç”¨textractåº“
        try:
            import textract
            raw_text = textract.process(str(filepath), encoding='utf-8').decode('utf-8')
            cleaned_text = self.clean_text(raw_text)
            text, tables = self.extract_tables_from_text(cleaned_text)
            return text, tables, []
        except Exception as e:
            print(f"  textractå¤„ç†å¤±è´¥: {e}")
        
        # æ–¹æ³•4ï¼šå°è¯•ä½¿ç”¨docx2txt
        try:
            import docx2txt
            raw_text = docx2txt.process(str(filepath))
            cleaned_text = self.clean_text(raw_text)
            text, tables = self.extract_tables_from_text(cleaned_text)
            return text, tables, []
        except Exception as e:
            print(f"  docx2txtå¤„ç†å¤±è´¥: {e}")
        
        # æ–¹æ³•5ï¼šä½¿ç”¨olefileæå–æ–‡æœ¬ï¼ˆçº¯æ–‡æœ¬æ–¹å¼ï¼‰
        try:
            import olefile
            if olefile.isOleFile(str(filepath)):
                ole = olefile.OleFileIO(str(filepath))
                if ole.exists('WordDocument'):
                    word_stream = ole.openstream('WordDocument').read()
                    text_parts = []
                    i = 0
                    while i < len(word_stream):
                        if 32 <= word_stream[i] <= 126 or word_stream[i] >= 128:
                            try:
                                char = bytes([word_stream[i]]).decode('utf-8', errors='ignore')
                                if char:
                                    text_parts.append(char)
                            except:
                                pass
                        elif word_stream[i] in (0x0d, 0x0a):
                            text_parts.append('\n')
                        i += 1
                    raw_text = ''.join(text_parts)
                    cleaned_text = self.clean_text(raw_text)
                    text, tables = self.extract_tables_from_text(cleaned_text)
                    return text, tables, []
        except Exception as e:
            print(f"  olefileå¤„ç†å¤±è´¥: {e}")
        
        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        raise ValueError(
            f"æ— æ³•å¤„ç†DOCæ–‡ä»¶: {filepath.name}\n"
            f"è¯·å®‰è£…ä»¥ä¸‹ä»»ä¸€ä¾èµ–ï¼š\n"
            f"  - antiword (ç³»ç»ŸåŒ…): apt-get install antiword\n"
            f"  - textract: pip install textract\n"
            f"  - docx2txt: pip install docx2txt\n"
            f"  - olefile: pip install olefile"
        )
    
    def _load_docx(self, filepath: Path) -> Tuple[str, List[TableData], List[int]]:
        """
        åŠ è½½DOCXæ–‡ä»¶ï¼Œè¿”å›(æ–‡æœ¬, è¡¨æ ¼åˆ—è¡¨, è¡¨æ ¼æ®µè½ç´¢å¼•åˆ—è¡¨)
        """
        try:
            from docx import Document
        except ImportError:
            print("è­¦å‘Š: python-docxæœªå®‰è£…ï¼Œè·³è¿‡DOCXå¤„ç†")
            return "", [], []
        
        doc = Document(filepath)
        
        # åˆ›å»ºæ®µè½å’Œè¡¨æ ¼å¯¹è±¡çš„IDæ˜ å°„
        para_by_elem = {p._element: p for p in doc.paragraphs}
        table_by_elem = {t._element: t for t in doc.tables}
        
        result_elements = []  # (type, content) - type: 'para' or 'table'
        tables = []
        table_counter = 1
        
        # éå†bodyçš„æ‰€æœ‰å­å…ƒç´ ï¼Œæ ¹æ®tagåˆ¤æ–­ç±»å‹
        for child in doc.element.body:
            tag = child.tag.lower()
            
            if tag.endswith('p'):  # æ®µè½
                para = para_by_elem.get(child)
                if para:
                    text = para.text.strip()
                    if text:
                        result_elements.append(('para', text))
            elif tag.endswith('tbl'):  # è¡¨æ ¼
                table = table_by_elem.get(child)
                if table:
                    rows = []
                    for row in table.rows:
                        cells = [cell.text.strip() for cell in row.cells]
                        rows.append(cells)
                    
                    if rows:
                        table_id = f"T1_{table_counter:03d}"
                        table_data = TableData(
                            table_id=table_id,
                            rows=rows
                        )
                        tables.append(table_data)
                        result_elements.append(('table', table_id))
                        table_counter += 1
        
        # æ„å»ºæ–‡æœ¬ï¼šæ®µè½å’Œè¡¨æ ¼äº¤æ›¿
        text_parts = []
        for elem_type, content in result_elements:
            if elem_type == 'para':
                text_parts.append(content)
            elif elem_type == 'table':
                text_parts.append(f"[TABLE_{content}]")
        
        text = '\n'.join(text_parts)
        cleaned_text = self.clean_text(text)
        
        # è¿”å›è¡¨æ ¼ç´¢å¼•ï¼ˆæ¯ä¸ªè¡¨æ ¼åœ¨ç¬¬å‡ ä¸ªæ®µè½ä¹‹åï¼‰
        table_indices = []
        para_count = 0
        for elem_type, _ in result_elements:
            if elem_type == 'para':
                para_count += 1
            elif elem_type == 'table':
                table_indices.append(para_count)
        
        return cleaned_text, tables, table_indices
    
    def _load_pdf(self, filepath: Path) -> Tuple[str, List[TableData]]:
        """åŠ è½½PDFæ–‡ä»¶"""
        try:
            import PyPDF2
        except ImportError:
            print("è­¦å‘Š: PyPDF2æœªå®‰è£…ï¼Œè·³è¿‡PDFå¤„ç†")
            return "", []
        
        text_parts = []
        tables = []
        
        with open(filepath, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(page_text)
        
        raw_text = '\n'.join(text_parts)
        cleaned_text = self.clean_text(raw_text)
        
        # å°è¯•æå–è¡¨æ ¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        text, tables = self.extract_tables_from_text(cleaned_text)
        
        return text, tables


# ==================== 2. ç»“æ„åˆ‡åˆ†æ¨¡å— ====================

class StructureParser:
    """ç»“æ„è§£æå™¨ï¼šæŒ‰ç« èŠ‚/æ¡æ¬¾è¿›è¡Œç²¾å‡†åˆ‡åˆ†"""
    
    # ç« èŠ‚æ ‡é¢˜æ­£åˆ™ - åªåŒ¹é…ã€Œç¬¬Xç« ã€æ ¼å¼
    CHAPTER_PATTERN = r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç« |ç¬¬\d+ç« )\s*'
    CHAPTER_REGEX = re.compile(CHAPTER_PATTERN)
    
    # æ¡æ¬¾æ­£åˆ™ - åªåŒ¹é…ã€Œç¬¬ä¸€æ¡ã€ã€Œç¬¬1æ¡ã€æ ¼å¼
    ARTICLE_PATTERN = r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡|ç¬¬\d+æ¡)\s*'
    ARTICLE_REGEX = re.compile(ARTICLE_PATTERN)
    
    def __init__(self):
        pass
    
    def is_chapter_start(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚å¼€å§‹"""
        return bool(self.CHAPTER_REGEX.match(line.strip()))
    
    def is_article_start(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æ¡æ¬¾å¼€å§‹"""
        return bool(self.ARTICLE_REGEX.match(line.strip()))
    
    def identify_line_type(self, line: str) -> str:
        """è¯†åˆ«è¡Œç±»å‹ï¼šç« èŠ‚æ ‡é¢˜ã€å­æ ‡é¢˜æˆ–æ­£æ–‡å†…å®¹"""
        line = line.strip()
        if not line:
            return "empty"
        if self.is_chapter_start(line):
            return "chapter"
        if self.is_article_start(line):
            return "subtitle"
        return "content"
    
    def extract_subtitle_number(self, line: str) -> str:
        """æå–å­æ ‡é¢˜ç¼–å·ï¼ˆå¦‚"ç¬¬ä¸€æ¡"ã€"ï¼ˆä¸€ï¼‰"ç­‰ï¼‰"""
        line = line.strip()
        if self.ARTICLE_REGEX.match(line):
            return line
        return ""
    
    def extract_chapter(self, line: str) -> str:
        """æå–ç« èŠ‚å®Œæ•´æ ‡é¢˜ï¼ˆå¦‚"ç¬¬ä¸€ç«  æ€»åˆ™"ï¼‰"""
        match = self.CHAPTER_REGEX.match(line.strip())
        if match:
            return match.group(0).strip()
        return ""
    
    def extract_article_number(self, line: str) -> str:
        """æå–æ¡æ¬¾ç¼–å·ï¼ˆå¦‚"ç¬¬ä¸€æ¡"ï¼‰"""
        match = self.ARTICLE_REGEX.match(line.strip())
        if match:
            return match.group(0).strip()
        return ""
    
    def parse_structure(self, text: str, tables: List[TableData], table_indices: List[int] = None) -> List[StructuredUnit]:
        """
        è§£ææ–‡æ¡£ç»“æ„ï¼Œç”Ÿæˆç»“æ„åŒ–å•å…ƒåˆ—è¡¨
        ç­–ç•¥ï¼š
        1. æŒ‰ã€Œç¬¬ä¸€å±‚ï¼šæŒ‰ã€Œç¬¬ä¸€ç« ã€åˆ‡åˆ†ç« èŠ‚
        2. åœ¨ç« èŠ‚å†…æŒ‰ã€Œç¬¬ä¸€å±‚ï¼šæŒ‰ã€Œç¬¬ä¸€æ¡ã€åˆ‡åˆ†æ¡æ¬¾
        3. æ¯ä¸ªæ¡æ¬¾ä½œä¸ºä¸€ä¸ªåŸºç¡€å•å…ƒ
        """
        if table_indices is None:
            table_indices = []
        
        lines = text.split('\n')
        units = []
        
        # åˆ›å»ºè¡¨æ ¼IDåˆ°è¡¨æ ¼å¯¹è±¡çš„æ˜ å°„
        table_map = {t.table_id: t for t in tables}
        
        # çŠ¶æ€å˜é‡
        current_chapter = ""
        current_article = ""  # å½“å‰æ¡æ¬¾å·
        content_buffer = []  # å½“å‰æ¡æ¬¾å†…å®¹ç¼“å­˜
        current_tables = []  # å½“å‰æ¡æ¬¾å…³è”çš„è¡¨æ ¼
        
        def flush_article():
            """ä¿å­˜å½“å‰æ¡æ¬¾ä¸ºä¸€ä¸ªunit"""
            nonlocal units, current_chapter, current_article, content_buffer, current_tables
            
            if not content_buffer:
                return
            
            content = '\n'.join(content_buffer).strip()
            if content:
                # å…³è”è¡¨æ ¼å†…å®¹
                table_text = ""
                for t in current_tables:
                    table_text += f"\n\nã€è¡¨æ ¼ {t.table_id}ã€‘\n"
                    for row in t.rows:
                        table_text += " | ".join(row) + "\n"
                
                full_content = content + table_text
                
                unit = StructuredUnit(
                    chapter=current_chapter if current_chapter else "æœªåˆ†ç±»",
                    subtitle=current_article,
                    articles=[current_article] if current_article else [],
                    content=full_content,
                    char_count=len(full_content),
                    tables=[t for t in current_tables]
                )
                units.append(unit)
            
            content_buffer = []
            current_tables = []
        
        # è·Ÿè¸ªå½“å‰å¤„ç†åˆ°çš„æ®µè½ç´¢å¼•
        current_para_idx = 0
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # éç©ºè¡Œè®¡æ•°
            if line:
                current_para_idx += 1
            
            if self.is_chapter_start(line):
                # æ–°ç« èŠ‚å¼€å§‹ï¼Œä¿å­˜ä¹‹å‰çš„æ¡æ¬¾
                flush_article()
                current_chapter = line.strip()
                current_article = ""
                
            elif self.is_article_start(line):
                # æ–°æ¡æ¬¾å¼€å§‹ï¼Œä¿å­˜ä¹‹å‰çš„æ¡æ¬¾
                flush_article()
                current_article = line.strip()
                content_buffer = [line]
                
            elif line:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼æ ‡è®°
                table_match = re.match(r'\[TABLE_(T\d+_\d+)\]', line)
                if table_match:
                    table_id = table_match.group(1)
                    if table_id in table_map:
                        table = table_map[table_id]
                        table.chapter = current_chapter
                        current_tables.append(table)
                else:
                    # ç´¯ç§¯åˆ°å½“å‰æ¡æ¬¾å†…å®¹
                    content_buffer.append(line)
            
            i += 1
        
        # ä¿å­˜æœ€åä¸€ä¸ªæ¡æ¬¾
        flush_article()
        
        return units
    
    def parse_structure(self, text: str, tables: List[TableData], table_indices: List[int] = None) -> List[StructuredUnit]:
        """
        è§£ææ–‡æ¡£ç»“æ„ï¼Œç”Ÿæˆç»“æ„åŒ–å•å…ƒåˆ—è¡¨
        æ ¸å¿ƒç­–ç•¥ï¼šä»¥å­æ ‡é¢˜/æ¡æ¬¾ä¸ºæœ€å°å•å…ƒï¼Œæ¯ä¸ªç‹¬ç«‹ä¸€ä¸ªunit
        è¡¨æ ¼æ ¹æ®ä½ç½®å…³è”åˆ°å½“å‰ç« èŠ‚
        """
        if table_indices is None:
            # ä»æ¸…æ´—åçš„æ–‡æœ¬ä¸­é‡æ–°è®¡ç®—è¡¨æ ¼ä½ç½®
            lines = text.split('\n')
            table_indices = []
            for i, line in enumerate(lines):
                if '[TABLE_' in line:
                    table_indices.append(i)
        
        lines = text.split('\n')
        units = []
        
        # åˆ›å»ºè¡¨æ ¼IDåˆ°è¡¨æ ¼å¯¹è±¡çš„æ˜ å°„
        table_map = {t.table_id: t for t in tables}
        
        # çŠ¶æ€å˜é‡
        current_chapter_title = ""  # ç« èŠ‚å®Œæ•´æ ‡é¢˜ï¼ˆå«"ç¬¬Xç« "ï¼‰
        current_subtitle = ""  # å½“å‰å­æ ‡é¢˜ç¼–å·ï¼ˆå¦‚"ï¼ˆä¸€ï¼‰"ï¼‰
        content_buffer = []  # å½“å‰unitå†…å®¹ç¼“å­˜
        current_tables = []  # å½“å‰unitå…³è”çš„è¡¨æ ¼
        
        # è·Ÿè¸ªå½“å‰å¤„ç†åˆ°çš„è¡Œç´¢å¼•
        current_line_idx = 0
        
        def flush_content():
            """ä¿å­˜å½“å‰å†…å®¹ä¸ºä¸€ä¸ªunit"""
            nonlocal units, current_chapter_title, current_subtitle, content_buffer, current_tables
            
            if not content_buffer and not current_tables:
                return
            
            content = '\n'.join(content_buffer).strip() if content_buffer else ""
            
            # å¦‚æœæœ‰å…³è”è¡¨æ ¼ï¼Œå°†è¡¨æ ¼å†…å®¹æ·»åŠ åˆ°contentä¸­
            if current_tables:
                table_content = "\n\nã€è¡¨æ ¼ã€‘\n"
                for t in current_tables:
                    table_content += f"\n{t.table_id}:\n"
                    for row in t.rows:
                        table_content += " | ".join(row) + "\n"
                content = content + table_content if content else table_content
            
            if content:
                unit = StructuredUnit(
                    chapter=current_chapter_title if current_chapter_title else "æœªåˆ†ç±»",
                    subtitle=current_subtitle,
                    articles=[current_subtitle] if current_subtitle else [],
                    content=content,
                    char_count=len(content),
                    tables=[t for t in current_tables]
                )
                units.append(unit)
            
            content_buffer = []
            current_tables = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            line_type = self.identify_line_type(line)
            
            # æ›´æ–°å½“å‰è¡Œç´¢å¼•ï¼ˆéç©ºè¡Œï¼‰
            if line:
                current_line_idx += 1
            
            if line_type == "chapter":
                # å…ˆä¿å­˜ä¹‹å‰çš„å†…å®¹
                flush_content()
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_chapter_title = line.strip()
                
            elif line_type == "subtitle":
                # é‡åˆ°æ–°çš„å­æ ‡é¢˜ï¼Œå…ˆä¿å­˜ä¹‹å‰çš„
                flush_content()
                
                # å¼€å§‹æ–°çš„å­æ ‡é¢˜
                current_subtitle = self.extract_subtitle_number(line)
                content_buffer = [line]
                
            elif line_type == "content":
                if line:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼æ ‡è®°
                    table_match = re.match(r'\[TABLE_(T\d+_\d+)\]', line)
                    if table_match:
                        # å…ˆä¿å­˜å½“å‰å†…å®¹
                        flush_content()
                        
                        # åˆ›å»ºå•ç‹¬çš„è¡¨æ ¼unitï¼Œå…³è”åˆ°å½“å‰ç« èŠ‚
                        table_id = table_match.group(1)
                        if table_id in table_map:
                            table = table_map[table_id]
                            table.chapter = current_chapter_title
                            # åˆ›å»ºä¸€ä¸ªåªåŒ…å«è¡¨æ ¼çš„unit
                            table_unit = StructuredUnit(
                                chapter=current_chapter_title,
                                subtitle="",
                                articles=[],
                                content=f"[è¡¨æ ¼]",
                                char_count=0,
                                tables=[table]
                            )
                            units.append(table_unit)
                    else:
                        # ç´¯ç§¯åˆ°å½“å‰å†…å®¹
                        content_buffer.append(line)
            
            i += 1
        
        # ä¿å­˜æœ€åä¸€ä¸ªå†…å®¹
        flush_content()
        
        return units
        
        # ä¿å­˜æœ€åä¸€ä¸ªå†…å®¹
        flush_content()
        
        return units
    
    def _merge_short_units(self, units: List[StructuredUnit], min_chars: int = 100) -> List[StructuredUnit]:
        """åˆå¹¶è¿‡çŸ­çš„è¿ç»­å•å…ƒ"""
        if not units:
            return units
        
        merged = []
        current = units[0]
        
        for i in range(1, len(units)):
            next_unit = units[i]
            
            # å¦‚æœå½“å‰å•å…ƒå¤ªçŸ­ä¸”å±äºåŒä¸€ç« èŠ‚/å­æ ‡é¢˜ï¼Œå°è¯•åˆå¹¶
            if (current.char_count < min_chars and 
                current.chapter == next_unit.chapter and
                current.subtitle == next_unit.subtitle and
                not current.tables and not next_unit.tables):  # ä¸åŒ…å«è¡¨æ ¼
                
                # åˆå¹¶å†…å®¹
                current.content += '\n' + next_unit.content
                current.articles.extend(next_unit.articles)
                current.char_count = len(current.content)
            else:
                merged.append(current)
                current = next_unit
        
        merged.append(current)
        return merged


# ==================== 3. è¯­ä¹‰åˆ‡åˆ†æ¨¡å— ====================

class SemanticChunker:
    """è¯­ä¹‰åˆ‡åˆ†å™¨ï¼šä½¿ç”¨BGEæ¨¡å‹è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
    
    def __init__(self, model_path: str, similarity_threshold: float = 0.8):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ‡åˆ†å™¨
        
        Args:
            model_path: BGEæ¨¡å‹è·¯å¾„
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œâ‰¥æ­¤å€¼åˆ™åˆå¹¶
        """
        self.model_path = model_path
        self.similarity_threshold = similarity_threshold
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½BGEåµŒå…¥æ¨¡å‹"""
        try:
            print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_path}")
            self.model = SentenceTransformer(self.model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆä¸è¿›è¡Œè¯­ä¹‰åˆ‡åˆ†ï¼‰")
            self.model = None
    
    def compute_embedding(self, text: str) -> np.ndarray:
        """è®¡ç®—æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        if self.model is None:
            # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
            return np.zeros(768)
        
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡é•¿
        if len(text) > 1000:
            text = text[:1000]
        
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding
    
    def compute_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        if self.model is None:
            return 1.0  # åå¤‡æ–¹æ¡ˆï¼šå‡è®¾é«˜ç›¸ä¼¼åº¦
        
        # å½’ä¸€åŒ–
        emb1_norm = emb1 / (np.linalg.norm(emb1) + 1e-8)
        emb2_norm = emb2 / (np.linalg.norm(emb2) + 1e-8)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(emb1_norm, emb2_norm)
        return float(similarity)
    
    def semantic_chunking(self, units: List[StructuredUnit], chapter_num: int) -> List[Chunk]:
        """
        å¯¹ç»“æ„åˆ‡åˆ†åçš„å•å…ƒè¿›è¡Œè¯­ä¹‰åˆ‡åˆ†
        
        ç­–ç•¥ï¼š
        1. è®¡ç®—æ¯ä¸ªå•å…ƒçš„åµŒå…¥å‘é‡
        2. éå†å•å…ƒï¼Œè®¡ç®—ç›¸é‚»å•å…ƒé—´çš„ç›¸ä¼¼åº¦
        3. ç›¸ä¼¼åº¦â‰¥é˜ˆå€¼åˆ™åˆå¹¶ï¼Œ<é˜ˆå€¼åˆ™æ‹†åˆ†
        4. æ¯ä¸ªchunkè®°å½•å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦
        """
        if not units:
            return []
        
        # è®¡ç®—æ‰€æœ‰å•å…ƒçš„åµŒå…¥
        print(f"  è®¡ç®— {len(units)} ä¸ªå•å…ƒçš„è¯­ä¹‰åµŒå…¥...")
        embeddings = []
        for i, unit in enumerate(units):
            emb = self.compute_embedding(unit.content)
            embeddings.append(emb)
        
        # è¯­ä¹‰èšç±»ä¸åˆå¹¶
        chunks = []
        current_chunk_units = [units[0]]
        current_chunk_embeddings = [embeddings[0]]
        similarities = []
        chunk_idx = 1
        
        for i in range(1, len(units)):
            current_unit = units[i]
            current_emb = embeddings[i]
            
            # è®¡ç®—ä¸å½“å‰chunkä¸­æ‰€æœ‰å•å…ƒçš„å¹³å‡ç›¸ä¼¼åº¦
            if current_chunk_embeddings:
                sims = [self.compute_similarity(current_emb, emb) for emb in current_chunk_embeddings]
                avg_sim = sum(sims) / len(sims)
                similarities.append(avg_sim)
                
                # å†³ç­–ï¼šæ˜¯å¦åˆå¹¶
                if avg_sim >= self.similarity_threshold:
                    # åˆå¹¶åˆ°å½“å‰chunk
                    current_chunk_units.append(current_unit)
                    current_chunk_embeddings.append(current_emb)
                else:
                    # ä¿å­˜å½“å‰chunkï¼Œå¼€å§‹æ–°chunk
                    chunk = self._create_chunk(
                        current_chunk_units, 
                        current_chunk_embeddings,
                        chapter_num,
                        chunk_idx,
                        similarities if similarities else [1.0]
                    )
                    chunks.append(chunk)
                    
                    # é‡ç½®
                    current_chunk_units = [current_unit]
                    current_chunk_embeddings = [current_emb]
                    similarities = []
                    chunk_idx += 1
        
        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if current_chunk_units:
            chunk = self._create_chunk(
                current_chunk_units,
                current_chunk_embeddings,
                chapter_num,
                chunk_idx,
                similarities if similarities else [1.0]
            )
            chunks.append(chunk)
        
        return chunks
    
    def _create_chunk(
        self, 
        units: List[StructuredUnit], 
        embeddings: List[np.ndarray],
        chapter_num: int,
        chunk_idx: int,
        similarities: List[float]
    ) -> Chunk:
        """ä»å•å…ƒåˆ—è¡¨åˆ›å»ºChunk"""
        
        # åˆå¹¶å†…å®¹
        contents = []
        articles = []
        tables = []
        chapter = units[0].chapter if units else ""
        subtitle = units[0].subtitle if units else ""
        
        for unit in units:
            if unit.content:
                contents.append(unit.content)
            articles.extend(unit.articles)
            tables.extend(unit.tables)
        
        merged_content = '\n\n'.join(contents)
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        avg_similarity = sum(similarities) / len(similarities) if similarities else 1.0
        avg_similarity = round(avg_similarity, 3)
        
        # ç¡®å®šåˆ‡åˆ†åŸå› 
        if avg_similarity >= self.similarity_threshold:
            chunking_reason = f"å¼ºè¯­ä¹‰å…³è”(ç›¸ä¼¼åº¦{avg_similarity:.3f})"
        else:
            chunking_reason = f"å¼±è¯­ä¹‰å…³è”(ç›¸ä¼¼åº¦{avg_similarity:.3f})"
        
        # ç”Ÿæˆchunk_id
        chunk_id = f"CH{chapter_num}_{chunk_idx:03d}"
        
        # ç»Ÿè®¡ä¿¡æ¯
        char_count = len(merged_content)
        article_count = len(articles)
        has_table = len(tables) > 0
        
        # æ„å»ºmetadata
        metadata = {
            "char_count": char_count,
            "article_count": article_count,
            "avg_similarity": avg_similarity,
            "has_table": has_table,
            "chunking_reason": chunking_reason
        }
        
        # è½¬æ¢tablesä¸ºdictåˆ—è¡¨
        tables_dict = [t.to_dict() for t in tables]
        
        return Chunk(
            chunk_id=chunk_id,
            chapter=chapter,
            subtitle=subtitle,
            articles=articles,
            content=merged_content,
            tables=tables_dict,
            metadata=metadata
        )


# ==================== 4. ä¸»å¤„ç†å™¨ ====================

class RAGDocumentProcessor:
    """RAGæ–‡æ¡£å¤„ç†å™¨ï¼šæ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹"""
    
    def __init__(
        self, 
        data_dir: str,
        output_dir: str,
        model_path: str = "backend/data/models/BAAI--bge-base-zh-v1.5",
        similarity_threshold: float = 0.8
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            data_dir: è¾“å…¥æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            model_path: BGEæ¨¡å‹è·¯å¾„
            similarity_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.cleaner = DataCleaner()
        self.parser = StructureParser()
        self.chunker = SemanticChunker(model_path, similarity_threshold)
        
        # å…¨å±€chunk_idè®¡æ•°å™¨
        self.chunk_id_counter = 0
        self.used_chunk_ids = set()
    
    def extract_chapter_number(self, chapter_name: str) -> int:
        """ä»ç« èŠ‚åç§°æå–æ•°å­—"""
        # å°è¯•åŒ¹é…"ç¬¬Xç« "
        match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+)ç« ', chapter_name)
        if match:
            num_str = match.group(1)
            # è½¬æ¢ä¸­æ–‡æ•°å­—
            chinese_nums = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                          'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10}
            if num_str in chinese_nums:
                return chinese_nums[num_str]
            elif num_str.isdigit():
                return int(num_str)
        
        # å°è¯•åŒ¹é…"Xã€"
        match = re.search(r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])[ã€ï¼.]', chapter_name)
        if match:
            chinese_nums = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                          'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10}
            return chinese_nums.get(match.group(1), 1)
        
        return 1
    
    def generate_unique_chunk_id(self, chapter_num: int, idx: int) -> str:
        """ç”Ÿæˆå…¨å±€å”¯ä¸€çš„chunk_id"""
        # ä½¿ç”¨å…¨å±€è®¡æ•°å™¨ç¡®ä¿å”¯ä¸€æ€§
        self.chunk_id_counter += 1
        chunk_id = f"CH{chapter_num}_{self.chunk_id_counter:03d}"
        
        # æ£€æŸ¥é‡å¤ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
        while chunk_id in self.used_chunk_ids:
            self.chunk_id_counter += 1
            chunk_id = f"CH{chapter_num}_{self.chunk_id_counter:03d}"
        
        self.used_chunk_ids.add(chunk_id)
        return chunk_id
    
    def process_file(self, filepath: Path) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        æµç¨‹ï¼š
        1. æ•°æ®æ¸…æ´—ï¼šåŠ è½½å¹¶æ¸…æ´—æ–‡æ¡£
        2. ç»“æ„åˆ‡åˆ†ï¼šæŒ‰ç« èŠ‚/å­æ ‡é¢˜/æ¡æ¬¾åˆ‡åˆ†
        3. è¯­ä¹‰åˆ‡åˆ†ï¼šåŸºäºç›¸ä¼¼åº¦åˆå¹¶/æ‹†åˆ†
        4. ç”ŸæˆJSONè¾“å‡º
        """
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ–‡ä»¶: {filepath.name}")
        print(f"{'='*60}")
        
        # æ­¥éª¤1ï¼šæ•°æ®æ¸…æ´—
        print("\n[æ­¥éª¤1] æ•°æ®æ¸…æ´—...")
        try:
            text, tables, table_indices = self.cleaner.load_document(filepath)
            print(f"  âœ“ æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {len(text)} å­—ç¬¦")
            print(f"  âœ“ æå–åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
        except Exception as e:
            print(f"  âœ— æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            return []
        
        # æ­¥éª¤2ï¼šç»“æ„åˆ‡åˆ†
        print("\n[æ­¥éª¤2] ç»“æ„åˆ‡åˆ†...")
        units = self.parser.parse_structure(text, tables, table_indices)
        print(f"  âœ“ ç»“æ„åˆ‡åˆ†å®Œæˆï¼Œå…± {len(units)} ä¸ªå•å…ƒ")
        
        # æŒ‰ç« èŠ‚åˆ†ç»„
        chapter_groups = {}
        for unit in units:
            chapter = unit.chapter if unit.chapter else "æœªåˆ†ç±»"
            if chapter not in chapter_groups:
                chapter_groups[chapter] = []
            chapter_groups[chapter].append(unit)
        
        print(f"  âœ“ è¯†åˆ«åˆ° {len(chapter_groups)} ä¸ªç« èŠ‚")
        
        # æ­¥éª¤3ï¼šè¯­ä¹‰åˆ‡åˆ†
        print("\n[æ­¥éª¤3] è¯­ä¹‰åˆ‡åˆ†...")
        all_chunks = []
        
        for chapter_name, chapter_units in chapter_groups.items():
            chapter_num = self.extract_chapter_number(chapter_name)
            print(f"\n  å¤„ç†ç« èŠ‚: {chapter_name} (ç¼–å·: {chapter_num})")
            
            # å¯¹è¯¥ç« èŠ‚çš„å•å…ƒè¿›è¡Œè¯­ä¹‰åˆ‡åˆ†
            chunks = self.chunker.semantic_chunking(chapter_units, chapter_num)
            
            # é‡æ–°åˆ†é…å…¨å±€å”¯ä¸€çš„chunk_id
            for i, chunk in enumerate(chunks):
                chunk.chunk_id = self.generate_unique_chunk_id(chapter_num, i + 1)
            
            all_chunks.extend(chunks)
            print(f"    â†’ ç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        
        print(f"\n  âœ“ è¯­ä¹‰åˆ‡åˆ†å®Œæˆï¼Œå…± {len(all_chunks)} ä¸ªchunks")
        
        # æ­¥éª¤4ï¼šè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        print("\n[æ­¥éª¤4] ç”ŸæˆJSONè¾“å‡º...")
        result = [chunk.to_dict() for chunk in all_chunks]
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(c.metadata["char_count"] for c in all_chunks)
        total_tables = sum(1 for c in all_chunks if c.metadata["has_table"])
        avg_similarity = sum(c.metadata["avg_similarity"] for c in all_chunks) / len(all_chunks) if all_chunks else 0
        
        print(f"  âœ“ æ€»å­—ç¬¦æ•°: {total_chars}")
        print(f"  âœ“ åŒ…å«è¡¨æ ¼çš„chunks: {total_tables}")
        print(f"  âœ“ å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
        
        return result
    
    def process_all(self):
        """å¤„ç†æ‰€æœ‰æ–‡æ¡£"""
        print("\n" + "="*60)
        print("RAGæ–‡æ¡£å¤„ç† - æ··åˆåˆ‡åˆ†")
        print("="*60)
        print(f"è¾“å…¥ç›®å½•: {self.data_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {self.chunker.similarity_threshold}")
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        supported_extensions = ['.txt', '.doc', '.docx', '.pdf']
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡æ¡£
        all_files = []
        for ext in supported_extensions:
            all_files.extend(self.data_dir.glob(f"*{ext}"))
        
        if not all_files:
            print(f"\nâš ï¸ åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£")
            return
        
        print(f"\nå‘ç° {len(all_files)} ä¸ªå¾…å¤„ç†æ–‡æ¡£")
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for filepath in sorted(all_files):
            chunks = self.process_file(filepath)
            
            if chunks:
                # ä¿å­˜ç»“æœ
                output_filename = f"{filepath.stem}_chunks.json"
                output_path = self.output_dir / output_filename
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(chunks, f, ensure_ascii=False, indent=2)
                
                print(f"\n  ğŸ’¾ å·²ä¿å­˜: {output_path}")
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        print(f"   å…¨å±€Chunk IDèŒƒå›´: CH1_001 ~ CH{len(self.used_chunk_ids)}_{self.chunk_id_counter:03d}")
        print("="*60)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    import shutil
    
    # é…ç½®è·¯å¾„
    data_dir = "/root/autodl-tmp/rag/test_chunking/data"
    output_dir = "/root/autodl-tmp/rag/test_chunking/output"
    # ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹å¿«ç…§è·¯å¾„
    model_path = "/root/autodl-tmp/rag/backend/data/models/BAAI--bge-base-zh-v1.5/models--BAAI--bge-base-zh-v1.5/snapshots/f03589ceff5aac7111bd60cfc7d497ca17ecac65"
    
    # æ¸…ç©ºè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿chunk_idå…¨å±€å”¯ä¸€ï¼‰
    output_path = Path(output_dir)
    if output_path.exists():
        for f in output_path.glob("*_chunks.json"):
            f.unlink()
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶æ‰§è¡Œ
    # æé«˜é˜ˆå€¼åˆ°0.95ï¼Œå‡å°‘åˆå¹¶ï¼Œè·å¾—æ›´ç»†ç²’åº¦çš„åˆ‡åˆ†
    processor = RAGDocumentProcessor(
        data_dir=data_dir,
        output_dir=output_dir,
        model_path=model_path,
        similarity_threshold=0.95
    )
    
    processor.process_all()


if __name__ == "__main__":
    main()
